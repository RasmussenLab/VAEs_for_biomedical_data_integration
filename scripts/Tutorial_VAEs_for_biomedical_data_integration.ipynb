{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUjdkp5dCNTj"
      },
      "source": [
        "\n",
        "\n",
        "# ON THE USE OF VAES FOR BIOMEDICAL DATA INTEGRATION: THE TUTORIAL\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpU4esWJDMK3"
      },
      "source": [
        "## Intro\n",
        "\n",
        "Hi!\n",
        "This notebook is a short tutorial to guide you through the main findings of the paper.\n",
        "\n",
        "**Background**\n",
        "\n",
        "We will explore the behavior of Multimodal Variational Autoencoders (VAEs) when integrating diverse data modalities.\n",
        "\n",
        ">📕 **What are VAEs?**\n",
        ">\n",
        ">If you are not familiar with VAEs, you can read:\n",
        ">- The original paper: [Autoencoding Variational Bayes](https://arxiv.org/abs/1312.6114), by Diederik P. Kingma and Max Welling\n",
        ">- The MultiOmics Variational Autoencoder [MOVE](https://www.nature.com/articles/s41587-022-01520-x).\n",
        ">- Deep Generative Modelling, by Jakub M. Tomczak. Chapter 4.3.\n",
        "\n",
        "\n",
        "\n",
        "**The notebook**\n",
        "\n",
        "In this notebook we will:\n",
        "- Install the Multiomics Variational Autoencoder [MOVE](https://www.nature.com/articles/s41587-022-01520-x)\n",
        "- Generate a synthetic dataset containing categorical and continuous features.\n",
        "- Run the main tasks in MOVE:\n",
        "  - Encode the data\n",
        "  - Analyze the latent space\n",
        "  - Identify associations\n",
        "  - Visualize the perturbations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoTku0mwFXcZ"
      },
      "source": [
        "## Install and import the required packages\n",
        "\n",
        "The code to create and run the Multiomics Variational AutoEncoder (MOVE) can be installed as a pip package.\n",
        "\n",
        ">⚠️ **Warning**\n",
        ">\n",
        ">You'll need to restart the runtime after running the command below to update the changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XKoX_2v_1vS",
        "outputId": "8bf0fdff-28fd-42e9-f6bf-0e638585aaf3"
      },
      "outputs": [],
      "source": [
        "! pip install move-dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iHdt0HbJI1B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random as rnd\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.datasets import make_sparse_spd_matrix\n",
        "from PIL import Image\n",
        "from matplotlib.pyplot import cm\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import ListedColormap\n",
        "from IPython.display import display\n",
        "from IPython.display import Image as Image_display\n",
        "\n",
        "\n",
        "#from scipy.stats import pearsonr\n",
        "#from move.data.preprocessing import scale\n",
        "#import os\n",
        "#import io\n",
        "#import seaborn as sns\n",
        "#import sys\n",
        "#import matplotlib as mpl\n",
        "#from itertools import chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X33ym7PUDWMJ"
      },
      "source": [
        "## Generating a synthetic dataset\n",
        "\n",
        "A synthetic dataset is created as a multivariate gaussian, where different features are different components of the Gaussian and each sample is a draw from the distribution. Explicit stronger correlations can be added by defining some features to be linear combinations of others. Categorical variables (binary) can be obtained by setting negative values to zero and positive values to one for a given feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JavLlueADdg6"
      },
      "outputs": [],
      "source": [
        "################################ Functions ####################################\n",
        "\n",
        "def get_feature_names(settings):\n",
        "    \"\"\"\n",
        "    This function returns a list with all feature names\n",
        "\n",
        "    Args:\n",
        "        settings (dict): dictionary with all settings\n",
        "\n",
        "    Returns:\n",
        "        all_feature_names: list with all feature names\n",
        "    \"\"\"\n",
        "    all_feature_names = [\n",
        "        f\"{key}_{i+1}\"\n",
        "        for key in settings.keys()\n",
        "        for i in range(settings[key][\"features\"])\n",
        "    ]\n",
        "    return all_feature_names\n",
        "\n",
        "\n",
        "def create_mean_profiles(settings):\n",
        "    \"\"\"\n",
        "    This function returns a list with all feature means.\n",
        "\n",
        "    Args:\n",
        "        settings (dict): dictionary with all settings.\n",
        "\n",
        "    Returns:\n",
        "        feature_means: list with all feature means.\n",
        "    \"\"\"\n",
        "    feature_means = []\n",
        "    for key in settings.keys():\n",
        "        mean = settings[key][\"offset\"]\n",
        "        for freq, coef in zip(\n",
        "            settings[key][\"frequencies\"], settings[key][\"coefficients\"]\n",
        "        ):\n",
        "            mean += coef * (\n",
        "                np.sin(\n",
        "                    freq * np.arange(settings[key][\"features\"]) + settings[key][\"phase\"]\n",
        "                )\n",
        "                + 1\n",
        "            )\n",
        "        feature_means.extend(list(mean))\n",
        "    return feature_means\n",
        "\n",
        "\n",
        "def create_ground_truth_correlations_file(correlations, COR_THRES):\n",
        "    \"\"\"\n",
        "    This function saves the ground truth associations in a Dataframe, which will be\n",
        "    then stored in a tsv file. Ground truth associations are defined to be the\n",
        "    pairs of fatures with a pearson correlation above COR_THRES.\n",
        "\n",
        "    Args:\n",
        "        correlations (np.array): array with all correlations.\n",
        "        COR_THRES (float): threshold for the pearson correlation.\n",
        "\n",
        "    Returns:\n",
        "        associations (pd.DataFrame): dataframe with all associations.\n",
        "    \"\"\"\n",
        "\n",
        "    sort_ids = np.argsort(abs(correlations), axis=None)[::-1]  # 1D: N x C\n",
        "    corr = np.take(correlations, sort_ids)  # 1D: N x C\n",
        "    sig_ids = sort_ids[abs(corr) > COR_THRES]\n",
        "    sig_ids = np.vstack(\n",
        "        (sig_ids // len(all_feature_names), sig_ids % len(all_feature_names))\n",
        "    ).T\n",
        "    associations = pd.DataFrame(sig_ids, columns=[\"feature_a_id\", \"feature_b_id\"])\n",
        "    a_df = pd.DataFrame(dict(feature_a_name=all_feature_names))\n",
        "    a_df.index.name = \"feature_a_id\"\n",
        "    a_df.reset_index(inplace=True)\n",
        "    b_df = pd.DataFrame(dict(feature_b_name=all_feature_names))\n",
        "    b_df.index.name = \"feature_b_id\"\n",
        "    b_df.reset_index(inplace=True)\n",
        "    associations = associations.merge(a_df, on=\"feature_a_id\", how=\"left\").merge(\n",
        "        b_df, on=\"feature_b_id\", how=\"left\"\n",
        "    )\n",
        "    associations[\"Correlation\"] = corr[abs(corr) > COR_THRES]\n",
        "    associations = associations[\n",
        "        associations.feature_a_id > associations.feature_b_id\n",
        "    ]  # Only one half of the matrix\n",
        "    return associations\n",
        "\n",
        "\n",
        "def plot_score_matrix(\n",
        "    array, feature_names, cmap=\"bwr\", vmin=None, vmax=None, label_step=5\n",
        "):\n",
        "    \"\"\"\n",
        "    This function plots a score matrix.\n",
        "\n",
        "    Args:\n",
        "        array (np.array): array with all correlations.\n",
        "        feature_names (list): list with all feature names.\n",
        "\n",
        "    Returns:\n",
        "        fig: fig object to save or show the plot.\n",
        "    \"\"\"\n",
        "    if vmin is None:\n",
        "        vmin = np.min(array)\n",
        "    elif vmax is None:\n",
        "        vmax = np.max(array)\n",
        "    # if ax is None:\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(array, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "    plt.xticks(\n",
        "        np.arange(0, len(feature_names), label_step),\n",
        "        feature_names[::label_step],\n",
        "        fontsize=8,\n",
        "        rotation=90,\n",
        "    )\n",
        "    plt.yticks(\n",
        "        np.arange(0, len(feature_names), label_step),\n",
        "        feature_names[::label_step],\n",
        "        fontsize=8,\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    # ax\n",
        "    return fig\n",
        "\n",
        "def save_splitted_datasets(\n",
        "    settings: dict, PROJECT_NAME, dataset, all_feature_names, n_samples, outpath\n",
        "):\n",
        "    \"\"\"\n",
        "    This function saves the splitted datasets in tsv files.\n",
        "\n",
        "    \"\"\"\n",
        "    # Save index file\n",
        "    index = pd.DataFrame({\"ID\": list(np.arange(1, n_samples + 1))})\n",
        "    index.to_csv(outpath / f\"random.{PROJECT_NAME}.ids.txt\", index=False, header=False)\n",
        "    # Save continuous files\n",
        "    df = pd.DataFrame(\n",
        "        dataset, columns=all_feature_names, index=list(np.arange(1, n_samples + 1))\n",
        "    )\n",
        "    cum_feat = 0\n",
        "    for key in settings.keys():\n",
        "        df_feat = settings[key][\"features\"]\n",
        "        df_cont = df.iloc[:, cum_feat : cum_feat + df_feat]\n",
        "        df_cont.insert(0, \"ID\", np.arange(1, n_samples + 1))\n",
        "        df_cont.to_csv(\n",
        "            outpath / f\"random.{PROJECT_NAME}.{key}.tsv\", sep=\"\\t\", index=False\n",
        "        )\n",
        "        cum_feat += df_feat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf-hC7ckJBWR"
      },
      "source": [
        "**Hyperparameters**\n",
        "\n",
        "We will set the value of a number of hyperparameters to create and store the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anw9f1V4E8SA"
      },
      "outputs": [],
      "source": [
        "########################### Hyperparameters ####################################\n",
        "PROJECT_NAME = \"random_all_sim\"\n",
        "HIGH_CORR = True # Add extra correlations as linear combinations of existing features\n",
        "SEED_1 = 1234 # Keeping the seed for reproducibility\n",
        "np.random.seed(SEED_1) # Setting seed values\n",
        "rnd.seed(SEED_1) # Setting seed values\n",
        "COV_ALPHA = 0.97 # Alpha: hyperparam governing the sparsity of the covariance matrix\n",
        "N_SAMPLES = 1000 # LOW N: 50, High N: 1000\n",
        "\n",
        "# Settings: Dictionary with names and values to create feature profiles.\n",
        "SETTINGS = {\n",
        "    \"Continuous_A\": {\n",
        "        \"features\": 10,\n",
        "        \"frequencies\": [0.002, 0.01, 0.02],\n",
        "        \"coefficients\": [500, 100, 50],\n",
        "        \"phase\": 0,\n",
        "        \"offset\": 500,\n",
        "    },\n",
        "    \"Continuous_B\": {\n",
        "        \"features\": 10,\n",
        "        \"frequencies\": [0.001, 0.05, 0.08],\n",
        "        \"coefficients\": [80, 20, 10],\n",
        "        \"phase\": np.pi / 2,\n",
        "        \"offset\": 400,\n",
        "    },\n",
        "    \"Categorical_A\": {\n",
        "        \"features\": 1,\n",
        "        \"frequencies\": [0.1, 0.5, 0.8],\n",
        "        \"coefficients\": [.2, .1, .05],\n",
        "        \"phase\": np.pi / 2,\n",
        "        \"offset\": 10,\n",
        "    },\n",
        "        \"Categorical_B\": {\n",
        "        \"features\": 1,\n",
        "        \"frequencies\": [0.01, 0.5, 0.08],\n",
        "        \"coefficients\": [10, .1, .05],\n",
        "        \"phase\": np.pi,\n",
        "        \"offset\": 1,\n",
        "    }\n",
        "}\n",
        "\n",
        "COR_THRES = 0.02 # Correlation threshold above which a pair of features is considered to be associated\n",
        "PAIRS_OF_INTEREST = [(1,2),(3,4)]\n",
        "\n",
        "\n",
        "# Path to store output files\n",
        "outpath = Path(\"./synthetic_data_II\")\n",
        "outpath.mkdir(exist_ok=True, parents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAs8RiUMI-5C"
      },
      "source": [
        "**Script**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CVHFlu9YI_Mo",
        "outputId": "830770a1-9dba-43e9-8998-6498fcf8a8bc"
      },
      "outputs": [],
      "source": [
        "###################### Script to create a synthetic dataset ####################\n",
        "\n",
        "# Add all datasets in a single matrix:\n",
        "all_feature_names = get_feature_names(SETTINGS)\n",
        "feat_means = create_mean_profiles(SETTINGS)\n",
        "\n",
        "# Covariance matrix\n",
        "covariance_matrix = make_sparse_spd_matrix(dim=len(all_feature_names), alpha=COV_ALPHA, norm_diag=True,random_state=SEED_1)\n",
        "ABS_MAX = np.max(abs(covariance_matrix))\n",
        "fig = plot_score_matrix(\n",
        "    covariance_matrix, all_feature_names, vmin=-ABS_MAX, vmax=ABS_MAX\n",
        ")\n",
        "fig.savefig(outpath / f\"Covariance_matrix_{PROJECT_NAME}.png\")\n",
        "\n",
        "# No scaling in the dataset creation! It will be handled in preprocessing.\n",
        "scaled_dataset = np.random.multivariate_normal(feat_means, covariance_matrix, N_SAMPLES)\n",
        "\n",
        "# Add extra correlations as linear combinations of existing features\n",
        "if HIGH_CORR: # The last half of the features are combinations of the first half:\n",
        "    for i in range(scaled_dataset.shape[1]//2):\n",
        "        col_1 = np.random.choice(range(scaled_dataset.shape[1]//2))\n",
        "        col_2 = np.random.choice(range(scaled_dataset.shape[1]//2))\n",
        "        scaled_dataset[:,i+scaled_dataset.shape[1]//2] = (scaled_dataset[:,col_1]+scaled_dataset[:,col_2])/2 + np.random.normal()\n",
        "\n",
        "# Binarize the categorical dataset\n",
        "NUM_CAT = SETTINGS[\"Categorical_A\"][\"features\"] + SETTINGS[\"Categorical_B\"][\"features\"]\n",
        "columns_to_binarize = scaled_dataset[:,-NUM_CAT:]\n",
        "\n",
        "# Compute the mean of each of the categorical columns\n",
        "means = columns_to_binarize.mean(axis=0)\n",
        "\n",
        "# Apply the binarization\n",
        "scaled_dataset[:,-NUM_CAT:] = (columns_to_binarize > means).astype(int)\n",
        "\n",
        "# Plot correlations matrix\n",
        "correlations = np.corrcoef(scaled_dataset, rowvar=False)\n",
        "fig = plot_score_matrix(correlations, all_feature_names, vmin=-1, vmax=1, label_step=5)\n",
        "fig.savefig(outpath / f\"Correlations_{PROJECT_NAME}.png\", dpi=200)\n",
        "\n",
        "# Sort correlations by absolute value\n",
        "associations = create_ground_truth_correlations_file(correlations, COR_THRES)\n",
        "associations.to_csv(outpath / f\"changes.{PROJECT_NAME}.txt\", sep=\"\\t\", index=False)\n",
        "\n",
        "# Write tsv files with feature values for all samples in both datasets:\n",
        "save_splitted_datasets(\n",
        "    SETTINGS, PROJECT_NAME, scaled_dataset, all_feature_names, N_SAMPLES, outpath\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpI0CjhjDBpf"
      },
      "source": [
        "## Running MOVE\n",
        "\n",
        "When running MOVE, we read a number of hyperparameters from the configuration (.yaml) files. We will create these configuration files now.\n",
        "\n",
        "### Create config files:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc9RmZZVOGG_"
      },
      "outputs": [],
      "source": [
        "config_paths = [Path(\"./config/data/\"),\n",
        "                Path(\"./config/task/\")]\n",
        "\n",
        "for config_path in config_paths:\n",
        "  config_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "data_yaml_contents = {\n",
        "\"random_continuous_paper_II.yaml\": \"\"\"\n",
        "# DO NOT EDIT DEFAULTS\n",
        "defaults:\n",
        "  - base_data\n",
        "\n",
        "# FEEL FREE TO EDIT BELOW\n",
        "\n",
        "raw_data_path: synthetic_data_II/              # where raw data is stored\n",
        "interim_data_path: interim_data_cont_paper_II/  # where intermediate files will be stored\n",
        "results_path: results_cont_paper_II/     # where result files will be placed\n",
        "\n",
        "sample_names: random.random_all_sim.ids  # names/IDs of each sample, must appear in the\n",
        "                                # other datasets\n",
        "\n",
        "categorical_inputs:\n",
        "  - name: random.random_all_sim.Categorical_A\n",
        "  - name: random.random_all_sim.Categorical_B\n",
        "\n",
        "continuous_inputs:   # a list of continuous datasets\n",
        "  - name: random.random_all_sim.Continuous_A\n",
        "    log2: true\n",
        "    scale: true\n",
        "  - name: random.random_all_sim.Continuous_B\n",
        "    log2: true\n",
        "    scale: true\n",
        "\"\"\"}\n",
        "\n",
        "task_yaml_contents = {\"random_continuous_paper_II__latent.yaml\":\"\"\"\n",
        "\n",
        "defaults:\n",
        "  - analyze_latent\n",
        "\n",
        "batch_size: 10\n",
        "\n",
        "feature_names:\n",
        "  - Continuous_A_1\n",
        "  - Continuous_B_1\n",
        "  - Continuous_B_2\n",
        "  - Continuous_B_3\n",
        "  - Continuous_B_4\n",
        "  - Continuous_B_5\n",
        "  - Continuous_B_6\n",
        "  - Continuous_B_7\n",
        "  - Continuous_B_8\n",
        "  - Categorical_A_1\n",
        "\n",
        "model:\n",
        "  num_hidden:\n",
        "    - 15\n",
        "  num_latent: 3\n",
        "  beta: .0001\n",
        "\n",
        "training_loop:\n",
        "  lr: 1e-4\n",
        "  num_epochs: 300\n",
        "  batch_dilation_steps:\n",
        "    - 100\n",
        "    - 200\n",
        "  kld_warmup_steps:\n",
        "    - 50\n",
        "    - 100\n",
        "    - 125\n",
        "    - 150\n",
        "    - 175\n",
        "    - 200\n",
        "    - 225\n",
        "    - 250\n",
        "    - 275\n",
        "  early_stopping: false\n",
        "  patience: 0 \"\"\",\"\"\"random_continuous_paper_II__id_assoc_ks.yaml\"\"\": \"\"\"\n",
        "defaults:\n",
        "  - identify_associations_ks_schema\n",
        "\n",
        "batch_size: 10\n",
        "\n",
        "num_refits: 1\n",
        "sig_threshold: 0.05\n",
        "\n",
        "target_dataset: random.random_all_sim.Continuous_B\n",
        "target_value: plus_std\n",
        "save_refits: True\n",
        "\n",
        "model:\n",
        "  categorical_weights: ${weights:${data.categorical_inputs}}\n",
        "  continuous_weights: ${weights:${data.continuous_inputs}}\n",
        "  num_hidden:\n",
        "    - 15\n",
        "  num_latent: 3\n",
        "  beta: .0001\n",
        "  dropout: 0.1\n",
        "  cuda: false\n",
        "\n",
        "training_loop:\n",
        "  lr: 1e-4\n",
        "  num_epochs: 300\n",
        "  batch_dilation_steps:\n",
        "    - 100\n",
        "    - 200\n",
        "  kld_warmup_steps:\n",
        "    - 50\n",
        "    - 100\n",
        "    - 125\n",
        "    - 150\n",
        "    - 175\n",
        "    - 200\n",
        "    - 225\n",
        "    - 250\n",
        "    - 275\n",
        "  early_stopping: false\n",
        "  patience: 0\n",
        "\n",
        "perturbed_feature_names:\n",
        "  - Continuous_B_1\n",
        "target_feature_names:\n",
        "  - Continuous_B_1\n",
        "  - Continuous_B_2\n",
        "  - Continuous_B_3\n",
        "  - Continuous_B_4\n",
        "  - Continuous_B_5\n",
        "  - Continuous_B_6\n",
        "  - Continuous_B_7\n",
        "  - Continuous_B_8\n",
        "  - Continuous_B_9\n",
        "  - Continuous_B_10 \"\"\"}\n",
        "\n",
        "\n",
        "for file,content in data_yaml_contents.items():\n",
        "  with open(config_paths[0] / file, 'w') as f:\n",
        "      f.write(content)\n",
        "\n",
        "\n",
        "for file,content in task_yaml_contents.items():\n",
        "  with open(config_paths[1] / file, 'w') as f:\n",
        "      f.write(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI9WvXl4TSdz"
      },
      "source": [
        "You can have a look at the global config file:\n",
        "\n",
        "```\n",
        "./content/config/data/\n",
        "```\n",
        "\n",
        "And the task specific configuration files:\n",
        "```\n",
        "./content/config/data/\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6ZvGAL1NMkj"
      },
      "source": [
        "### Performing the different tasks:\n",
        "\n",
        "Now we will:\n",
        "- encode the data\n",
        "- train MOVE models according to the settings predefined in the configuration files\n",
        "- visualize the latent space\n",
        "- Perform feature importance analyses with SHAP\n",
        "- Perturb the inputs to identify associated variables in the output.\n",
        "\n",
        "We will do it in command-line style, all at once, and we will discuss the results afterwards.\n",
        "\n",
        "\n",
        "> ⏰ This step will take some minutes. Coffee break!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwyFWar6DFIt",
        "outputId": "b7aa085c-921b-4f37-ba64-cdf3ba3b0f88"
      },
      "outputs": [],
      "source": [
        "### Running MOVE on simple synthetic data\n",
        "\n",
        "# Encode data\n",
        "! move-dl task=encode_data data=random_continuous_paper_II\n",
        "\n",
        "# Latent space analysis\n",
        "! move-dl task=random_continuous_paper_II__latent data=random_continuous_paper_II\n",
        "\n",
        "# Identify assoc ks\n",
        "! move-dl task=random_continuous_paper_II__id_assoc_ks data=random_continuous_paper_II"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azr_96MeDG6G"
      },
      "source": [
        "## Analyzing the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swFCMp79YpSQ"
      },
      "source": [
        "### Latent space analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "xbeoNH_TRkxZ",
        "outputId": "c81b653d-0d01-43bc-95cb-b6c8a7ff8c48"
      },
      "outputs": [],
      "source": [
        "results_latent_path = Path(\"./results_cont_paper_II/latent_space/\")\n",
        "img = mpimg.imread(results_latent_path / 'reconstruction_metrics.png')\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "sJH5YrUrYo9x",
        "outputId": "8e3869a1-c035-4f61-cc9e-032fce4a7fc2"
      },
      "outputs": [],
      "source": [
        "results_latent_path = Path(\"./results_cont_paper_II/latent_space/\")\n",
        "img = mpimg.imread(results_latent_path / 'loss_curve.png')\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKm_TeqUvAsS"
      },
      "source": [
        "**Loss curves:**\n",
        "\n",
        "The overall loss is a combination of Cross-Entropy for the categorical variables, SSE for continuous variables and a KLD term to enforce the sample distribution in latent space to be like our prior, a Normal Gaussian.\n",
        "\n",
        "Note:\n",
        "1. We are under a low regularization regime, KLD has no influence on the latent space distribution of samples.\n",
        "2. The model benefits a lot from reconstructing properly the categoical variables, seen as a Cross-Entropy loss going to zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "97tZFiQ1vz9P",
        "outputId": "b5185ea4-f898-4087-e9f7-2c863e4d586c"
      },
      "outputs": [],
      "source": [
        "categorical_variable = \"Categorical_A_1\"\n",
        "continuous_variables = [\"Continuous_B_6\", \"Continuous_B_8\"]\n",
        "\n",
        "# Categorical variable:\n",
        "results_latent_path = Path(\"./results_cont_paper_II/latent_space/\")\n",
        "img = mpimg.imread(results_latent_path / f'latent_space_{categorical_variable}.png')\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Continuous variable I:\n",
        "results_latent_path = Path(\"./results_cont_paper_II/latent_space/\")\n",
        "img = mpimg.imread(results_latent_path / f'latent_space_{continuous_variables[0]}.png')\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Continuous variable II:\n",
        "results_latent_path = Path(\"./results_cont_paper_II/latent_space/\")\n",
        "img = mpimg.imread(results_latent_path / f'latent_space_{continuous_variables[1]}.png')\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5QYOlcAwsk7"
      },
      "source": [
        "**Latent space visualization:**\n",
        "\n",
        "This is a Umap representation of our 3D latent space.\n",
        "\n",
        "*Samples are ordered in clusters according to their categorical labels*\n",
        "\n",
        "We can already see that the network splits the latent space into clusters to perfectly classify the samples according to their categorical features. In this case, we have 2 categorical variables with two classes each, yielding 4 clasters with unique labels.\n",
        "\n",
        "*Samples are ordered following value gradients of learned continuous variables*\n",
        "\n",
        "Here the Umap representations provide a hint but do not help to illustrate this idea. We'll clearly see this behavior in the next section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "QWkBiX4NyKrV",
        "outputId": "56858174-616d-44b1-d44f-ff8973ebd1b7"
      },
      "outputs": [],
      "source": [
        "categorical_dataset = \"random.random_all_sim.Categorical_A\"\n",
        "continuous_dataset = \"random.random_all_sim.Continuous_B\"\n",
        "\n",
        "# Categorical variable:\n",
        "results_latent_path = Path(\"./results_cont_paper_II/latent_space/\")\n",
        "img = mpimg.imread(results_latent_path / f'feat_importance_{categorical_dataset}.png')\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Continuous variable:\n",
        "results_latent_path = Path(\"./results_cont_paper_II/latent_space/\")\n",
        "img = mpimg.imread(results_latent_path / f'feat_importance_{continuous_dataset}.png')\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgCLHqmazAK8"
      },
      "source": [
        "**Feature importance analysis: SHAP**\n",
        "\n",
        "The results obtained from SHAP analysis are a direct consequence of MOVE's way of organizing samples in latent space. Each dot in the plot corresponds to a sample, color coded by its label (top) for the categorical variable being shown, or its feature value of the continuous variable being shown. On the x axis we see the impact on latent space, which corresponds to the displacement of each sample when setting the value of the feature of interest to 0, sample by sample.\n",
        "\n",
        "Note that the most important features are learned, i.e. samples with similar values move similarly. In addition, samples move more when removing these features. At the bottom of the list we find variables that either varied faster, i.e. in shorter distances, or variables that the model ignored.\n",
        "\n",
        "\n",
        "> **Theoretical insights:**\n",
        ">\n",
        ">An adaptation of the SHAP algorithm was used to define the importance that the model attributed to each input feature. The input dataset $X$, a matrix with $N_{samples}$ rows and $N_{features}$ columns, served as a reference and was encoded into its latent representation $z$, with $N_{samples}$ rows and $N_{latent}$ columns.  A perturbed dataset $X’$,  where the feature of interest had been substituted in all samples for the missing value (i.e. 0), was encoded into its latent representation $z’$. The induced movement of the samples in latent space was then computed as the Euclidean distance $d$ between both encodings, i.e. $d = z’ - z$. Finally, the overall movement of each sample was obtained by adding the movements in all latent components, as a sum of all elements in each row. This protocol was repeated for each feature, one by one, and the 10 features with the largest absolute sum difference across samples were taken to be the most important ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va9TJIJzuRDy"
      },
      "source": [
        "### Visualizing the perturbations\n",
        "\n",
        "UMAP and t-SNE distort and compress dimensions to ease visualization. To get the real, exact latent space, we trained MOVE to compress the inputs to 3 latent dimensions, which we can directly visualize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8SPVw1z5a6E"
      },
      "outputs": [],
      "source": [
        "def plot_3D_latent_and_displacement(\n",
        "    mu_baseline,\n",
        "    mu_perturbed,\n",
        "    feature_values,\n",
        "    feature_name,\n",
        "    show_baseline=True,\n",
        "    show_perturbed=True,\n",
        "    show_arrows=True,\n",
        "    step: int=1,\n",
        "    altitude: int=30,\n",
        "    azimuth: int=45,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot the movement of the samples in the 3D latent space after perturbing one\n",
        "    input variable.\n",
        "\n",
        "    Args:\n",
        "        mu_baseline:\n",
        "            ND array with dimensions n_samples x n_latent_nodes containing\n",
        "            the latent representation of each sample\n",
        "        mu_perturbed:\n",
        "            ND array with dimensions n_samples x n_latent_nodes containing\n",
        "            the latent representation of each sample after perturbing the input\n",
        "        feature_values:\n",
        "            1D array with feature values to map to a colormap (\"bwr\"). Each sample is\n",
        "            colored according to its value for the feature of interest.\n",
        "        feature_name:\n",
        "            name of the feature mapped to a colormap\n",
        "        show_baseline:\n",
        "            plot orginal location of the samples in the latent space\n",
        "        show_perturbed:\n",
        "            plot final location (after perturbation) of the samples in latent space\n",
        "        show_arrows:\n",
        "            plot arrows from original to final location of each sample\n",
        "        angle:\n",
        "            elevation from dim1-dim2 plane for the visualization of latent space.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If latent space is not 3-dimensional (3 hidden nodes).\n",
        "    Returns:\n",
        "        Figure\n",
        "    \"\"\"\n",
        "\n",
        "    my_cmap = sns.color_palette(\"RdYlBu\", as_cmap=True)\n",
        "\n",
        "    eps = 1e-16\n",
        "    if [np.shape(mu_baseline)[1], np.shape(mu_perturbed)[1]] != [3, 3]:\n",
        "        raise ValueError(\n",
        "            \" The latent space must be 3-dimensional. Redefine num_latent to 3.\"\n",
        "        )\n",
        "\n",
        "    fig = plt.figure(layout=\"constrained\", figsize=(7, 7))\n",
        "    ax = fig.add_subplot(projection=\"3d\")\n",
        "    ax.view_init(altitude, azimuth)\n",
        "\n",
        "    if show_baseline:\n",
        "        vmin, vmax = np.min(feature_values[::step]), np.max(feature_values[::step])\n",
        "        abs_max = np.max([abs(vmin), abs(vmax)])\n",
        "        ax.scatter(\n",
        "            mu_baseline[::step, 0],\n",
        "            mu_baseline[::step, 1],\n",
        "            mu_baseline[::step, 2],\n",
        "            marker=\"o\",\n",
        "            c=feature_values[::step],\n",
        "            s=15,\n",
        "            lw=0,\n",
        "            cmap=my_cmap,\n",
        "        )\n",
        "        ax.set_title(feature_name)\n",
        "\n",
        "    if show_perturbed:\n",
        "        ax.scatter(\n",
        "            mu_perturbed[::step, 0],\n",
        "            mu_perturbed[::step, 1],\n",
        "            mu_perturbed[::step, 2],\n",
        "            marker=\"o\",\n",
        "            c=feature_values[::step],\n",
        "            s=15,\n",
        "            label=\"perturbed\",\n",
        "            lw=0,\n",
        "        )\n",
        "    if show_arrows:\n",
        "        u = mu_perturbed[::step, 0] - mu_baseline[::step, 0]\n",
        "        v = mu_perturbed[::step, 1] - mu_baseline[::step, 1]\n",
        "        w = mu_perturbed[::step, 2] - mu_baseline[::step, 2]\n",
        "\n",
        "        module = np.sqrt(u * u + v * v + w * w)\n",
        "\n",
        "        mask = module > eps\n",
        "\n",
        "        max_u, max_v, max_w = np.max(abs(u)), np.max(abs(v)), np.max(abs(w))\n",
        "\n",
        "        # Arrow colors will be weighted contributions of red -> dim1, green -> dim2, and blue-> dim3. I.e. purple arrow means movement in dims 1 and 3\n",
        "        colors = [\n",
        "            (abs(du) / max_u, abs(dv) / max_v, abs(dw) / max_w, 0.7)\n",
        "            for du, dv, dw in zip(u, v, w)\n",
        "        ]\n",
        "        ax.quiver(\n",
        "            mu_baseline[::step, 0][mask],\n",
        "            mu_baseline[::step, 1][mask],\n",
        "            mu_baseline[::step, 2][mask],\n",
        "            u[mask],\n",
        "            v[mask],\n",
        "            w[mask],\n",
        "            color=colors,\n",
        "            lw=.8,\n",
        "            )\n",
        "    ax.set_xlabel(\"Dim 1\")\n",
        "    ax.set_ylabel(\"Dim 2\")\n",
        "    ax.set_zlabel(\"Dim 3\")\n",
        "\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5NGBdcbuTdO"
      },
      "outputs": [],
      "source": [
        "! mkdir -p figures\n",
        "feature_list = [(\"Categorical_A\", 1),(\"Continuous_B\",8)]\n",
        "figure_path = Path(\"./figures/\")\n",
        "results_path = Path(\"./results/identify_associations/\")\n",
        "\n",
        "# Load latent space locations: Shape = (N_samples, N_latent, N_perturb +1)\n",
        "latent_space_location = np.load(\"./results_cont_paper_II/identify_associations/latent_location.npy\")\n",
        "latent_space_baseline = latent_space_location[:,:,-1]\n",
        "\n",
        "for (dataset, feature) in feature_list:\n",
        "    feature_values = pd.read_csv(f\"./synthetic_data_II/random.random_all_sim.{dataset}.tsv\", sep=\"\\t\")\n",
        "    feature_values = feature_values[dataset + '_' + str(feature)].values\n",
        "\n",
        "    # # Plot latent space:\n",
        "    pic_num = 0\n",
        "    n_pictures = 50\n",
        "\n",
        "    for azimuth, altitude in zip(\n",
        "        np.linspace(0, 60, n_pictures), np.linspace(15, 60, n_pictures)\n",
        "    ):\n",
        "\n",
        "        title = dataset + '_' + str(feature)\n",
        "\n",
        "        fig = plot_3D_latent_and_displacement(\n",
        "            latent_space_baseline,\n",
        "            latent_space_baseline,\n",
        "            feature_values=feature_values,\n",
        "            feature_name=f\"Sample movement\",\n",
        "            show_baseline=True,\n",
        "            show_perturbed=False,\n",
        "            show_arrows=False,\n",
        "            step=1,\n",
        "            altitude=altitude,\n",
        "            azimuth=azimuth,\n",
        "        )\n",
        "\n",
        "        fig.savefig(figure_path / f\"3D_latent_movement_{pic_num}_perturbed_feature.png\", dpi=100)\n",
        "        plt.close(fig)\n",
        "\n",
        "        if \"Continuous\" in dataset:\n",
        "            latent_space_perturbed = latent_space_location[:,:,feature-1]\n",
        "            fig = plot_3D_latent_and_displacement(\n",
        "                latent_space_baseline,\n",
        "                latent_space_perturbed,\n",
        "                feature_values=feature_values,\n",
        "                feature_name=f\"{title}\",\n",
        "                show_baseline=False,\n",
        "                show_perturbed=False,\n",
        "                show_arrows=True,\n",
        "                altitude=altitude,\n",
        "                azimuth=azimuth,\n",
        "            )\n",
        "            fig.savefig(figure_path / f\"3D_latent_movement_{pic_num}_arrows.png\", dpi=100)\n",
        "            plt.close(fig)\n",
        "\n",
        "        pic_num += 1\n",
        "\n",
        "\n",
        "    # Creating gifs\n",
        "    plot_types = [\"arrows\", \"perturbed_feature\"] if \"Continuous\" in dataset else [\"perturbed_feature\"]\n",
        "    for plot_type in plot_types:\n",
        "        frames = [\n",
        "            Image.open(figure_path / f\"3D_latent_movement_{pic_num}_{plot_type}.png\")\n",
        "            for pic_num in range(n_pictures)\n",
        "        ]  # sorted(glob.glob(\"*3D_latent*\"))]\n",
        "        frames[0].save(\n",
        "            figure_path / f\"{plot_type}_{title}.gif\",\n",
        "            format=\"GIF\",\n",
        "            append_images=frames[1:],\n",
        "            save_all=True,\n",
        "            duration=75,\n",
        "            loop=0,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i19TAgs7DISP"
      },
      "source": [
        "**Visualize gifs:**\n",
        "\n",
        "\n",
        "\n",
        "We started with multimodal samples, for which we simulated the measurement of 20 continuous features and 2 categorical features. Then we used MOVE to compress the representations of these samples to 3 dimensions in the network's latent layer. We will now visualize the real, complete 3D latent space.\n",
        "\n",
        "Here, each input sample is represented by a small sphere. Each sample's latent representation $z$ is a vector with three components, where each component corresponds to the value of a latent node.\n",
        "\n",
        "For example, sample 1 might have a latent vector $z = (\\text{Value of node 1},\\text{Value of node 2}, \\text{Value of node 3}) = (0.5,-5,-1)$. This sample would then be plotted at 0.5 for dimension 1, -5 for dimension 2, etc.\n",
        "\n",
        "\n",
        "We will plot:\n",
        "\n",
        "1) The latent space where each sample is color coded by the categorical label of Categorical_A_1, which has two possible classes.\n",
        "\n",
        "2) The latent space where each sample is color coded by the value of the continuous feature Continuous_B_8.\n",
        "\n",
        "3) The movement of each sample when adding a small perturbation (1 std) to the original value of Continuous_B_8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wu7gmLaI--Ez",
        "outputId": "0df8fbcf-80b7-4f17-f4ac-218ab9c1a031"
      },
      "outputs": [],
      "source": [
        "# Path to your local GIF file\n",
        "gif_list = ['perturbed_feature_Categorical_A_1.gif', 'perturbed_feature_Continuous_B_8.gif','arrows_Continuous_B_8.gif' ]\n",
        "gif_path = 'figures/perturbed_feature_Categorical_A_1.gif'\n",
        "\n",
        "# Display the GIF\n",
        "\n",
        "for gif_path in gif_list:\n",
        "  display(Image_display(filename=\"figures/\" + gif_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9ttU5N5CFRF"
      },
      "source": [
        "Here we can clearly see that:\n",
        "\n",
        "1. MOVE orders samples in clusters according to their categorical labels.\n",
        "2. MOVE orders samples in latent space following value gradients of learned continuous features.\n",
        "3. When performing perturbations on learned continuous variables, the latent representations of the samples move following the local value gradient of the perturbed feature. Of note, they do not necessarily all move in the same direction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0asa40SkFl5-"
      },
      "source": [
        "## Final remarks\n",
        "\n",
        "Thanks for making it this far!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
