{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> On the use of VAEs for biomedical data integration <center> \n",
    "\n",
    "## Introduction\n",
    "    \n",
    "In this project we will use MOVE (Multi-Omics Variational Autoencoder) to explore the power and potential pitfalls when using VAEs to perform multimodal data integration in biomedical settings.\n",
    "\n",
    "We will cover:\n",
    "- Latent space organization of the samples.\n",
    "- SHAP analyses to identify relevant features.\n",
    "- _In silico_ perturbations of the inputs to identify associations.\n",
    "\n",
    "We will finally show how all of these concepts can be applied to real biomedical setting, namely the study of the microbial ecosystem changes in patients with Inflammatory Bowel Diseases (IBD).\n",
    "\n",
    "**Papers of interest:**\n",
    "    \n",
    "- AllesÃ¸e, R.L., Lundgaard, A.T., HernÃ¡ndez Medina, R. *et al*. Discovery of\n",
    "drugâ€“omics associations in type 2 diabetes with generative deep-learning models.*Nat Biotechnol* (2023). https://www.nature.com/articles/s41587-022-01520-x\n",
    "- Diederik P Kingma, Max Welling. Auto-Encoding Variational Bayes. https://arxiv.org/abs/1312.6114\n",
    "- Lloyd-Price, J., Arze, C., Ananthakrishnan, A.N. et al. Multi-omics of the gut microbial ecosystem in inflammatory bowel diseases. Nature 569, 655â€“662 (2019). https://doi.org/10.1038/s41586-019-1237-9\n",
    "\n",
    "**About MOVE:**\n",
    "\n",
    "MOVE is a Variational AutoEncoder, i.e., it is a neural network trained to compress the information and reconstruct it as accurately as possible. The variational part of it makes it more robust to noise in the input, allows us to treat it from a bayesian perspective, and enables us to use it as a generative model, i.e. we can generate new samples.\n",
    "\n",
    "MOVE applies *in silico* perturbations to the input in order to find associations between features.\n",
    "\n",
    "**The architecture of the model:**\n",
    "\n",
    "MOVE consists in three parts: an encoder, a decoder, and a prior distribution set to be normal.\n",
    "\n",
    "The encoder will compress the input information and extract shared information between features. The encoder is composed by the input layer, a vector of feature values for a given sample; hidden layers connecting the different features, and a latent layer. This latent layer will contain a compressed representation of our data (lower dimensionality feature vector). Each sample will hence have an assigned latent representation, i.e. it will lie somewhere in what we call \"the latent space\". The decoder will then reconstruct the input, recovering the high dimensional vector that constituted our sample.\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a designated environment\n",
    "\n",
    "We will first create an environment where we will store all the required packages, including MOVE. Make sure to run this notebook from said environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda create -n move_env python=3.9 --y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Note:_\n",
    "> You might need to install ipykernel from the terminal to be able to run the cells in the notebook. You can do that as follows:\n",
    "> ```\n",
    "> pip install ipykernel\n",
    ">```\n",
    "\n",
    "**Installing MOVE:**\n",
    "\n",
    "MOVE can be found online at RasmussenLab's Github repository (https://github.com/RasmussenLab/MOVE). It can be downloaded as a pip package from the command line as follows:\n",
    "```bash\n",
    "pip install move-dl\n",
    "```\n",
    "\n",
    "> âš ï¸ **MOVE source code edits**\n",
    ">\n",
    "> A few edits to MOVE's code were made for this project. These can be found in the python notebook ```MOVE_edits.ipynb``` and can be summarized as follows:\n",
    ">\n",
    "> 1) **Removing feature_mask:**\n",
    ">   When identifying associations (```identify_associations.py```) , feature_masks were removed and we only kept nan_masks.\n",
    ">\n",
    "> 2) **Corrected Cumulative distributions:**\n",
    "> Cumulative distributions obtained when using the KS method were not properly normalyzed. We now multiply by bin_width to obtain a distribution between 0-1. plot_cumulative function found in ```dataset_distributions.py```\n",
    ">\n",
    "> 3) **Check categorical reconstructions after perturbations:**\n",
    "> We noticed that categorical perturbations in the inputs induced tiny movements of the representations of the samples in latent space. Since samples did not abandon the cluster of origin, we suspected that the reconstructions would not recapitulate the perturbed state, but rather go back to the class of origin. To do that, we added a couple of lines to store the true targets and predicted targets for both baseline and perturbed inputs when identifying associations using the Bayes approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVE can be installed as a pip package:\n",
    "! pip install move-dl matplotlib_venn upsetplot biom-format wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages\n",
    "The first step is to load all third party packages required to perform the different tasks in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import random as rnd\n",
    "from sklearn.datasets import make_sparse_spd_matrix\n",
    "import argparse\n",
    "import wget\n",
    "import biom\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from matplotlib_venn import venn2, venn3\n",
    "from upsetplot import UpSet\n",
    "from matplotlib import cm\n",
    "\n",
    "# Reset all matplotlib settings to defaults\n",
    "plt.rcdefaults()\n",
    "plt.rcParams['font.family'] = 'Times New Roman'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sythetic datasets benchmark\n",
    "\n",
    "### Synthetic dataset creation\n",
    "\n",
    "A synthetic dataset is created as a multivariate gaussian, where different features are different components of the Gaussian and each sample is a draw from the distribution. Explicit stronger correlations can be added by defining some features to be linear combinations of others. Categorical variables (binary) can be obtained by setting negative values to zero and positive values to one for a given feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Functions ####################################\n",
    "def get_feature_names(settings):\n",
    "    all_feature_names = [\n",
    "        f\"{key}_{i+1}\"\n",
    "        for key in settings.keys()\n",
    "        for i in range(settings[key][\"features\"])\n",
    "    ]\n",
    "    return all_feature_names\n",
    "\n",
    "\n",
    "def create_mean_profiles(settings):\n",
    "    feature_means = []\n",
    "    for key in settings.keys():\n",
    "        mean = settings[key][\"offset\"]\n",
    "        for freq, coef in zip(\n",
    "            settings[key][\"frequencies\"], settings[key][\"coefficients\"]\n",
    "        ):\n",
    "            mean += coef * (\n",
    "                np.sin(\n",
    "                    freq * np.arange(settings[key][\"features\"]) + settings[key][\"phase\"]\n",
    "                )\n",
    "                + 1\n",
    "            )\n",
    "        feature_means.extend(list(mean))\n",
    "    return feature_means\n",
    "\n",
    "\n",
    "def create_ground_truth_correlations_file(correlations):\n",
    "    sort_ids = np.argsort(abs(correlations), axis=None)[::-1]  # 1D: N x C\n",
    "    corr = np.take(correlations, sort_ids)  # 1D: N x C\n",
    "    sig_ids = sort_ids[abs(corr) > COR_THRES]\n",
    "    sig_ids = np.vstack(\n",
    "        (sig_ids // len(all_feature_names), sig_ids % len(all_feature_names))\n",
    "    ).T\n",
    "    associations = pd.DataFrame(sig_ids, columns=[\"feature_a_id\", \"feature_b_id\"])\n",
    "    a_df = pd.DataFrame(dict(feature_a_name=all_feature_names))\n",
    "    a_df.index.name = \"feature_a_id\"\n",
    "    a_df.reset_index(inplace=True)\n",
    "    b_df = pd.DataFrame(dict(feature_b_name=all_feature_names))\n",
    "    b_df.index.name = \"feature_b_id\"\n",
    "    b_df.reset_index(inplace=True)\n",
    "    associations = associations.merge(a_df, on=\"feature_a_id\", how=\"left\").merge(\n",
    "        b_df, on=\"feature_b_id\", how=\"left\"\n",
    "    )\n",
    "    associations[\"Correlation\"] = corr[abs(corr) > COR_THRES]\n",
    "    associations = associations[\n",
    "        associations.feature_a_id > associations.feature_b_id\n",
    "    ]  # Only one half of the matrix\n",
    "    return associations\n",
    "\n",
    "\n",
    "def plot_score_matrix(\n",
    "    array, feature_names, cmap=\"bwr\", vmin=None, vmax=None, label_step=5\n",
    "):\n",
    "    if vmin is None:\n",
    "        vmin = np.min(array)\n",
    "    elif vmax is None:\n",
    "        vmax = np.max(array)\n",
    "    # if ax is None:\n",
    "    fig = plt.figure(figsize=(3.5, 3.5))\n",
    "    plt.imshow(array, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    plt.xticks(\n",
    "        np.arange(0, len(feature_names), label_step),\n",
    "        feature_names[::label_step],\n",
    "        fontsize=10,\n",
    "        rotation=90,\n",
    "    )\n",
    "    plt.yticks(\n",
    "        np.arange(0, len(feature_names), label_step),\n",
    "        feature_names[::label_step],\n",
    "        fontsize=10,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    # ax\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_feature_profiles(dataset, feature_means):\n",
    "    ## Plot profiles\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    plt.plot(\n",
    "        np.arange(len(feature_means)), feature_means, lw=1, marker=\".\", markersize=0\n",
    "    )\n",
    "    for sample in dataset:\n",
    "        plt.plot(\n",
    "            np.arange(len(feature_means)), sample, lw=0.1, marker=\".\", markersize=0\n",
    "        )\n",
    "    plt.xlabel(\"Feature number\")\n",
    "    plt.ylabel(\"Count number\")\n",
    "    plt.title(\"Patient specific profiles\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_feature_correlations(dataset, pairs_2_plot):\n",
    "    fig = plt.figure()\n",
    "    for f1, f2 in pairs_2_plot:\n",
    "        plt.plot(\n",
    "            dataset[:, f1],\n",
    "            dataset[:, f2],\n",
    "            lw=0,\n",
    "            marker=\".\",\n",
    "            markersize=1,\n",
    "            label=f\"{correlations[f1,f2]:.2f}\",\n",
    "        )\n",
    "    plt.xlabel(\"Feature A\")\n",
    "    plt.ylabel(\"Feature B\")\n",
    "    plt.legend(\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, -0.1),\n",
    "        fancybox=True,\n",
    "        shadow=True,\n",
    "        ncol=5,\n",
    "    )\n",
    "    plt.title(\"Feature correlations\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def save_splitted_datasets(\n",
    "    settings: dict, PROJECT_NAME, dataset, all_feature_names, n_samples, outpath\n",
    "):\n",
    "    # Save index file\n",
    "    index = pd.DataFrame({\"ID\": list(np.arange(1, n_samples + 1))})\n",
    "    index.to_csv(outpath / f\"random.{PROJECT_NAME}.ids.txt\", index=False, header=False)\n",
    "    # Save continuous files\n",
    "    df = pd.DataFrame(\n",
    "        dataset, columns=all_feature_names, index=list(np.arange(1, n_samples + 1))\n",
    "    )\n",
    "    cum_feat = 0\n",
    "    for key in settings.keys():\n",
    "        df_feat = settings[key][\"features\"]\n",
    "        df_cont = df.iloc[:, cum_feat : cum_feat + df_feat]\n",
    "        df_cont.insert(0, \"ID\", np.arange(1, n_samples + 1))\n",
    "        df_cont.to_csv(\n",
    "            outpath / f\"random.{PROJECT_NAME}.{key}.tsv\", sep=\"\\t\", index=False\n",
    "        )\n",
    "        cum_feat += df_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create high dimensional sythetic dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Hyperparameters ####################################\n",
    "PROJECT_NAME = \"random_all_sim\"\n",
    "MODE = \"linear\"  # \"non-linear\"\n",
    "HIGH_CORR = True\n",
    "SEED_1 = 1 #1234\n",
    "np.random.seed(SEED_1)\n",
    "rnd.seed(SEED_1)\n",
    "\n",
    "COV_ALPHA = .99 #0.99 #.01 \n",
    "N_SAMPLES = 5000\n",
    "\n",
    "SETTINGS = {\n",
    "    \"Continuous_A\": {\n",
    "        \"features\": 50,\n",
    "        \"frequencies\": [0.002, 0.01, 0.02],\n",
    "        \"coefficients\": [500, 100, 50],\n",
    "        \"phase\": 0,\n",
    "        \"offset\": 700,\n",
    "    },\n",
    "    \"Continuous_B\": {\n",
    "        \"features\": 100,\n",
    "        \"frequencies\": [0.001, 0.05, 0.08],\n",
    "        \"coefficients\": [80, 20, 10],\n",
    "        \"phase\": np.pi / 2,\n",
    "        \"offset\": 300,\n",
    "    },\n",
    "    \"Categorical_A\": {\n",
    "        \"features\": 1,\n",
    "        \"frequencies\": [0.1, 0.5, 0.8],\n",
    "        \"coefficients\": [.2, .1, .05],\n",
    "        \"phase\": np.pi / 2,\n",
    "        \"offset\": 10,\n",
    "    },\n",
    "        \"Categorical_B\": {\n",
    "        \"features\": 1,\n",
    "        \"frequencies\": [0.01, 0.5, 0.08],\n",
    "        \"coefficients\": [10, .1, .05],\n",
    "        \"phase\": np.pi,\n",
    "        \"offset\": 10,\n",
    "    }\n",
    "}\n",
    "\n",
    "COR_THRES = 0.02\n",
    "PAIRS_OF_INTEREST = [(1,2),(3,4)]  # ,(77,75),(99,70),(38,2),(67,62)]\n",
    "\n",
    "# Path to store output files\n",
    "outpath = Path(\"./synthetic_data/\")\n",
    "outpath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "################################## Main script ##################################\n",
    "# %%\n",
    "# Add all datasets in a single matrix:\n",
    "all_feature_names = get_feature_names(SETTINGS)\n",
    "feat_means = create_mean_profiles(SETTINGS)\n",
    "\n",
    "# %%\n",
    "###### Covariance matrix definition ######\n",
    "if MODE == \"linear\":\n",
    "    covariance_matrix = make_sparse_spd_matrix(\n",
    "        n_dim=len(all_feature_names),\n",
    "        alpha=COV_ALPHA,\n",
    "        smallest_coef=0,\n",
    "        largest_coef=1,\n",
    "        norm_diag=False,\n",
    "        random_state=SEED_1,\n",
    "    )\n",
    "elif MODE == \"non-linear\":\n",
    "    covariance_matrix = np.identity(len(all_feature_names))\n",
    "\n",
    "ABS_MAX = np.max(abs(covariance_matrix))\n",
    "fig = plot_score_matrix(\n",
    "    covariance_matrix, all_feature_names, vmin=-ABS_MAX, vmax=ABS_MAX\n",
    ")\n",
    "fig.savefig(outpath / f\"Covariance_matrix_{PROJECT_NAME}.svg\")\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = np.random.multivariate_normal(feat_means, covariance_matrix, N_SAMPLES)\n",
    "\n",
    "# Add non-linearities\n",
    "if MODE == \"non-linear\":\n",
    "    for i, j in PAIRS_OF_INTEREST:\n",
    "        freq = np.random.choice([4, 5, 6])\n",
    "        dataset[:, i] += np.sin(freq * dataset[:, j])\n",
    "\n",
    "\n",
    "# No scaling in the dataset creation! It will be handled in preprocessing.\n",
    "scaled_dataset = dataset\n",
    "\n",
    "if HIGH_CORR: # The last half of the features are combinations of the first half:\n",
    "    for i in range(scaled_dataset.shape[1]//2):\n",
    "        col_1 = np.random.choice(range(scaled_dataset.shape[1]//2))\n",
    "        col_2 = np.random.choice(range(scaled_dataset.shape[1]//2))\n",
    "        scaled_dataset[:,i+scaled_dataset.shape[1]//2] = (scaled_dataset[:,col_1]+scaled_dataset[:,col_2])/2 + np.random.normal()\n",
    "\n",
    "# Binarize the categorical dataset\n",
    "NUM_CAT = SETTINGS[\"Categorical_A\"][\"features\"] + SETTINGS[\"Categorical_B\"][\"features\"]\n",
    "columns_to_binarize = scaled_dataset[:,-NUM_CAT:]\n",
    "\n",
    "# Compute the mean of each of the categorical columns    \n",
    "means = columns_to_binarize.mean(axis=0)\n",
    "\n",
    "# Apply the binarization\n",
    "scaled_dataset[:,-NUM_CAT:] = (columns_to_binarize > means).astype(int)\n",
    "\n",
    "print(np.min(scaled_dataset),np.max(scaled_dataset))\n",
    "# Actual correlations:\n",
    "# correlations = np.empty(np.shape(covariance_matrix))\n",
    "# for ifeat in range(len(covariance_matrix)):\n",
    "#     for jfeat in range(len(covariance_matrix)):\n",
    "#         correlations[ifeat, jfeat] = pearsonr(dataset[:, ifeat], dataset[:, jfeat])[\n",
    "#             0\n",
    "#         ]\n",
    "\n",
    "correlations = np.corrcoef(scaled_dataset, rowvar=False)\n",
    "fig = plot_score_matrix(correlations, all_feature_names, vmin=-1, vmax=1, label_step=20)\n",
    "fig.savefig(outpath / f\"Correlations_{PROJECT_NAME}.svg\", dpi=200)\n",
    "\n",
    "# Sort correlations by absolute value\n",
    "associations = create_ground_truth_correlations_file(correlations)\n",
    "associations.to_csv(outpath / f\"changes.{PROJECT_NAME}.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Plot feature profiles per sample\n",
    "fig = plot_feature_profiles(scaled_dataset, feat_means)\n",
    "fig.savefig(outpath / \"Multi-omic_profiles.svg\")\n",
    "\n",
    "## Plot correlations\n",
    "fig = plot_feature_correlations(dataset, PAIRS_OF_INTEREST)\n",
    "fig.savefig(outpath / \"Feature_correlations.svg\")\n",
    "\n",
    "fig = plot_feature_correlations(scaled_dataset, PAIRS_OF_INTEREST)\n",
    "fig.savefig(outpath / \"Feature_correlations_scaled.svg\")\n",
    "\n",
    "# Write tsv files with feature values for all samples in both datasets:\n",
    "save_splitted_datasets(\n",
    "    SETTINGS, PROJECT_NAME, scaled_dataset, all_feature_names, N_SAMPLES, outpath\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOVE \n",
    "\n",
    "The MOVE pipeline consists in the following steps: \n",
    "1) **Encoding the data**\n",
    "2) **Analyze latent space**\n",
    "3) **Identify associations between features**\n",
    "- Associations between a given categorical feature (e.g. Is the patient taking a drug? Yes / No) against the contiuous features.\n",
    "- Associations between a given continuous feature vs. the rest of continuous features.\n",
    "\n",
    "The following description of the different steps is mostly taken from the [NNFC workshop](https://github.com/RasmussenLab/MOVE/tree/nnfc-workshop), feel free to look at that as well!\n",
    "\n",
    "### 1) Encode data\n",
    "\n",
    "The first step in order to run MOVE is to encode the data in a format that the model can understand.\n",
    "\n",
    "To encode the data we store all datasets in a TSV format. Each table needs to have a shape `N` &times; `M`, i.e. `N` rows and `M` columns where `N` is the number of samples/individuals and `M` is the number of features.\n",
    "\n",
    "> ðŸ“¡ **How is data encoded?**\n",
    ">\n",
    "> **_Categorical data is one-hot encoded._** _For a feature like drug status , which has discrete values/categories (e.g., yes or no), we encode these categories as_ binary bit flags. _This means each category is assigned a value starting with one, and then represented in binary format (with zeros and ones)._\n",
    ">\n",
    ">_A useful property of flags is that they do not have hierarchy; they are incompatible with \"<\" or \">\" operators. So (in our example), yes would not be considered more or less important than no._\n",
    ">\n",
    "> **_Continuous data can be z-score normalized_**, _meaning that each feature can be rescaled to have zero mean and unit variance:_\n",
    ">\n",
    ">  $$ Z = \\frac{x-\\mu}{\\sigma} $$\n",
    "> Where $x$ is the vector of feature values for all samples, $\\mu$ and $\\sigma$ its mean and standard deviation, respectively.\n",
    "> We can either transform the data beforehand or set the config flags for log2 transformation and scaling to True.\n",
    "\n",
    "\n",
    "The first step is to read the configuration called `random_continuous_paper` and specify the pre-defined task called `encode_data`.\n",
    "\n",
    "âš ï¸ Remember that the notebook takes user-defined configs in a `config/data` directory located in the current working directory.\n",
    "\n",
    "\n",
    "### 2) Latent space analyses:\n",
    "\n",
    "This section trains MOVE to integrate the data into a latent space. We will then plot the results and find the important variables for the integration using SHAP analysis.\n",
    "\n",
    "> â„¹ï¸ About SHAP analysis.\n",
    ">\n",
    "> There are many ways to identify the most important features in the data, or the set of features that the model will use the most when encoding the data into a compressed/latent representation. One of them is SHAP (SHapley Additive exPlanations) analysis.\n",
    ">\n",
    "> This method measures how much do samples move in latent space when removing one variable at a time from the input. If the model gives a lot of importance to an input variable, e.g. the concentration of a metabolite, removing it from the input will lead to a significant movement of the samples in latent space (i.e., wide band in the SHAP plots, impact on latent space). On the other hand, if an input variable is not really needed, the model will \"ignore\" it and hence the latent space representation of the samples will not change much when that feature is not there anymore (impact on latent space close to 0).\n",
    "\n",
    "\n",
    "\n",
    "As in previous examples, first we need to read our configuration files and then we can run the `random_continuous_paper__latent` task. You can have a look at the file in ```config/task/random_continuous_paper__latent.yaml```.\n",
    "\n",
    "\n",
    ">_ðŸ“– For the advanced reader:_ _This config file, in addition, explicitly sets the learning rate to be 1e-4 and introduces the factors beta (controlling how variational do we make the autoencoder) and the Kullback-Leibler divergence warmup steps, which will gradually introduce the loss term that pushes the samples towards the center of the latent space._\n",
    "\n",
    "\n",
    "âš ï¸  If you get a similar error:\n",
    "\n",
    "```\n",
    "RuntimeError: Error(s) in loading state_dict for VAE:\n",
    "size mismatch for encoderlayers.0.weight: copying a param with shape torch.Size([720, 1342]) from checkpoint, the shape in current model is torch.Size([900, 1342]).\n",
    "```\n",
    "\n",
    "Erase the model.pt file in the results folder, e.g. ```results_cont_paper/latent_space```\n",
    "\n",
    "### 3) Identify associations between features: \n",
    "\n",
    "Lastly, we will use what the model has learnt to identify entangled or associated variables.\n",
    "\n",
    "When perturbing the input value of a certain feature for all samples, the latent representation of said samples will change, and so will their reconstructions. We can track the induced shifts in output values to identify the features that were affected the most when perturbing the original feature, at a cohort level.\n",
    "\n",
    "> â„¹ï¸ We will use use the [Bayesian decision theory-based approach](https://www.nature.com/articles/s41587-022-01520-x#Sec15), presented in the Methods section of the original paper. We will also use an approach based on Kolmogorov-Smirnov distances between the feature reconstruction distributions, comparing them before and after perturbing an input feature.\n",
    "\n",
    "\n",
    "MOVE can be run in a jupyter-notebook friendly manner by importing specific functions from the package as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running MOVE on synthetic data\n",
    "from move.data import io\n",
    "from move.tasks import encode_data, analyze_latent, identify_associations\n",
    "\n",
    "# Configuration mapping\n",
    "configs = {\n",
    "    'encode': (\"random_continuous_paper\", \"encode_data\"),\n",
    "    'latent': (\"random_continuous_paper\", \"random_continuous_paper__latent\"),\n",
    "    'ttest': (\"random_continuous_paper\", \"random_continuous_paper__id_assoc_ttest\"),\n",
    "    'bayes': (\"random_continuous_paper\", \"random_continuous_paper__id_assoc_bayes\"),\n",
    "    'ks': (\"random_continuous_paper\", \"random_continuous_paper__id_assoc_ks\")\n",
    "}\n",
    "\n",
    "# Read all configs at once\n",
    "config_data = {name: io.read_config(*params) for name, params in configs.items()}\n",
    "\n",
    "# Execute tasks\n",
    "encode_data(config_data['encode'].data)\n",
    "analyze_latent(config_data['latent'])\n",
    "\n",
    "# Run all association identification methods\n",
    "for method in ['ttest', 'bayes', 'ks']:\n",
    "    identify_associations(config_data[method])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, move can be run using a command line style as follows. Please note that the folder structure must be such that move can find the config files.\n",
    "\n",
    "```python\n",
    "# Encode data\n",
    "! move-dl task=encode_data data=random_continuous_paper \n",
    "# Identify assoc bayes\n",
    "! move-dl task=random_continuous_paper__id_assoc_bayes data=random_continuous_paper\n",
    "# Identify assoc KS\n",
    "! move-dl task=random_continuous_paper__id_assoc_ks data=random_continuous_paper \n",
    "# Identify assoc t-test\n",
    "! move-dl task=random_continuous_paper__id_assoc_ttest data=random_continuous_paper \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing methods to identify associations\n",
    "\n",
    "We just run MOVE using the three methods to identify associations (t-test, Bayes and KS). The resulting tsv files can be compared with the ground truths file to benchmark their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a colormap from seaborn\n",
    "cmap = sns.color_palette(\"Dark2\", 3,  as_cmap=False)  # False for a list of colors\n",
    "\n",
    "# Set the color cycle for matplotlib using the colormap\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=cmap)\n",
    "\n",
    "##################################### Functions #############################################\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          cmap=None,\n",
    "                          normalize=False):\n",
    "    \n",
    "    \"\"\" Function that plots the confusion matrix given cm.\"\"\"\n",
    "\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    fig = plt.figure(figsize=(4,3))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=0, fontsize=12)\n",
    "        plt.yticks(tick_marks, target_names,fontsize=12)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=14)\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=14)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=14)\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass), fontsize=14)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def classify_associations(target_file, assoc_tuples):\n",
    "    self_assoc = 0 # Self associations\n",
    "    found_assoc_dict = {}\n",
    "    false_assoc_dict = {}\n",
    "    tp_fp = np.array([[0,0]])\n",
    "    with open (target_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line[0] != \"f\":\n",
    "                splitline = line.strip().split(\"\\t\") \n",
    "                feat_a = splitline[2]\n",
    "                feat_b = splitline[3]\n",
    "                score = abs(float(splitline[5]))\n",
    "                if feat_a == feat_b: # Self associations will not be counted\n",
    "                    self_assoc += 1\n",
    "                else:\n",
    "                    if (feat_a,feat_b) in assoc_tuples:\n",
    "                        found_assoc_dict[(feat_a,feat_b)] = score\n",
    "                        if (feat_b,feat_a) not in found_assoc_dict.keys(): #If we had not found it yet\n",
    "                            tp_fp = np.vstack((tp_fp,tp_fp[-1]+[0,1]))\n",
    "                    elif (feat_a,feat_b) not in assoc_tuples:\n",
    "                        false_assoc_dict[(feat_a,feat_b)] = score\n",
    "                        if (feat_b,feat_a) not in false_assoc_dict.keys():\n",
    "                            tp_fp = np.vstack((tp_fp,tp_fp[-1]+[1,0]))\n",
    "\n",
    "    # Remove duplicated associations:\n",
    "    for (i,j) in list(found_assoc_dict.keys()):\n",
    "        if (j,i) in found_assoc_dict.keys():\n",
    "            del found_assoc_dict[(j,i)] # remove the weakest direction for the association\n",
    "\n",
    "\n",
    "    for (i,j) in list(false_assoc_dict.keys()):\n",
    "        if (j,i) in false_assoc_dict.keys():\n",
    "            del false_assoc_dict[(i,j)]\n",
    "\n",
    "    return self_assoc, found_assoc_dict, false_assoc_dict, tp_fp\n",
    "\n",
    "def create_confusion_matrix(n_feat,associations,real_assoc,false_assoc):\n",
    "    cm = np.empty((2,2))\n",
    "    # TN: only counting the upper half matrix (non doubled associations)\n",
    "    cm[0,0] = (n_feat*n_feat-n_feat)/2 - (associations+false_assoc) # Diagonal is discarded\n",
    "    cm[0,1] = false_assoc\n",
    "    cm[1,0] = associations- real_assoc\n",
    "    cm[1,1] = real_assoc\n",
    "\n",
    "    return cm\n",
    "\n",
    "def get_precision_recall(found_assoc_dict,false_assoc_dict,associations):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # True Positives \n",
    "    for score in found_assoc_dict.values():\n",
    "        y_true.append(1)\n",
    "        y_pred.append(score)\n",
    "    # False Positives\n",
    "    for score in false_assoc_dict.values():\n",
    "        y_true.append(0)\n",
    "        y_pred.append(score)\n",
    "    # False negatives\n",
    "    for _ in range(associations-len(found_assoc_dict)):\n",
    "        y_true.append(1)\n",
    "        y_pred.append(0)\n",
    "\n",
    "    precision, recall, thr = precision_recall_curve(y_true,y_pred) #thr will tell us score values\n",
    "    avg_prec = average_precision_score(y_true,y_pred)\n",
    "\n",
    "    return precision, recall, thr, avg_prec\n",
    "\n",
    "def plot_precision_recall(precision,recall,avg_prec,label, ax):\n",
    "    ax.scatter(recall,precision, lw=0, marker=\".\", s=5, edgecolors='none', label = f\"{label} - APS:{avg_prec:.2f}\")\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "def plot_thr_recall(thr, recall,label,  ax):\n",
    "    ax.scatter(recall[:-1],thr, lw=0, marker=\".\", s=5, edgecolors='none', label=label)\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "def plot_TP_vs_FP(tp_fp, label, ax):\n",
    "    ax.scatter(tp_fp[:,0],tp_fp[:,1],s=2, label=label, edgecolors='none')\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "def plot_filling_order(order_list, last_rank=None):\n",
    "    \n",
    "    if last_rank is None:\n",
    "        last_rank = len(order_list)\n",
    "    fig = plt.figure()\n",
    "    order_img = np.zeros((np.max(order_list),len(order_list)))\n",
    "    for i, element in enumerate(order_list):\n",
    "        order_img[element-1,i:] = 1\n",
    "\n",
    "    plt.imshow(order_img[:last_rank,:], cmap=\"binary\")\n",
    "    plt.xlabel(\"Correct prediction number\")\n",
    "    plt.ylabel(\"Association ranking\")\n",
    "    plt.plot(np.arange(last_rank),np.arange(last_rank))\n",
    "    return fig\n",
    "\n",
    "def plot_effect_size_matching(assoc_tuples_dict,found_assoc_dict,label, ALGORITHM, ax):\n",
    "\n",
    "\n",
    "    ground_truth_effects = [assoc_tuples_dict[key] for key in list(found_assoc_dict.keys())]\n",
    "    predicted_effects = np.array(list(found_assoc_dict.values()))\n",
    "\n",
    "    if ALGORITHM == 'ttest':\n",
    "        #Eq 15 on https://doi.org/10.1146/annurev-statistics-031017-100307\n",
    "        predicted_effects = [-np.log10(p) if p !=0 else -1 for p in predicted_effects]\n",
    "        predicted_effects[predicted_effects == -1] = np.max(predicted_effects) # Change zeros for max likelihood, -1 as dummy value\n",
    "        predicted_effects = np.array(predicted_effects)\n",
    "\n",
    "    max, min  = np.max(predicted_effects), np.min(predicted_effects)\n",
    "    standarized_pred_effects = (predicted_effects-min)/(max-min)\n",
    "    ax.scatter(ground_truth_effects,standarized_pred_effects,s=12, edgecolors='none', label=label)\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "def plot_venn_diagram(venn, ax, mode = 'all', scale='log'):\n",
    "    sets = [set(venn[key][mode]) for key in list(venn.keys())]\n",
    "    labels = (key for key in  list(venn.keys()))\n",
    "\n",
    "    if len(venn) == 2:\n",
    "        venn2(sets, labels, ax=ax)\n",
    "    elif len(venn) == 3:\n",
    "        venn3(sets, labels, ax=ax)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported number of input files.\")\n",
    "\n",
    "def plot_upsetplot(venn,assoc_tuples):\n",
    "    \n",
    "    all_assoc = set([association for ALGORITHM in venn.keys() for association in venn[ALGORITHM]['all']])\n",
    "    columns = ['ground truth']\n",
    "    columns.extend([ALGORITHM for ALGORITHM in list(venn.keys())])\n",
    "\n",
    "    df = {}\n",
    "    for association in all_assoc:\n",
    "        df[association] = []\n",
    "\n",
    "        if association in assoc_tuples:\n",
    "            df[association].append('TP')\n",
    "        else:\n",
    "            df[association].append('FP')\n",
    "        \n",
    "        for ALGORITHM in list(venn.keys()):\n",
    "            if association in venn[ALGORITHM]['all']:\n",
    "                df[association].append(1)\n",
    "            else:\n",
    "                df[association].append(0)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(df, orient='index', columns = columns)\n",
    "    df = df.set_index([pd.Index(df[ALGORITHM] == 1) for ALGORITHM in list(venn.keys())])\n",
    "    upset = UpSet(df, intersection_plot_elements=0, show_counts=True)\n",
    "\n",
    "    upset.add_stacked_bars(by=\"ground truth\", colors=cm.Pastel1,\n",
    "                       title=\"Count by ground truth value\", elements=10)\n",
    "\n",
    "    return upset\n",
    "\n",
    "###################################### Main code ################################################\n",
    "\n",
    "def main(args_list):\n",
    "    parser = argparse.ArgumentParser(description='Read two files with ground truth associations and predicted associations.')\n",
    "    parser.add_argument('-p', '--perturbed', metavar='pert', type=str, required=True, help='perturbed feature names')\n",
    "    parser.add_argument('-n', '--features', metavar='feat', type=int, required=True, help='total number of features')\n",
    "    parser.add_argument('-r', '--reference', metavar='ref', type=str, required=True, help='path to the ground truth associations file')\n",
    "    parser.add_argument('-o', '--outpath', metavar='outpath', type=str, required=True, help='path where figures will be saved')\n",
    "    parser.add_argument('-t', '--targets', metavar='tar', type=str, required=True, nargs='+', help='path to the predicted associations files')\n",
    "    \n",
    "    args = parser.parse_args(args_list)\n",
    "\n",
    "\n",
    "    # Defining main performance evaluation figures:\n",
    "    fig_0, ax_0 = plt.subplots(figsize=(5,5))\n",
    "    fig_1, ax_1 = plt.subplots(figsize=(5,5))\n",
    "    fig_2, ax_2 = plt.subplots(figsize=(5,5))\n",
    "    fig_3, ax_3 = plt.subplots(figsize=(5,5))\n",
    "\n",
    "    assoc_tuples_dict = {}\n",
    "\n",
    "    # Reading the file with the ground truth changes:\n",
    "    with open (args.reference, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line[0] != \"f\" and line[0] != \"n\":\n",
    "                splitline = line.strip().split(\"\\t\") \n",
    "                feat_a = splitline[2]\n",
    "                feat_b = splitline[3]\n",
    "                assoc_strength = abs(float(splitline[4]))\n",
    "                # Only can detect associations with perturbed features\n",
    "                if args.perturbed in feat_a or args.perturbed in feat_b: \n",
    "                    assoc_tuples_dict[(feat_a,feat_b)] = assoc_strength\n",
    "                    assoc_tuples_dict[(feat_b,feat_a)] = assoc_strength\n",
    "\n",
    "    associations = int(len(assoc_tuples_dict)/2) \n",
    "    venn = {}\n",
    "    \n",
    "    # Count and save found associations\n",
    "    for target_file in args.targets:      \n",
    "        ALGORITHM = target_file.split('/')[-1].split('_')[3][:-4] \n",
    "        self_assoc, found_assoc_dict, false_assoc_dict, tp_fp = classify_associations(target_file,list(assoc_tuples_dict.keys()))\n",
    "        real_assoc = len(found_assoc_dict) # True predicted associations\n",
    "        false_assoc = len(false_assoc_dict) # False predicted associations\n",
    "        total_assoc = real_assoc + false_assoc\n",
    "\n",
    "        venn[ALGORITHM] = {}\n",
    "        venn[ALGORITHM]['correct'] = list(found_assoc_dict.keys()) \n",
    "        venn[ALGORITHM]['all'] = list(found_assoc_dict.keys()) +  list(false_assoc_dict.keys())\n",
    "\n",
    "        # Assess ranking of associations (they are doubled in assoc_tuples):\n",
    "        order_list = [list(assoc_tuples_dict.keys()).index((feat_a,feat_b))//2 for (feat_a,feat_b) in list(found_assoc_dict.keys())]\n",
    "        fig = plot_filling_order(order_list)\n",
    "        fig.savefig(f\"Order_image_{ALGORITHM}.png\", dpi=200)\n",
    "\n",
    "        ax_0 = plot_effect_size_matching(assoc_tuples_dict, found_assoc_dict, ALGORITHM, ALGORITHM, ax_0)\n",
    "\n",
    "        # Plot confusion matrix:\n",
    "        cm = create_confusion_matrix(args.features,associations,real_assoc,false_assoc)\n",
    "        fig = plot_confusion_matrix(cm,\n",
    "                              [\"No assoc\",\"Association\"],\n",
    "                              cmap=None,\n",
    "                              normalize=False)\n",
    "\n",
    "        fig.savefig(f'Confusion_matrix_{ALGORITHM}.png', dpi=100, bbox_inches='tight')\n",
    "\n",
    "        # Plot precision-recall and TP-FP curves\n",
    "        precision, recall, thr, avg_prec = get_precision_recall(found_assoc_dict,false_assoc_dict,associations)\n",
    "\n",
    "        ax_1 = plot_precision_recall(precision,recall, avg_prec,ALGORITHM, ax_1)\n",
    "        ax_2 = plot_TP_vs_FP(tp_fp, ALGORITHM, ax_2)\n",
    "        ax_3 = plot_thr_recall(thr, recall, ALGORITHM, ax_3)\n",
    "\n",
    "\n",
    "        # Write results:\n",
    "        with open('Performance_evaluation_summary_results.txt','a') as f:\n",
    "            f.write(f\" File:  {target_file}\\n\")\n",
    "            f.write(f\"Ground truth detectable associations (i.e. involving perturbed feature,{args.perturbed}):{associations}\\n\")\n",
    "            f.write(f\"{total_assoc} unique associations found\\n{self_assoc} self-associations were found before filtering\\n{real_assoc} were real associations\\n{false_assoc} were either false or below the significance threshold\\n\")\n",
    "            #print(\"Correct associations:\\n\", found_assoc_tuples, \"\\n\")\n",
    "            f.write(f\"Sensitivity:{real_assoc}/{associations} = {real_assoc/associations}\\n\")\n",
    "            f.write(f\"Precision:{real_assoc}/{total_assoc} = {(real_assoc)/total_assoc}\\n\")\n",
    "            f.write(f\"Order list:{order_list}\\n\\n\")\n",
    "            f.write(\"______________________________________________________\\n\")\n",
    "\n",
    "\n",
    "    # Edit figures: layout\n",
    "    ax_0.set_xlabel(\"Real effect\")\n",
    "    ax_0.set_ylabel(\"Predicted effect\")\n",
    "    ax_0.set_ylim((-0.02,1.02))\n",
    "    ax_0.set_xlim((0,1.02))\n",
    "    ax_0.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
    "              ncol=3, fancybox=True, shadow=True)\n",
    "\n",
    "    ax_1.set_xlabel(\"Recall\")\n",
    "    ax_1.set_ylabel(\"Precision\")\n",
    "    ax_1.legend()\n",
    "    ax_1.set_ylim((0,1.05))\n",
    "    ax_1.set_xlim((0,1.05))\n",
    "\n",
    "    ax_2.set_xlabel(\"False Positives\")\n",
    "    ax_2.set_ylabel(\"True Positives\")\n",
    "    ax_2.set_aspect(\"auto\")\n",
    "\n",
    "    ax_3.set_ylabel(\"Threshold\")\n",
    "    ax_3.set_xlabel(\"Recall\")\n",
    "\n",
    "\n",
    "    # Save main figures:\n",
    "    fig_0.savefig(args.outpath + \"Effect_size_matchin.svg\", dpi=200)\n",
    "    fig_1.savefig(args.outpath + \"Precision_recall.svg\", dpi=200)\n",
    "    fig_2.savefig(args.outpath + \"TP_vs_FP.svg\", dpi=200)\n",
    "    fig_3.savefig(args.outpath + \"thr_vs_recall.svg\", dpi=200)\n",
    "\n",
    "    # Plotting venn diagram:\n",
    "    if len(venn) == 2 or len(venn) == 3:\n",
    "        fig_v, ax_v = plt.subplots(figsize=(4,4))\n",
    "        ax_v = plot_venn_diagram(venn, ax_v, mode = 'correct')\n",
    "        fig_v.savefig(args.outpath + 'Venn_diagram.svg', dpi=200)\n",
    "\n",
    "    # Plotting UpSet plot\n",
    "    upset = plot_upsetplot(venn, list(assoc_tuples_dict.keys()))\n",
    "    upset.plot()\n",
    "    plt.savefig(args.outpath + 'UpSet.svg', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_list = [\n",
    "    '-p', 'Continuous_A_2',\n",
    "    '-n', '36',\n",
    "    '-r', './synthetic_data/changes.random_all_sim.txt',\n",
    "    '-o', './synthetic_data/',\n",
    "    '-t', \n",
    "    './results_cont_paper/identify_associations/results_sig_assoc_ttest.tsv',\n",
    "    './results_cont_paper/identify_associations/results_sig_assoc_bayes.tsv',\n",
    "     './results_cont_paper/identify_associations/results_sig_assoc_ks.tsv'\n",
    "]\n",
    "\n",
    "main(args_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing Bayes and KS score distributions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df_bayes = pd.read_csv(\"./results_cont_paper/identify_associations/results_sig_assoc_bayes.tsv\", sep=\"\\t\")\n",
    "df_ks = pd.read_csv(\"./results_cont_paper/identify_associations/results_sig_assoc_ks.tsv\", sep=\"\\t\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Plot histograms using ax.hist() instead of np.histogram()\n",
    "axs[0].hist(abs(df_bayes['bayes_k']), bins=80)\n",
    "axs[1].hist(abs(df_ks['ks_distance']), bins=80)\n",
    "\n",
    "# Add labels and titles\n",
    "axs[0].set_title('Bayes K Distribution')\n",
    "axs[0].set_xlabel('|Bayes K|')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "\n",
    "axs[1].set_title('KS Distance Distribution')\n",
    "axs[1].set_xlabel('|KS Distance|')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(\"./results_cont_paper/identify_associations/KS_vs_Bayes.svg\", dpi = 200)\n",
    "\n",
    "df_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synthetic data 2:**\n",
    "\n",
    "\n",
    "Here we will create a simplified version of the datasets in order to:\n",
    "-  explore the behavior of MOVE at high and low sample regimes (Low N: 50 / High N: 1000).\n",
    "-  visualize the latent representations in a 3D latent space and their movement after the perturbations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r synthetic_data_II/\n",
    "! rm -r results_cont_paper_II/\n",
    "! rm -r results_cont_paper_II_high_N/\n",
    "! rm -r results_cont_paper_II_low_N/\n",
    "! rm -r interim_data_cont_paper_II/\n",
    "! rm -r interim_data_cont_paper_II_high_N/\n",
    "! rm -r interim_data_cont_paper_II_low_N/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Hyperparameters ####################################\n",
    "import random as rnd\n",
    "\n",
    "PROJECT_NAME = \"random_all_sim\"\n",
    "MODE = \"linear\"  # \"non-linear\"\n",
    "SEED_1 = 123 #1234\n",
    "np.random.seed(SEED_1)\n",
    "rnd.seed(SEED_1)\n",
    "\n",
    "COV_ALPHA = 0.97 #.01 \n",
    "\n",
    "data_regimes_dict = {\"synthetic_data_II\":{\"N_SAMPLES\":1000,\"HIGH_CORR\":True, \"interim_path\":\"interim_data_cont_paper_II\", \"results_path\":\"results_cont_paper_II\"}, #high N, added explicit corr\n",
    "                     \"low_N_gaussian_corr\":{\"N_SAMPLES\":50,\"HIGH_CORR\":False,\"interim_path\":\"interim_data_cont_paper_II_low_N\",\"results_path\":\"results_cont_paper_II_low_N\"},\n",
    "                     \"high_N_gaussian_corr\":{\"N_SAMPLES\":1000,\"HIGH_CORR\":False,\"interim_path\":\"interim_data_cont_paper_II_high_N\",\"results_path\":\"results_cont_paper_II_high_N\"},\n",
    "                     } \n",
    "\n",
    "SETTINGS = {\n",
    "    \"Continuous_A\": {\n",
    "        \"features\": 10,\n",
    "        \"frequencies\": [0.002, 0.01, 0.02],\n",
    "        \"coefficients\": [500, 100, 50],\n",
    "        \"phase\": 0,\n",
    "        \"offset\": 500,\n",
    "    },\n",
    "    \"Continuous_B\": {\n",
    "        \"features\": 10,\n",
    "        \"frequencies\": [0.001, 0.05, 0.08],\n",
    "        \"coefficients\": [80, 20, 10],\n",
    "        \"phase\": np.pi / 2,\n",
    "        \"offset\": 400,\n",
    "    },\n",
    "    \"Categorical_A\": {\n",
    "        \"features\": 1,\n",
    "        \"frequencies\": [0.1, 0.5, 0.8],\n",
    "        \"coefficients\": [.2, .1, .05],\n",
    "        \"phase\": np.pi / 2,\n",
    "        \"offset\": 10,\n",
    "    },\n",
    "        \"Categorical_B\": {\n",
    "        \"features\": 1,\n",
    "        \"frequencies\": [0.01, 0.5, 0.08],\n",
    "        \"coefficients\": [10, .1, .05],\n",
    "        \"phase\": np.pi,\n",
    "        \"offset\": 1,\n",
    "    }\n",
    "}\n",
    "\n",
    "for path, hyperparams in data_regimes_dict.items():\n",
    "\n",
    "    COR_THRES = 0.02\n",
    "    PAIRS_OF_INTEREST = [(1,2),(3,4)]  # ,(77,75),(99,70),(38,2),(67,62)]\n",
    "    HIGH_CORR = hyperparams[\"HIGH_CORR\"]\n",
    "    N_SAMPLES = hyperparams[\"N_SAMPLES\"]\n",
    "\n",
    "    # Path to store output files\n",
    "    outpath = Path(f\"{path}\")\n",
    "    outpath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # %%\n",
    "    # Add all datasets in a single matrix:\n",
    "    all_feature_names = get_feature_names(SETTINGS)\n",
    "    feat_means = create_mean_profiles(SETTINGS)\n",
    "\n",
    "    # %%\n",
    "    ###### Covariance matrix definition ######\n",
    "    if MODE == \"linear\":\n",
    "        covariance_matrix = make_sparse_spd_matrix(\n",
    "            n_dim=len(all_feature_names),\n",
    "            alpha=COV_ALPHA,\n",
    "            smallest_coef=0,\n",
    "            largest_coef=1,\n",
    "            norm_diag=True,\n",
    "            random_state=SEED_1,\n",
    "        )\n",
    "    elif MODE == \"non-linear\":\n",
    "        covariance_matrix = np.identity(len(all_feature_names))\n",
    "\n",
    "    ABS_MAX = np.max(abs(covariance_matrix))\n",
    "    fig = plot_score_matrix(\n",
    "        covariance_matrix, all_feature_names, vmin=-ABS_MAX, vmax=ABS_MAX\n",
    "    )\n",
    "    fig.savefig(outpath / f\"Covariance_matrix_{PROJECT_NAME}.svg\")\n",
    "\n",
    "    #    dataset = np.array(\n",
    "    #        [\n",
    "    #            list(np.random.multivariate_normal(feat_means, covariance_matrix))\n",
    "    #            for _ in range(N_SAMPLES)\n",
    "    #        ]\n",
    "    #    )\n",
    "\n",
    "    dataset = np.random.multivariate_normal(feat_means, covariance_matrix, N_SAMPLES)\n",
    "\n",
    "    # Add non-linearities\n",
    "    if MODE == \"non-linear\":\n",
    "        for i, j in PAIRS_OF_INTEREST:\n",
    "            freq = np.random.choice([4, 5, 6])\n",
    "            dataset[:, i] += np.sin(freq * dataset[:, j])\n",
    "\n",
    "    #scaled_dataset, _ = scale(dataset)\n",
    "    # No scaling in the dataset creation! It will be handled in preprocessing.\n",
    "    scaled_dataset = dataset\n",
    "\n",
    "    if HIGH_CORR: # The last half of the features are combinations of the first half:\n",
    "        for i in range(scaled_dataset.shape[1]//2):\n",
    "            col_1 = np.random.choice(range(scaled_dataset.shape[1]//2))\n",
    "            col_2 = np.random.choice(range(scaled_dataset.shape[1]//2))\n",
    "            scaled_dataset[:,i+scaled_dataset.shape[1]//2] = (scaled_dataset[:,col_1]+scaled_dataset[:,col_2])/2 + np.random.normal()\n",
    "\n",
    "    # Binarize the categorical dataset\n",
    "    NUM_CAT = SETTINGS[\"Categorical_A\"][\"features\"] + SETTINGS[\"Categorical_B\"][\"features\"]\n",
    "    columns_to_binarize = scaled_dataset[:,-NUM_CAT:]\n",
    "\n",
    "    # Compute the mean of each of the categorical columns    \n",
    "    means = columns_to_binarize.mean(axis=0)\n",
    "\n",
    "    # Apply the binarization\n",
    "    scaled_dataset[:,-NUM_CAT:] = (columns_to_binarize > means).astype(int)\n",
    "\n",
    "    print(np.min(scaled_dataset),np.max(scaled_dataset))\n",
    "    # Actual correlations:\n",
    "    # correlations = np.empty(np.shape(covariance_matrix))\n",
    "    # for ifeat in range(len(covariance_matrix)):\n",
    "    #     for jfeat in range(len(covariance_matrix)):\n",
    "    #         correlations[ifeat, jfeat] = pearsonr(dataset[:, ifeat], dataset[:, jfeat])[\n",
    "    #             0\n",
    "    #         ]\n",
    "\n",
    "    correlations = np.corrcoef(scaled_dataset, rowvar=False)\n",
    "    fig = plot_score_matrix(correlations, all_feature_names, vmin=-1, vmax=1, label_step=5)\n",
    "    fig.savefig(outpath / f\"Correlations_{PROJECT_NAME}.svg\", dpi=200)\n",
    "\n",
    "    # Sort correlations by absolute value\n",
    "    associations = create_ground_truth_correlations_file(correlations)\n",
    "    associations.to_csv(outpath / f\"changes.{PROJECT_NAME}.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "    # Plot feature profiles per sample\n",
    "    fig = plot_feature_profiles(scaled_dataset, feat_means)\n",
    "    fig.savefig(outpath / \"Multi-omic_profiles.svg\")\n",
    "\n",
    "    ## Plot correlations\n",
    "    fig = plot_feature_correlations(dataset, PAIRS_OF_INTEREST)\n",
    "    fig.savefig(outpath / \"Feature_correlations.svg\")\n",
    "\n",
    "    fig = plot_feature_correlations(scaled_dataset, PAIRS_OF_INTEREST)\n",
    "    fig.savefig(outpath / \"Feature_correlations_scaled.svg\")\n",
    "\n",
    "    # Write tsv files with feature values for all samples in both datasets:\n",
    "    save_splitted_datasets(\n",
    "        SETTINGS, PROJECT_NAME, scaled_dataset, all_feature_names, N_SAMPLES, outpath\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running MOVE on simple synthetic data (command line style)\n",
    "data_regimes_dict = {\"synthetic_data_II\":{\"N_SAMPLES\":1000,\"HIGH_CORR\":True, \"interim_path\":\"interim_data_cont_paper_II\", \"results_path\":\"results_cont_paper_II\"}, #high N, added explicit corr\n",
    "                     \"low_N_gaussian_corr\":{\"N_SAMPLES\":50,\"HIGH_CORR\":False,\"interim_path\":\"interim_data_cont_paper_II_low_N\",\"results_path\":\"results_cont_paper_II_low_N\"},\n",
    "                     \"high_N_gaussian_corr\":{\"N_SAMPLES\":1000,\"HIGH_CORR\":False,\"interim_path\":\"interim_data_cont_paper_II_high_N\",\"results_path\":\"results_cont_paper_II_high_N\"},\n",
    "                     } \n",
    "\n",
    "for data_regime, hyperparams in data_regimes_dict.items():\n",
    "\n",
    "    # Encode data\n",
    "    ! move-dl task=encode_data data=random_continuous_paper_II data.raw_data_path={data_regime} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]}     # where result files will be placed\n",
    "\n",
    "    # Analyze latent\n",
    "    ! move-dl task=random_continuous_paper_II__latent data=random_continuous_paper_II data.raw_data_path={data_regime} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]}\n",
    "\n",
    "    # Copy model trained on analyze latent to id_assoc directory\n",
    "    ! mkdir -p {hyperparams[\"interim_path\"]}/models\n",
    "    ! mv {hyperparams[\"results_path\"]}/latent_space/model.pt {hyperparams[\"interim_path\"]}/models/model_3_0.pt\n",
    "\n",
    "    # Identify assoc ks\n",
    "    ! move-dl task=random_continuous_paper_II__id_assoc_ks data=random_continuous_paper_II data.raw_data_path={data_regime} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot reconstruction metrics plots separately:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot categorical box-plots separately (generate tsv's by changing N and HIGH_CORR)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Times New Roman',\n",
    "    'font.size': 16,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'axes.labelsize': 14,\n",
    "    'legend.fontsize': 14\n",
    "})\n",
    "\n",
    "folders = [\"results_cont_paper_II\", \"results_cont_paper_II_high_N\",\"results_cont_paper_II_low_N\"]\n",
    "dfs = [pd.read_csv(f\"{folder}/latent_space/reconstruction_metrics.tsv\", sep=\"\\t\") for folder in folders]\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    fig = plt.figure(figsize=(7,5))  # Increased height for 4 rows\n",
    "    \n",
    "    # Scatter points for categorical data\n",
    "    plt.scatter([df['random.random_all_sim.Categorical_A'].mean(),df['random.random_all_sim.Categorical_B'].mean()], \n",
    "                [0.8, 0.6], marker=\"d\", s=128, color='#3e4297')\n",
    "\n",
    "    # Horizontal boxplots for continuous data\n",
    "    bp = plt.boxplot([df['random.random_all_sim.Continuous_A'], df['random.random_all_sim.Continuous_B']], \n",
    "                    positions=[0.4, 0.2], vert=False, widths=0.10, \n",
    "                    patch_artist=True, medianprops=dict(color='black'))\n",
    "\n",
    "    bp['boxes'][0].set_facecolor('#980000')\n",
    "    bp['boxes'][1].set_facecolor('#980000')\n",
    "    \n",
    "    plt.xlim(-0.05,1.05)\n",
    "    plt.ylim(0.05, 0.95)\n",
    "    plt.yticks([0.8, 0.6, 0.4, 0.2], [\"Categorical_A\", \"Categorical_B\", \"Continuous_A\", \"Continuous_B\"], size=14)\n",
    "    plt.xticks(size=14)\n",
    "    plt.xlabel(\"Score\", size=16)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(f\"./{folders[i]}/latent_space/Categorical_reconstructions_{i}.svg\", dpi=200)\n",
    "\n",
    "    #3e4297 #980000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting continuous importance:**\n",
    "\n",
    "Here we will train MOVE by setting a high weight (w=10) on the loss associated to the continuous datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\"N_SAMPLES\":1000,\n",
    "               \"HIGH_CORR\":True,\n",
    "               \"data_path\":\"synthetic_data_II\",\n",
    "               \"interim_path\":\"interim_data_cont_paper_II_10x_cont\",\n",
    "               \"results_path\":\"results_cont_paper_II_10x_cont\",\n",
    "               \"w\":10}\n",
    "\n",
    "! move-dl task=encode_data data=random_continuous_paper_II data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} +data.continuous_inputs.0.weight={hyperparams[\"w\"]} +data.continuous_inputs.1.weight={hyperparams[\"w\"]} \n",
    "! move-dl task=random_continuous_paper_II__latent data=random_continuous_paper_II data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} +data.continuous_inputs.0.weight={hyperparams[\"w\"]} +data.continuous_inputs.1.weight={hyperparams[\"w\"]} \n",
    "# Copy model trained on analyze latent to id_assoc directory\n",
    "! mkdir -p {hyperparams[\"interim_path\"]}/models\n",
    "! mv {hyperparams[\"results_path\"]}/latent_space/model.pt {hyperparams[\"interim_path\"]}/models/model_3_0.pt\n",
    "! move-dl task=random_continuous_paper_II__id_assoc_ks data=random_continuous_paper_II data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} +data.continuous_inputs.0.weight={hyperparams[\"w\"]} +data.continuous_inputs.1.weight={hyperparams[\"w\"]} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Latent space.\n",
    "\n",
    "We will now represent the simplified dataset in a 3D latent space. This will allow us to immediately and intuitively understand the emerging behavior of MOVE given its architecture and loss function. We will also be able to check how the perturbations impact the latent location of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib.pyplot import cm\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Times New Roman',\n",
    "    'font.size': 14,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'axes.labelsize': 14,\n",
    "    'legend.fontsize': 14\n",
    "})\n",
    "\n",
    "DATASET = \"Continuous_B\" #\"Continuous_A\"\n",
    "#feature_list = [\"Day\"]# + list(genes_of_interest.keys())\n",
    "feature_list = pd.read_csv(f\"./interim_data_cont_paper_II/random.random_all_sim.{DATASET}.txt\", header=None)\n",
    "feature_list = feature_list.values.flatten()\n",
    "\n",
    "figure_path = Path(\"./synthetic_data_II/figures_synthetic_small/\")\n",
    "! mkdir -p {figure_path}\n",
    "\n",
    "latent_space = np.load(\"./results_cont_paper_II/identify_associations/latent_location.npy\")\n",
    "\n",
    "def plot_3D_latent_and_displacement(\n",
    "    mu_baseline,\n",
    "    mu_perturbed,\n",
    "    feature_values,\n",
    "    feature_name,\n",
    "    show_baseline=True,\n",
    "    show_perturbed=True,\n",
    "    show_arrows=True,\n",
    "    step: int=1,\n",
    "    altitude: int=30,\n",
    "    azimuth: int=45,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the movement of the samples in the 3D latent space after perturbing one\n",
    "    input variable.\n",
    "\n",
    "    Args:\n",
    "        mu_baseline:\n",
    "            ND array with dimensions n_samples x n_latent_nodes containing\n",
    "            the latent representation of each sample\n",
    "        mu_perturbed:\n",
    "            ND array with dimensions n_samples x n_latent_nodes containing\n",
    "            the latent representation of each sample after perturbing the input\n",
    "        feature_values:\n",
    "            1D array with feature values to map to a colormap (\"bwr\"). Each sample is\n",
    "            colored according to its value for the feature of interest.\n",
    "        feature_name:\n",
    "            name of the feature mapped to a colormap\n",
    "        show_baseline:\n",
    "            plot orginal location of the samples in the latent space\n",
    "        show_perturbed:\n",
    "            plot final location (after perturbation) of the samples in latent space\n",
    "        show_arrows:\n",
    "            plot arrows from original to final location of each sample\n",
    "        angle:\n",
    "            elevation from dim1-dim2 plane for the visualization of latent space.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If latent space is not 3-dimensional (3 hidden nodes).\n",
    "    Returns:\n",
    "        Figure\n",
    "    \"\"\"\n",
    "    # construct cmap\n",
    "    #hex_colors= ['#36429bff','#78b0d3ff','#fedc8cff','#d22b27ff']\n",
    "    #my_cmap = ListedColormap(hex_colors)\n",
    "    #my_cmap = sns.color_palette(\"Dark2\", as_cmap=True)\n",
    "    my_cmap = sns.color_palette(\"RdYlBu\", as_cmap=True)\n",
    "\n",
    "    eps = 1e-16\n",
    "    if [np.shape(mu_baseline)[1], np.shape(mu_perturbed)[1]] != [3, 3]:\n",
    "        raise ValueError(\n",
    "            \" The latent space must be 3-dimensional. Redefine num_latent to 3.\"\n",
    "        )\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(5, 5))\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    ax.view_init(altitude, azimuth)\n",
    "\n",
    "    if show_baseline:\n",
    "        vmin, vmax = np.min(feature_values[::step]), np.max(feature_values[::step])\n",
    "        abs_max = np.max([abs(vmin), abs(vmax)])\n",
    "        plot = ax.scatter(\n",
    "            mu_baseline[::step, 0],\n",
    "            mu_baseline[::step, 1],\n",
    "            mu_baseline[::step, 2],\n",
    "            marker=\"o\",\n",
    "            c=feature_values[::step],\n",
    "            s=10,\n",
    "            lw=0,\n",
    "            cmap=my_cmap,\n",
    "            #vmin=0,\n",
    "            #vmax=1\n",
    "        )\n",
    "        #ax.set_title(feature_name)\n",
    "        fig.colorbar(plot)\n",
    "        \n",
    "        #plt.colorbar()  # Normalize(min(feature_values[::step]),max(feature_values[::step]))), ax=ax)\n",
    "    if show_perturbed:\n",
    "        ax.scatter(\n",
    "            mu_perturbed[::step, 0],\n",
    "            mu_perturbed[::step, 1],\n",
    "            mu_perturbed[::step, 2],\n",
    "            marker=\"o\",\n",
    "            c=feature_values[::step],\n",
    "            s=10,\n",
    "            label=\"perturbed\",\n",
    "            lw=0,\n",
    "        )\n",
    "    if show_arrows:\n",
    "        u = mu_perturbed[::step, 0] - mu_baseline[::step, 0]\n",
    "        v = mu_perturbed[::step, 1] - mu_baseline[::step, 1]\n",
    "        w = mu_perturbed[::step, 2] - mu_baseline[::step, 2]\n",
    "\n",
    "        module = np.sqrt(u * u + v * v + w * w)\n",
    "\n",
    "        mask = module > eps\n",
    "\n",
    "        max_u, max_v, max_w = np.max(abs(u)), np.max(abs(v)), np.max(abs(w))\n",
    "        # Arrow colors will be weighted contributions of red -> dim1, green -> dim2, and blue-> dim3. I.e. purple arrow means movement in dims 1 and 3\n",
    "        colors = [\n",
    "            (abs(du) / max_u, abs(dv) / max_v, abs(dw) / max_w, 0.7)\n",
    "            for du, dv, dw in zip(u, v, w)\n",
    "        ]\n",
    "        ax.quiver(\n",
    "            mu_baseline[::step, 0][mask],\n",
    "            mu_baseline[::step, 1][mask],\n",
    "            mu_baseline[::step, 2][mask],\n",
    "            u[mask],\n",
    "            v[mask],\n",
    "            w[mask],\n",
    "            color=colors,\n",
    "            lw=.6,\n",
    "            )  # alpha=(1-module/np.max(module))**6, arrow_length_ratio=0)\n",
    "        # help(ax.quiver)\n",
    "    ax.set_xlabel(\"Dim 1\")\n",
    "    ax.set_ylabel(\"Dim 2\")\n",
    "    ax.set_zlabel(\"Dim 3\")\n",
    "\n",
    "    \n",
    "    # ax.set_axis_off()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Main code:\n",
    "for i,feature in enumerate(feature_list):\n",
    "    print(i)\n",
    "    feature_values = np.load(f\"./interim_data_cont_paper_II/random.random_all_sim.{DATASET}.npy\")\n",
    "    CONDITION = \"Categorical\" in DATASET\n",
    "    if CONDITION:\n",
    "        feature_values = np.argmax(feature_values, axis=2)[:,i]\n",
    "    else:\n",
    "        feature_values = feature_values[:,i]\n",
    "\n",
    "    print(feature_values)\n",
    "    latent_space_baseline = latent_space[:,:,-1]\n",
    "    latent_space_perturbed = latent_space[:,:,i]\n",
    "    \n",
    "    # # Plot latent space:\n",
    "    n_pictures = 10\n",
    "    PLOT = [0] if CONDITION else [0,2,3,9] # Plot Categorical A1\n",
    "    if i in PLOT:\n",
    "        print(i)\n",
    "        #for pic_num, (azimuth, altitude) in enumerate(zip(\n",
    "        #    np.linspace(90, 180, n_pictures), np.linspace(35, 55, n_pictures)\n",
    "        #)):\n",
    "            \n",
    "        for pic_num, (azimuth, altitude) in enumerate([[120,44]]):\n",
    "            print(pic_num, azimuth, altitude)\n",
    "            title = feature \n",
    "            fig = plot_3D_latent_and_displacement(\n",
    "                latent_space_baseline,\n",
    "                latent_space_perturbed,\n",
    "                feature_values=feature_values,\n",
    "                feature_name=f\"{title}\",\n",
    "                show_baseline=True,\n",
    "                show_perturbed=False,\n",
    "                show_arrows=False,\n",
    "                altitude=altitude,\n",
    "                azimuth=azimuth,\n",
    "            )\n",
    "            fig.savefig(\n",
    "                figure_path / f\"3D_latent_{pic_num}_perturbed_{feature}.svg\"\n",
    "            )\n",
    "            plt.close(fig)\n",
    "\n",
    "            fig = plot_3D_latent_and_displacement(\n",
    "                latent_space_baseline,\n",
    "                latent_space_perturbed,\n",
    "                feature_values=feature_values,\n",
    "                feature_name=f\"{title}\",\n",
    "                show_baseline=False,\n",
    "                show_perturbed=False,\n",
    "                show_arrows=True,\n",
    "                altitude=altitude,\n",
    "                azimuth=azimuth,\n",
    "            )\n",
    "            fig.savefig(\n",
    "                figure_path / f\"3D_latent_{pic_num}_perturbed_{feature}_arrows_max.svg\"\n",
    "            )\n",
    "            plt.close(fig)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real world applications: \n",
    "### A) A Parkinsons dataset\n",
    "\n",
    "Download the csv file The original table is provided as a Supplementary Dataset in the original article: [Parkinsons disease dataset](https://static-content.springer.com/esm/art%3A10.1038%2Fs41598-017-00047-5/MediaObjects/41598_2017_47_MOESM3_ESM.xls) by clicking on the link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "output_path = Path(\"./parkinsons_data/\")\n",
    "output_path.mkdir(exist_ok=True)\n",
    "df = pd.read_csv(output_path / \"dataset.csv\", sep=\",\")\n",
    "\n",
    "# Setting the index\n",
    "df['ID'] = pd.RangeIndex(1, len(df)+1)\n",
    "df = df.set_index('ID')\n",
    "\n",
    "\n",
    "# Function to convert columns to proper data types and handle missing values\n",
    "def process_dataframe(dataframe, clean_medications=False):\n",
    "    \"\"\"Convert numerical columns to proper types and replace missing values with 'NA'\"\"\"\n",
    "    processed_df = dataframe.copy()\n",
    "    \n",
    "    for col in processed_df.columns:\n",
    "        # Skip columns that are clearly categorical\n",
    "        if col in [' Gender ', ' Positive  history  of  Parkinson  disease  in  family ', \n",
    "                   ' Antidepressant  therapy ', ' Antiparkinsonian  medication ', \n",
    "                   ' Antipsychotic  medication ', ' Benzodiazepine  medication ']:\n",
    "            # For categorical columns, just replace empty strings and various missing indicators\n",
    "            processed_df[col] = processed_df[col].replace(['', ' ', 'nan', 'NaN', np.nan, None], 'NA')\n",
    "\n",
    "            if clean_medications and 'therapy' in col or 'medication' in col:\n",
    "                # Clean medication columns to keep only Yes/No\n",
    "                processed_df[col] = processed_df[col].astype(str).str.strip()\n",
    "                processed_df[col] = processed_df[col].apply(lambda x: \n",
    "                    'Yes' if x.startswith('Yes') or x.startswith(' Yes') else\n",
    "                    'No' if x.startswith('No') or x.startswith(' No') else\n",
    "                    'NA')\n",
    "            else:\n",
    "                # For other categorical columns, just replace empty strings and various missing indicators\n",
    "                processed_df[col] = processed_df[col].replace(['', ' ', 'nan', 'NaN', np.nan, None], 'NA')\n",
    "        else:\n",
    "            # For numerical columns, convert to numeric and handle missing values\n",
    "            # First, replace various missing value indicators with NaN\n",
    "            processed_df[col] = processed_df[col].replace(['', ' ', '-', 'nan', 'NaN'], np.nan)\n",
    "            \n",
    "            # Convert to numeric, coercing errors to NaN\n",
    "            processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')\n",
    "            \n",
    "            # Replace NaN with 'NA' string for output\n",
    "            processed_df[col] = processed_df[col].fillna('NA')\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# 1. parkinson.ids.txt - Only the index numbers, no header\n",
    "with open(output_path / \"parkinsons.ids.txt\", 'w') as f:\n",
    "    for idx in df.index:\n",
    "        f.write(f\"{idx}\\n\")\n",
    "\n",
    "# 2. demographics.tsv - Index + Gender and Positive history of Parkinson disease in family\n",
    "demographics_cols = ['Gender', ' Positive  history  of  Parkinson  disease  in  family ']\n",
    "demographics_df = df[demographics_cols]\n",
    "demographics_df.to_csv(output_path / \"demographics.tsv\", sep='\\t', na_rep='NA')\n",
    "\n",
    "# 3. age_related.tsv - Index + Age, Age of disease onset, Duration of disease\n",
    "age_cols = [' Age  (years) ', ' Age  of  disease  onset  (years) ', ' Duration  of  disease  from  first  symptoms  (years) ']\n",
    "age_df = process_dataframe(df[age_cols])\n",
    "age_df.to_csv(output_path / \"age_related.tsv\", sep='\\t', na_rep='NA')\n",
    "\n",
    "# 4. medication.tsv (without Levodopa and Clonazepam)\n",
    "medication_cols = df.columns[6:10].tolist()\n",
    "medication_df = process_dataframe(df[medication_cols], clean_medications=True)\n",
    "medication_df.to_csv(output_path / \"medication.tsv\", sep='\\t', na_rep='NA')\n",
    "\n",
    "# 5. overview_of_motor_exam.tsv \n",
    "motor_overview_cols = df.columns[12:14].tolist()\n",
    "motor_overview_df = process_dataframe(df[motor_overview_cols])\n",
    "motor_overview_df.to_csv(output_path / \"overview_of_motor_exam.tsv\", sep='\\t', na_rep='NA')\n",
    "\n",
    "# 6. motor_scale.tsv \n",
    "motor_scale_cols = df.columns[14:41].tolist()\n",
    "motor_scale_df = process_dataframe(df[motor_scale_cols])\n",
    "motor_scale_df.to_csv(output_path / \"motor_scale.tsv\", sep='\\t', na_rep='NA')\n",
    "\n",
    "# 7. speech_1.tsv \n",
    "speech1_cols = df.columns[41:53].tolist()\n",
    "speech1_df = process_dataframe(df[speech1_cols])\n",
    "speech1_df.to_csv(output_path / \"speech_1.tsv\", sep='\\t', na_rep='NA')\n",
    "\n",
    "# 8. speech_2.tsv \n",
    "speech2_cols = df.columns[53:].tolist()\n",
    "speech2_df = process_dataframe(df[speech2_cols])\n",
    "speech2_df.to_csv(output_path / \"speech2.tsv\", sep='\\t', na_rep='NA')\n",
    "\n",
    "print(\"Files created successfully!\")\n",
    "print(f\"Total columns in original dataset: {len(df.columns)}\")\n",
    "print(\"\\nFiles created:\")\n",
    "for file_name in [\"parkinson.ids.txt\", \"demographics.tsv\", \"age_related.tsv\", \n",
    "                  \"medication.tsv\", \"overview_of_motor_exam.tsv\", \"motor_scale.tsv\", \n",
    "                  \"speech_1.tsv\", \"speech2.tsv\"]:\n",
    "    file_path = output_path / file_name\n",
    "    if file_path.exists():\n",
    "        print(f\"âœ“ {file_name}\")\n",
    "    else:\n",
    "        print(f\"âœ— {file_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run move on parkinson's data:\n",
    "# Encode data\n",
    "! move-dl task=encode_data data=parkinsons\n",
    "\n",
    "# Analyze latent\n",
    "! move-dl task=parkinsons__latent data=parkinsons\n",
    "\n",
    "# Identify assoc ks\n",
    "! move-dl task=parkinsons__id_assoc_bayes_cont data=parkinsons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Machine Learning Omics: \n",
    "\n",
    "This is a multimodal dataset on diverse cancers, covering CNV, methylation, miRNA and mRNA. Please note that all of these data modalities are described by continuous features.\n",
    "\n",
    "https://www.nature.com/articles/s41597-025-05235-x\n",
    "\n",
    "From all MLOmics data, I used the files for clustering on LUAD data, the aligned files to ease integration.\n",
    "\n",
    "```\n",
    "MLOmics -> Clustering -> LUAD -> aligned\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('./MLOmics_Clustering_LUAD_aligned/')\n",
    "#df = pd.read_csv(data_path / 'LUAD_CNV_aligned.csv', sep=',', header=0, index_col=0).T\n",
    "\n",
    "def transpose_luad_files(folder_path):\n",
    "   \"\"\"Transpose LUAD CSV files and save with _MOVE suffix. Create LUAD.ids.txt.\"\"\"\n",
    "   \n",
    "   folder = Path(folder_path)\n",
    "   luad_files = list(folder.glob(\"LUAD*.csv\"))\n",
    "   \n",
    "   if not luad_files:\n",
    "       print(f\"No LUAD files found in {folder_path}\")\n",
    "       return\n",
    "   \n",
    "   all_ids = set()\n",
    "   \n",
    "   for file_path in luad_files:\n",
    "       # Read, transpose, save\n",
    "       df = pd.read_csv(file_path, index_col=0).T\n",
    "       df.index.name = \"ID\"  # Add \"ID\" as header for index column\n",
    "       \n",
    "       # Save with _MOVE suffix\n",
    "       output_name = file_path.stem + \"_MOVE.tsv\"\n",
    "       output_path = folder / output_name\n",
    "       df.to_csv(output_path, sep='\\t')\n",
    "       \n",
    "       # Collect sample IDs\n",
    "       all_ids.update(df.index)\n",
    "       \n",
    "       print(f\"Transposed: {file_path.name} -> {output_name}\")\n",
    "   \n",
    "   # Save all unique IDs to LUAD.ids.txt\n",
    "   ids_file = folder / \"LUAD.ids.txt\"\n",
    "   with open(ids_file, 'w') as f:\n",
    "       for sample_id in sorted(all_ids):\n",
    "           f.write(f\"{sample_id}\\n\")\n",
    "   \n",
    "   print(f\"Created {ids_file.name} with {len(all_ids)} unique IDs\")\n",
    "\n",
    "\n",
    "transpose_luad_files(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run move on MLOmics data:\n",
    "# Encode data\n",
    "! move-dl task=encode_data data=MLOmics\n",
    "\n",
    "# Analyze latent\n",
    "#! move-dl task=MLOmics__latent data=MLOmics\n",
    "\n",
    "# Identify assoc ks\n",
    "! move-dl task=MLOmics__id_assoc_bayes data=MLOmics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Inflammatory Bowel Disease (IBD)\n",
    "\n",
    "This is the main real world, biological dataset that we will use for the manuscript. The following analyses and cells are an updated version of our [NNFC workshop](https://github.com/RasmussenLab/MOVE/tree/nnfc-workshop). \n",
    "\n",
    "**About IBD:**\n",
    " IBDs are a group of conditions that debilitate and inflammate the gastrointestinal tract or the colon. We will use part of the data presented in the paper [_Multi-omics of the gut microbial ecosystem in inflammatory bowel diseases_](https://www.nature.com/articles/s41586-019-1237-9) by Lloyd-Price, J., Arze, C., Ananthakrishnan, A.N. et al. Their results and conclusions will guide our data analysis. \n",
    "\n",
    " The data can be obtained from [The Inflammatory Bowel Disease Multiomics Database (IBDMDB)](https://ibdmdb.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD = False\n",
    "ibdmdb_data_path = Path('./ibdmdb_data/')\n",
    "ibdmdb_data_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "ibdmdb_files = {'ibdmdb_metadata.csv':'https://g-227ca.190ebd.75bc.data.globus.org/ibdmdb/metadata/hmp2_metadata_2018-08-20.csv', #For diagnostics\n",
    "                'ibdmdb_mbx_w_metadata.biom.gz': 'https://g-227ca.190ebd.75bc.data.globus.org/ibdmdb/products/HMP2/MBX/HMP2_metabolomics_w_metadata.biom.gz', # Metabolomics with metabolite names\n",
    "                'ibdmdb_mgx.tsv.gz': 'https://g-227ca.190ebd.75bc.data.globus.org/ibdmdb/products/HMP2/MGX/2018-05-04/taxonomic_profiles.tsv.gz', # Metagenomics\n",
    "                'ibdmdb_mtx.tsv.gz': 'https://g-227ca.190ebd.75bc.data.globus.org/ibdmdb/products/HMP2/MTX/2017-12-14/ecs_3.tsv.gz', # Meta transcriptomics\n",
    "                'ibdmdb_dysbiosis.tsv': 'https://forum.biobakery.org/uploads/short-url/umwfR0kDJ6s5RXHwtIMgLaOEoOI.tsv'} # Diagnosis\n",
    "\n",
    "# Download the files the first time:\n",
    "if DOWNLOAD:\n",
    "    for filename, file in ibdmdb_files.items():\n",
    "        filename = wget.download(file, str(ibdmdb_data_path) + '/' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing the data:** \n",
    "- Process data into a MOVE-friendly format.\n",
    "- Drop columns with high missingness (more than 50% missing).\n",
    "- Keep only named metabolites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a missingness threshold\n",
    "MISSING_THRESHOLD = 0.5  # Drop columns with >25% missing\n",
    "na_value = 0.0\n",
    "\n",
    "# Step 1: Create index file from dysbiosis.tsv (first column)\n",
    "print(\"Creating index file...\")\n",
    "dysbiosis_df = pd.read_csv(ibdmdb_data_path / 'ibdmdb_dysbiosis.tsv', sep='\\t')\n",
    "index_ids = dysbiosis_df.iloc[:, 0].tolist()  # First column IDs\n",
    "index_df = pd.DataFrame({'ID': index_ids})\n",
    "index_df.to_csv(ibdmdb_data_path / 'ibdmdb.ids.txt', sep='\\t', index=False, header=None)\n",
    "print(f\"Index file created with {len(index_ids)} IDs\")\n",
    "\n",
    "# Step 2: Create dysbiosis.tsv with first and third columns\n",
    "print(\"Creating dysbiosis.tsv...\")\n",
    "dysbiosis_subset = dysbiosis_df.iloc[:, [0, 2]].copy()  # First and third columns\n",
    "dysbiosis_subset.columns = ['ID', 'dysbiosis']\n",
    "dysbiosis_subset.to_csv(ibdmdb_data_path / 'dysbiosis.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Step 3: Create mbx.tsv from BIOM file\n",
    "# Extract valid metabolite IDs and names\n",
    "with gzip.open(ibdmdb_data_path / 'ibdmdb_mbx_w_metadata.biom.gz', 'rt') as f:\n",
    "    biom_data = json.load(f)\n",
    "\n",
    "# Keep only the named metabolites\n",
    "valid_metabolites = {}\n",
    "for row in biom_data['rows']:\n",
    "    metabolite = row['metadata'].get('Metabolite')\n",
    "    if metabolite and metabolite != \"\":\n",
    "        valid_metabolites[row['id']] = metabolite\n",
    "\n",
    "# Load biom table and filter columns\n",
    "print(\"Creating mbx.tsv from BIOM file...\")\n",
    "table = biom.load_table(str(ibdmdb_data_path / 'ibdmdb_mbx_w_metadata.biom.gz'))\n",
    "mbx_df = table.to_dataframe().T\n",
    "\n",
    "# Keep only valid metabolite columns and rename them\n",
    "valid_cols = [col for col in mbx_df.columns if col in valid_metabolites]\n",
    "mbx_df = mbx_df[valid_cols]\n",
    "mbx_df.rename(columns=valid_metabolites, inplace=True)\n",
    "\n",
    "# Reset index to get sample IDs as a column\n",
    "mbx_df.reset_index(inplace=True)\n",
    "mbx_df.rename(columns={'index': 'ID'}, inplace=True)\n",
    "mbx_df.to_csv(ibdmdb_data_path / 'mbx.tsv', sep='\\t', index=False)\n",
    "\n",
    "def process_microbiome_data(file_path, output_path, data_type='mtx', na_value = na_value, MISSING_THRESHOLD = MISSING_THRESHOLD):\n",
    "    \"\"\"Process MTX or MGX data with simplified column filtering\"\"\"\n",
    "    \n",
    "    print(f\"Creating {data_type}.tsv...\")\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        df = pd.read_csv(f, sep='\\t', index_col=0, low_memory=False, dtype=str if data_type == 'mtx' else None)\n",
    "        df = df.T.reset_index().rename(columns={'index': 'ID'})\n",
    "        \n",
    "        # Clean ID column\n",
    "        df['ID'] = df['ID'].str.split('_').str[0]\n",
    "        \n",
    "        # Get data columns\n",
    "        data_cols = df.columns[1:]  # All except ID\n",
    "        \n",
    "        # Convert to numeric and handle zeros\n",
    "        for col in data_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        if data_type == 'mtx':\n",
    "            # Filter species columns (UNGROUPED with s__)\n",
    "            species_cols = [col for col in data_cols if col.startswith('UNGROUPED|') and '.s__' in col]\n",
    "            # Rename columns\n",
    "            rename_dict = {col: f\"s__{col.split('.s__')[1]}\" for col in species_cols}\n",
    "        else:\n",
    "            # MGX: Keep 0.0 as is, filter by taxonomic levels\n",
    "            species_cols = [col for col in data_cols \n",
    "                          if col.startswith('k__') and col.count('|') == 6 and col.endswith(col.split('|s__')[1])]\n",
    "            # Rename columns\n",
    "            rename_dict = {col: f\"s__{col.split('|s__')[1]}\" for col in species_cols}\n",
    "        \n",
    "        # Filter and rename\n",
    "        df_filtered = df[['ID'] + species_cols].rename(columns=rename_dict)\n",
    "        \n",
    "        # Filter by missingness\n",
    "        final_cols = df_filtered.columns[1:]  # Exclude ID\n",
    "        cols_to_keep = [col for col in final_cols \n",
    "                       if (df_filtered[col] == na_value).mean() <= MISSING_THRESHOLD]\n",
    "        \n",
    "        df_final = df_filtered[['ID'] + cols_to_keep]\n",
    "        \n",
    "        print(f\"{data_type.upper()}: {len(data_cols)} â†’ {len(species_cols)} species â†’ {len(cols_to_keep)} kept\")\n",
    "        print(f\"Final shape: {df_final.shape}\")\n",
    "    \n",
    "    df_final.to_csv(output_path, sep='\\t', index=False)\n",
    "    return df_final\n",
    "\n",
    "# Use the function for both data modalities: Step 4 and 5.\n",
    "mtx_df = process_microbiome_data(\n",
    "    ibdmdb_data_path / 'ibdmdb_mtx.tsv.gz', \n",
    "    ibdmdb_data_path / 'mtx.tsv', \n",
    "    'mtx'\n",
    ")\n",
    "\n",
    "mgx_df = process_microbiome_data(\n",
    "    ibdmdb_data_path / 'ibdmdb_mgx.tsv.gz', \n",
    "    ibdmdb_data_path / 'mgx.tsv', \n",
    "    'mgx'\n",
    ")\n",
    "\n",
    "# Step 6: Create ibd.tsv from metadata\n",
    "print(\"Creating ibd.tsv...\")\n",
    "metadata_df = pd.read_csv(ibdmdb_data_path / 'ibdmdb_metadata.csv')\n",
    "\n",
    "# Extract External ID and diagnosis columns\n",
    "ibd_df = metadata_df[['External ID', 'diagnosis']].copy()\n",
    "ibd_df.columns = ['ID', 'IBD_status']\n",
    "\n",
    "# Clean up IBD status - map to UC, CD, or Control\n",
    "def map_ibd_status(diagnosis):\n",
    "    if pd.isna(diagnosis):\n",
    "        return 'Control'\n",
    "    diagnosis_str = str(diagnosis).upper()\n",
    "    if 'UC' in diagnosis_str or 'ULCERATIVE' in diagnosis_str:\n",
    "        return 'UC'\n",
    "    elif 'CD' in diagnosis_str or 'CROHN' in diagnosis_str:\n",
    "        return 'CD'\n",
    "    else:\n",
    "        return 'Control'\n",
    "\n",
    "ibd_df['IBD_status'] = ibd_df['IBD_status'].apply(map_ibd_status)\n",
    "ibd_df.to_csv(ibdmdb_data_path / 'ibd.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Align the different files to share the same IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tsv_files(folder_path='./ibdmdb_data/'):\n",
    "    folder = Path(folder_path)\n",
    "    tsv_files = [f for f in folder.glob('*.tsv') if 'ibdmdb' not in f.name]\n",
    "    \n",
    "    # Load files and find shared IDs\n",
    "    dataframes = {}\n",
    "    all_ids = []\n",
    "    \n",
    "    for file in tsv_files:\n",
    "        df = pd.read_csv(file, sep='\\t')\n",
    "        df['ID'] = df['ID'].astype(str).str.strip()\n",
    "        dataframes[file.name] = df\n",
    "        all_ids.append(set(df['ID']))\n",
    "    \n",
    "    shared_ids = set.intersection(*all_ids)\n",
    "    print(f\"Shared IDs: {len(shared_ids)}\")\n",
    "\n",
    "    #Overwrite older ids file.\n",
    "    with open(folder_path + 'ibdmdb.ids.txt', 'w') as f:\n",
    "        for id_val in sorted(shared_ids):\n",
    "            f.write(f\"{id_val}\\n\")\n",
    "    \n",
    "    # Filter to shared IDs only, removing duplicates\n",
    "    for filename, df in dataframes.items():\n",
    "        # Keep only shared IDs and drop duplicates\n",
    "        aligned_df = df[df['ID'].isin(shared_ids)].drop_duplicates(subset=['ID']).copy()\n",
    "        output_path = folder / f\"aligned_{filename}\"\n",
    "        aligned_df.to_csv(output_path, sep='\\t', index=False)\n",
    "        print(f\"Created {output_path.name}: {len(aligned_df)} rows\")\n",
    "\n",
    "shared_ids = align_tsv_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the correlational structure in the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "path = Path(\"./ibdmdb_data/\")\n",
    "savepath = Path(\"./results_ibdmdb/\")\n",
    "# Merge the different dataframes while adding suffixes:\n",
    "mgx_df = pd.read_csv(path / 'aligned_mgx.tsv', sep='\\t')\n",
    "mtx_df = pd.read_csv(path / 'aligned_mtx.tsv', sep='\\t')\n",
    "mbx_df = pd.read_csv(path / 'aligned_mbx.tsv', sep='\\t')\n",
    "\n",
    "# Add suffixes to all non-ID columns\n",
    "mgx_df = mgx_df.rename(columns={col: f\"{col}_mgx\" for col in mgx_df.columns if col != 'ID'})\n",
    "mtx_df = mtx_df.rename(columns={col: f\"{col}_mtx\" for col in mtx_df.columns if col != 'ID'})\n",
    "\n",
    "df = mgx_df.merge(mtx_df, on='ID').merge(mbx_df, on='ID').set_index('ID')\n",
    "df\n",
    "STEP = 1\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "columns = corr_matrix.columns\n",
    "\n",
    "# Compute the linkage matrix\n",
    "linkage_matrix = linkage(corr_matrix, method='average')\n",
    "\n",
    "# Get the order of the variables\n",
    "ordered_indices = leaves_list(linkage_matrix)\n",
    "corr_matrix = corr_matrix.iloc[ordered_indices, ordered_indices]\n",
    "columns = columns[ordered_indices]\n",
    "\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "fig = plt.figure(figsize=(50, 50))\n",
    "plt.imshow(corr_matrix, cmap='seismic', vmin=-1, vmax=1)\n",
    "plt.xticks(np.arange(0,len(columns),STEP), columns[::STEP], rotation = 90, fontsize=5)\n",
    "plt.yticks(np.arange(0,len(columns),STEP), columns[::STEP],fontsize=5)\n",
    "plt.title(f'Correlation Matrix')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "fig.savefig(savepath / f\"Correlation_matrix.png\", dpi=200)\n",
    "\n",
    "# Extract the upper triangle of the correlation matrix without the diagonal\n",
    "upper_triangle_indices = np.triu_indices_from(corr_matrix, k=1)\n",
    "upper_triangle_values = corr_matrix.values[upper_triangle_indices]\n",
    "\n",
    "# Plot the frequency distribution of the correlation coefficients\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "sns.kdeplot(abs(upper_triangle_values), bw_adjust=0.5)\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Density')\n",
    "plt.title(f'Density Plot of Correlation Coefficients ')\n",
    "plt.tight_layout()\n",
    "fig.savefig(savepath / \"Correlation_distribution.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running MOVE:\n",
    "\n",
    "Since this is the main biological dataset, we will explore a bit more in detail what can be done with MOVE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run move on ibdmdb data:\n",
    "from omegaconf import OmegaConf\n",
    "from move.data import io\n",
    "from move.tasks import encode_data\n",
    "\n",
    "config = io.read_config(\"ibdmdb\", None)\n",
    "# Print data config file\n",
    "print(OmegaConf.to_yaml(config, resolve=True))\n",
    "\n",
    "# Encode data\n",
    "encode_data(config.data)\n",
    "\n",
    "path = Path(config.data.interim_data_path)\n",
    "cat_datasets, cat_names, con_datasets, con_names = io.load_preprocessed_data(path, config.data.categorical_names, config.data.continuous_names)\n",
    "dataset_names = config.data.categorical_names + config.data.continuous_names\n",
    "\n",
    "# Print shapes of the different datasets\n",
    "for dataset, dataset_name in zip(cat_datasets + con_datasets, dataset_names):\n",
    "    print(f\"{dataset_name}: {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze the latent space:**\n",
    "\n",
    "This step includes our self-implemented SHAP analysis. \n",
    "\n",
    "We will explore the hyperparameter space aiming to:\n",
    "- Identify changes in the way MOVE distributes samples in latent space.\n",
    "- Identify trends that might be shared regardless of the choice of hyperparameters.\n",
    "\n",
    "We will modify:\n",
    "- Number of latent nodes.\n",
    "- Number of hidden nodes.\n",
    "- Beta (KLD strength).\n",
    "- Weight on the loss from continuous datasets.\n",
    "    - Since equal weights leads to latent space fragmentation (i.e. categorical variables like dysbiosis or ibd status\"win\"), we can increase the importance of properly reconstructing the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze latent\n",
    "#! move-dl task=ibdmdb__latent data=ibdmdb # Command line style\n",
    "\n",
    "# Analyze latent\n",
    "results_path = \"./results_ibdmdb/results_temp/\"\n",
    "Path(results_path).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Keep original format but quote the shell command properly\n",
    "hyper_list = [(lat,hid,bet,w) for bet in [.01,.0001] for hid in [[64], [256], [512]] for lat in [16, 32] for w in [1,2]]\n",
    "\n",
    "# Loop over hyperparameters:\n",
    "for i,(lat,hid,bet,w) in enumerate(hyper_list):\n",
    "    foldername = f\"latent_{lat}_{hid}_{bet}_{w}\"\n",
    "    if foldername not in os.listdir(results_path):\n",
    "        os.mkdir(results_path + foldername)\n",
    "        \n",
    "        # Remove model file if it exists\n",
    "        model_path = \"./results_ibdmdb/latent_space/model.pt\"\n",
    "        if os.path.exists(model_path):\n",
    "            os.remove(model_path)\n",
    "        \n",
    "        # Train the model - quote the list parameter\n",
    "        cmd = f'move-dl data=ibdmdb task=ibdmdb__latent task.model.beta={bet} task.model.num_latent={lat} \"task.model.num_hidden={hid}\" data.continuous_inputs.0.weight={w} data.continuous_inputs.1.weight={w} data.continuous_inputs.2.weight={w}'\n",
    "        ! {cmd}\n",
    "        \n",
    "        # Copy results - quote the path with brackets\n",
    "        ! cp -r results_ibdmdb/latent_space/ \"{results_path}{foldername}\"\n",
    "        print(f\"Finished {foldername}\")\n",
    "    else:\n",
    "        print(f\"{foldername} was already created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rankings: \n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = Path(\"./results_ibdmdb/results_temp/\")\n",
    "savepath = Path(\"./results_ibdmdb/\")\n",
    "\n",
    "ls = \"latent_space\"\n",
    "\n",
    "datasets = [\"mgx\",\"mtx\",\"mbx\"]\n",
    "\n",
    "for folder in os.listdir(path):\n",
    "    print(folder)\n",
    "    _, lat, hid, beta, w = folder.split(\"_\")\n",
    "    for dataset in datasets:\n",
    "        df = pd.read_csv(f\"./results_ibdmdb/results_temp/latent_{lat}_{hid}_{beta}_{w}/feat_importance_aligned_{dataset}.tsv\", sep=\"\\t\").set_index(\"sample\")\n",
    "        # Order\n",
    "        top10_ids = np.argsort(np.sum(np.abs(df.values), axis=0))[::-1] #[:10] to trully get the first 10 in each hyperparam set\n",
    "        order = np.take(df.columns, top10_ids)\n",
    "        with open(savepath /  f\"SHAP_{dataset}.txt\", 'a') as f:\n",
    "            for o, feature in enumerate(order):\n",
    "                f.write(f\"{o}\\t{feature}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visualize the rankings: ####\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for dataset in datasets:\n",
    "    colnames= [\"Rank\",\"feature\"]\n",
    "\n",
    "    df = pd.read_csv(savepath / f\"SHAP_{dataset}.txt\", sep=\"\\t\", names=colnames)\n",
    "\n",
    "    df_violin = pd.DataFrame()\n",
    "\n",
    "    df_violin[\"Lists\"] = df.groupby(\"feature\")[\"Rank\"].apply(list)\n",
    "\n",
    "    df_violin['median'] = [np.median(l) for l in df_violin['Lists']]\n",
    "    #df_violin[\"mean\"] = df_violin[\"Lists\"].apply(lambda x: sum(x) / len(x))\n",
    "    df_violin = df_violin.sort_values(by=\"median\")\n",
    "    #df_violin = df_violin.drop(columns=\"median\")\n",
    "\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    parts = plt.violinplot(df_violin[\"Lists\"], showmedians=True, showextrema=False)\n",
    "\n",
    "    for pc in parts['bodies']:\n",
    "        pc.set_facecolor('#c9dbeb')\n",
    "        #pc.set_edgecolor('black')  # Optional: change edge color if desired\n",
    "        pc.set_alpha(1)  # Optional: adjust transparency\n",
    "        \n",
    "    # Customize median points\n",
    "    median_line = parts['cmedians']\n",
    "    median_line.set_linewidth(0)  # Hide the line\n",
    "    scatter_x = range(1, len(df_violin[\"Lists\"]) + 1)\n",
    "    scatter_y = [np.median(subset) for subset in df_violin[\"Lists\"]]\n",
    "    plt.scatter(scatter_x, scatter_y, color='black', marker='D', s=5, zorder=3)\n",
    "\n",
    "    plt.xticks(np.arange(1,len(df_violin[\"Lists\"]) + 1), labels=df_violin.index, rotation=90)\n",
    "    plt.ylabel(\"Rank\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(savepath / f\"{dataset}_SHAP.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune reconstructions\n",
    "! move-dl experiment=ibdmdb__tune_reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"./results_ibdmdb/tune_model/reconstruction_stats.tsv\", sep=\"\\t\")\n",
    "results.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "savepath = Path(\"./results_ibdmdb/tune_model/\")\n",
    "\n",
    "fig = plt.figure(layout=\"constrained\", figsize=(20, 8))\n",
    "subfigs = fig.subfigures(2, 1)\n",
    "\n",
    "batch_sizes = [16, 32]\n",
    "num_hidden = [[256], [512]]\n",
    "num_latent = [8, 32, 64]\n",
    "\n",
    "# Colors for batch sizes\n",
    "batch_colors = {\"train\": [\"#7570b3\", \"#4c4982\"], \"test\": [\"#1b9e77\", \"#0f6b4f\"]}\n",
    "\n",
    "for i in range(len(num_hidden)):\n",
    "    subfigs[i].suptitle(f\"Hidden: {num_hidden[i]}\")\n",
    "    \n",
    "    subset_results = results.query(f\"`task.model.num_hidden` == '{num_hidden[i]}'\")\n",
    "    \n",
    "    datasets = ['aligned_mbx', 'aligned_mgx', 'aligned_mtx', 'aligned_ibd', \"aligned_dysbiosis\"]\n",
    "    titles = ['Metabolomics', 'Metagenomics', 'Metatranscriptomics', 'IBD status', \"Dysbiosis\"]\n",
    "    \n",
    "    axes = subfigs[i].subplots(1, 5)\n",
    "    \n",
    "    legend_handles_all = []  # Store handles for legend\n",
    "    \n",
    "    for j, (dataset, title) in enumerate(zip(datasets, titles)):\n",
    "        dataset_results = subset_results.query(f\"dataset == '{dataset}'\")\n",
    "        \n",
    "        for batch_idx, batch_size in enumerate(batch_sizes):\n",
    "            batch_results = dataset_results.query(f\"`task.batch_size` == {batch_size}\")\n",
    "            \n",
    "            train_stats = []\n",
    "            test_stats = []\n",
    "            \n",
    "            for latent in num_latent:\n",
    "                train_data = batch_results.query(f\"split == 'train' and `task.model.num_latent` == {latent}\")\n",
    "                test_data = batch_results.query(f\"split == 'test' and `task.model.num_latent` == {latent}\")\n",
    "                \n",
    "                if len(train_data) > 0:\n",
    "                    train_stats.append(train_data.to_dict(orient=\"records\")[0])\n",
    "                    train_stats[-1][\"fliers\"] = []\n",
    "                \n",
    "                if len(test_data) > 0:\n",
    "                    test_stats.append(test_data.to_dict(orient=\"records\")[0])\n",
    "                    test_stats[-1][\"fliers\"] = []\n",
    "            \n",
    "            n_fields = len(train_stats)\n",
    "            offset = batch_idx * 0.2 - 0.1\n",
    "            positions_train = [x + offset for x in range(0, n_fields * 2, 2)]\n",
    "            positions_test = [x + offset for x in range(1, n_fields * 2, 2)]\n",
    "            \n",
    "            if train_stats:\n",
    "                coll1 = axes[j].bxp(train_stats, positions=positions_train, widths=0.15,\n",
    "                                   boxprops=dict(facecolor=batch_colors[\"train\"][batch_idx]), \n",
    "                                   patch_artist=True)\n",
    "                if j == 0:  # Only collect from first dataset\n",
    "                    legend_handles_all.append(coll1[\"boxes\"][0])\n",
    "            \n",
    "            if test_stats:\n",
    "                coll2 = axes[j].bxp(test_stats, positions=positions_test, widths=0.15,\n",
    "                                   boxprops=dict(facecolor=batch_colors[\"test\"][batch_idx]), \n",
    "                                   patch_artist=True)\n",
    "                if j == 0:  # Only collect from first dataset\n",
    "                    legend_handles_all.append(coll2[\"boxes\"][0])\n",
    "        \n",
    "        axes[j].set(\n",
    "            xticks=np.arange(0.5, n_fields * 2, 2), \n",
    "            xticklabels=num_latent[:n_fields], \n",
    "            ylim=(0, 1),\n",
    "            xlabel=\"# latent dimensions\", \n",
    "            title=title\n",
    "        )\n",
    "        \n",
    "        if j == 0:\n",
    "            axes[j].set_ylabel(\"Reconstruction accuracy\")\n",
    "        else:\n",
    "            axes[j].set_yticklabels([])\n",
    "        \n",
    "        if i == 0:\n",
    "            axes[j].set_xlabel(\"\")\n",
    "    \n",
    "    # Add legend to last subplot of bottom row\n",
    "    if i == 1 and legend_handles_all:\n",
    "        legend_labels = [f\"train (bs={batch_sizes[0]})\", f\"test (bs={batch_sizes[0]})\",\n",
    "                        f\"train (bs={batch_sizes[1]})\", f\"test (bs={batch_sizes[1]})\"]\n",
    "        axes[-1].legend(legend_handles_all, legend_labels, title=\"split & batch size\", fontsize='small')\n",
    "\n",
    "fig.savefig(savepath / \"Tunning_reconstructions.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify associations**\n",
    "\n",
    "Here we will identify associations between Dysbiosis (cat), Metagenomics and Metatranscriptomics (cont) and the other continuous variables. We will use both KS and Bayes methods for the continuous, and the associations will be calculated when setting the weights on all continuous data modalities to 1 or 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified identification associations analysis\n",
    "results_path = \"./results_ibdmdb/results_temp_id_assoc/\"\n",
    "Path(results_path).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Configuration\n",
    "target_ds_list = [(\"aligned_mgx\", \"plus_std\"), (\"aligned_mtx\", \"plus_std\"), (\"aligned_dysbiosis\", 1)]\n",
    "NUM_EPOCHS = 400\n",
    "N_REFITS = 24\n",
    "BAYES_SIG_THR = 0.1\n",
    "hyper_list = [(lat,hid,bet) for bet in [0.0001] for hid in [[256]] for lat in [32]]\n",
    "w_values = [1, 2]\n",
    "\n",
    "for w in w_values:\n",
    "    print(f\"=== Processing weight {w} ===\")\n",
    "    \n",
    "    for i, (lat, hid, bet) in enumerate(hyper_list):\n",
    "        hid_flat = hid[0]  # Extract 500 from [500]\n",
    "        \n",
    "        # Step 1: Run Bayes with N_REFITS for all targets\n",
    "        for TARGET_DS, TARGET_VALUE in target_ds_list:\n",
    "            ds_results_path = f\"{results_path}{TARGET_DS}_bayes_w{w}/\"\n",
    "            ! mkdir -p {ds_results_path}\n",
    "            \n",
    "            foldername = f\"id_assoc_{lat}_{hid_flat}_{bet}_{TARGET_DS}_bayes_w{w}\"\n",
    "            \n",
    "            if foldername not in os.listdir(ds_results_path):\n",
    "                ! mkdir -p {ds_results_path}{foldername}\n",
    "                print(f\"Starting Bayes {foldername}\")\n",
    "                \n",
    "                cmd = f'move-dl data=ibdmdb task=ibdmdb__id_assoc_bayes task.model.beta={bet} task.model.num_latent={lat} \"task.model.num_hidden={hid}\" task.target_dataset={TARGET_DS} task.num_refits={N_REFITS} task.target_value={TARGET_VALUE} task.sig_threshold={BAYES_SIG_THR} task.training_loop.num_epochs={NUM_EPOCHS} data.continuous_inputs.0.weight={w} data.continuous_inputs.1.weight={w} data.continuous_inputs.2.weight={w}'\n",
    "                ! {cmd}\n",
    "                \n",
    "                ! cp -r results_ibdmdb/identify_associations/ \"{ds_results_path}{foldername}\"\n",
    "                print(f\"Finished Bayes {foldername}\")\n",
    "            else:\n",
    "                print(f\"Bayes {foldername} already exists\")\n",
    "        \n",
    "        # Step 2: Store models after Bayes training\n",
    "        models_backup_path = f\"./all_models/models_{lat}_{hid_flat}_{bet}_w{w}/\"\n",
    "        ! mkdir -p {models_backup_path}\n",
    "        ! cp -r interim_data_ibdmdb/models {models_backup_path}\n",
    "        \n",
    "        # Step 3: Run KS N_REFITS times (only for continuous targets)\n",
    "        for run_idx in range(N_REFITS):\n",
    "            # Reset models directory and load single model\n",
    "            ! rm -rf interim_data_ibdmdb/models\n",
    "            ! mkdir -p interim_data_ibdmdb/models\n",
    "            ! cp \"{models_backup_path}models/model_{lat}_{run_idx}.pt\" interim_data_ibdmdb/models/model_{lat}_0.pt\n",
    "            \n",
    "            for TARGET_DS, TARGET_VALUE in target_ds_list:\n",
    "                # Skip categorical targets for KS\n",
    "                if TARGET_VALUE == 1:\n",
    "                    continue\n",
    "                    \n",
    "                ds_results_path = f\"{results_path}{TARGET_DS}_ks_w{w}/\"\n",
    "                ! mkdir -p {ds_results_path}\n",
    "                \n",
    "                foldername = f\"id_assoc_{lat}_{hid_flat}_{bet}_{TARGET_DS}_ks_{run_idx}_w{w}\"\n",
    "                \n",
    "                if foldername not in os.listdir(ds_results_path):\n",
    "                    ! mkdir -p {ds_results_path}{foldername}\n",
    "                    print(f\"Starting KS {foldername}\")\n",
    "                    \n",
    "                    cmd = f'move-dl data=ibdmdb task=ibdmdb__id_assoc_ks task.model.beta={bet} task.model.num_latent={lat} \"task.model.num_hidden={hid}\" task.target_dataset={TARGET_DS} task.num_refits=1 task.target_value={TARGET_VALUE} task.sig_threshold=0.999 task.training_loop.num_epochs={NUM_EPOCHS} data.continuous_inputs.0.weight={w} data.continuous_inputs.1.weight={w} data.continuous_inputs.2.weight={w}'\n",
    "                    ! {cmd}\n",
    "                    \n",
    "                    ! cp -r results_ibdmdb/identify_associations/ \"{ds_results_path}{foldername}\"\n",
    "                    print(f\"Finished KS {foldername}\")\n",
    "                else:\n",
    "                    print(f\"KS {foldername} already exists\")\n",
    "\n",
    "print(\"All analyses completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model - quote the list parameter\n",
    "cmd = f'move-dl data=ibdmdb task=ibdmdb__latent task.model.beta=0.0001 task.model.num_latent=16 \"task.model.num_hidden=[64]\" data.continuous_inputs.0.weight=1 data.continuous_inputs.1.weight=1 data.continuous_inputs.2.weight=1'\n",
    "! {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "target_ds_list = [(\"aligned_mgx\", \"plus_std\"), (\"aligned_mtx\", \"plus_std\"), (\"aligned_dysbiosis\", 1)]\n",
    "NUM_EPOCHS = 400\n",
    "N_REFITS = 24\n",
    "BAYES_SIG_THR = 0.1\n",
    "hyper_list = [(lat,hid,bet) for bet in [0.0001] for hid in [[256]] for lat in [32]]\n",
    "w_values = [1, 2]\n",
    "\n",
    "cmd = f'move-dl data=ibdmdb task=ibdmdb__id_assoc_bayes task.model.beta=0.0001 task.model.num_latent=32 \"task.model.num_hidden=[256]\" task.target_dataset=aligned_dysbiosis task.num_refits=24 task.target_value=1 task.sig_threshold={BAYES_SIG_THR} task.training_loop.num_epochs={NUM_EPOCHS} data.continuous_inputs.0.weight=2 data.continuous_inputs.1.weight=2 data.continuous_inputs.2.weight=2'\n",
    "! {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def ks_threshold(N, alpha):\n",
    "    return np.sqrt(-(1/N) * np.log(alpha/2))\n",
    "\n",
    "N_SAMPLES= 293\n",
    "ALPHA = 0.05\n",
    "KS_THR = ks_threshold(N_SAMPLES,ALPHA)\n",
    "\n",
    "KS_THR = 0.075\n",
    "\n",
    "def extract_folder_info(folder_name):\n",
    "    \"\"\"Extract dataset, method, weight, and run from folder names\"\"\"\n",
    "    parts = folder_name.split('_')\n",
    "    \n",
    "    # Find weight (always last part starting with 'w')\n",
    "    weight = parts[-1][1:]  # Remove 'w' prefix\n",
    "    \n",
    "    # Find method (bayes or ks)\n",
    "    method_idx = None\n",
    "    for i, part in enumerate(parts):\n",
    "        if part in ['bayes', 'ks']:\n",
    "            method_idx = i\n",
    "            method = part\n",
    "            break\n",
    "    \n",
    "    if method_idx is None:\n",
    "        return None\n",
    "    \n",
    "    # Dataset is the part before method\n",
    "    dataset = parts[method_idx - 1]\n",
    "    \n",
    "    # For ks, run is the part after method\n",
    "    run = None\n",
    "    if method == 'ks' and method_idx + 1 < len(parts) - 1:\n",
    "        run = parts[method_idx + 1]\n",
    "    \n",
    "    return {\n",
    "        'dataset': dataset,\n",
    "        'method': method,\n",
    "        'weight': weight,\n",
    "        'run': run\n",
    "    }\n",
    "\n",
    "def process_bayes_folders(results_dir):\n",
    "    \"\"\"Process all bayes folders and combine TSVs\"\"\"\n",
    "    bayes_dfs = []\n",
    "    \n",
    "    for method_folder in os.listdir(results_dir):\n",
    "        if 'bayes' not in method_folder:\n",
    "            continue\n",
    "            \n",
    "        method_path = os.path.join(results_dir, method_folder)\n",
    "        if not os.path.isdir(method_path):\n",
    "            continue\n",
    "            \n",
    "        for subfolder in os.listdir(method_path):\n",
    "            subfolder_path = os.path.join(method_path, subfolder)\n",
    "            if not os.path.isdir(subfolder_path):\n",
    "                continue\n",
    "                \n",
    "            info = extract_folder_info(subfolder)\n",
    "            if not info or info['method'] != 'bayes' or info['dataset'] == 'dysbiosis':\n",
    "                continue\n",
    "                \n",
    "            tsv_file = os.path.join(subfolder_path, f\"results_sig_assoc_bayes.tsv\")\n",
    "            if os.path.exists(tsv_file):\n",
    "                df = pd.read_csv(tsv_file, sep='\\t')\n",
    "                df['Perturbing_dataset'] = info['dataset']\n",
    "                df['Continuous_weight'] = info['weight']\n",
    "                bayes_dfs.append(df)\n",
    "    \n",
    "    if bayes_dfs:\n",
    "        return pd.concat(bayes_dfs, ignore_index=True)\n",
    "    return None\n",
    "\n",
    "def process_ks_folders(results_dir):\n",
    "    \"\"\"Process all ks folders and combine TSVs into single table\"\"\"\n",
    "    ks_dfs = []\n",
    "    \n",
    "    for method_folder in os.listdir(results_dir):\n",
    "        if 'ks' not in method_folder:\n",
    "            continue\n",
    "            \n",
    "        method_path = os.path.join(results_dir, method_folder)\n",
    "        if not os.path.isdir(method_path):\n",
    "            continue\n",
    "            \n",
    "        for subfolder in os.listdir(method_path):\n",
    "            subfolder_path = os.path.join(method_path, subfolder)\n",
    "            if not os.path.isdir(subfolder_path):\n",
    "                continue\n",
    "                \n",
    "            info = extract_folder_info(subfolder)\n",
    "            if not info or info['method'] != 'ks' or info['dataset'] == 'dysbiosis':\n",
    "                continue\n",
    "                \n",
    "            tsv_file = os.path.join(subfolder_path, f\"results_sig_assoc_ks.tsv\")\n",
    "            if os.path.exists(tsv_file):\n",
    "                df = pd.read_csv(tsv_file, sep='\\t')\n",
    "                df['Perturbing_dataset'] = info['dataset']\n",
    "                df['Continuous_weight'] = info['weight']\n",
    "                df['Run'] = info['run']\n",
    "                ks_dfs.append(df)\n",
    "    \n",
    "    if ks_dfs:\n",
    "        return pd.concat(ks_dfs, ignore_index=True)\n",
    "    return None\n",
    "\n",
    "# Main execution\n",
    "results_dir = \"./results_ibdmdb/results_temp_id_assoc\"\n",
    "\n",
    "# Process bayes folders\n",
    "bayes_combined = process_bayes_folders(results_dir)\n",
    "if bayes_combined is not None:\n",
    "    bayes_combined.to_csv(\"combined_bayes_results.tsv\", sep='\\t', index=False)\n",
    "    print(f\"Bayes results saved: {len(bayes_combined)} rows\")\n",
    "\n",
    "# Process ks folders\n",
    "ks_combined = process_ks_folders(results_dir)\n",
    "if ks_combined is not None:\n",
    "    # At the top with other variables\n",
    "    ks_combined= ks_combined[ks_combined['ks_distance'] >= KS_THR].sort_values('ks_distance', ascending=False)\n",
    "    ks_combined.to_csv(\"combined_ks_results.tsv\", sep='\\t', index=False)\n",
    "    print(f\"KS results saved: {len(ks_combined)} rows\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_sorted = ks_combined.sort_values('ks_distance', ascending=True)\n",
    "ks_sorted[ks_sorted['feature_b_dataset'] == \"aligned_mbx\"].head(25)\n",
    "                                                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous: \n",
    "\n",
    "**KS threshold as a function of the number of samples and the alpha / p-value threshold we'd like to set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to compute KS threshold\n",
    "def ks_threshold(N, alpha):\n",
    "    return np.sqrt(-(1/N) * np.log(alpha/2))\n",
    "\n",
    "# Define the range for N and alpha\n",
    "N_values = np.linspace(100, 500, 10000)  # Avoid zero to prevent division by zero\n",
    "alpha_values = np.linspace(0.001, 1, 1000)  # Avoid zero for alpha to prevent log(0)\n",
    "\n",
    "# Create a meshgrid for N and alpha\n",
    "N_grid, alpha_grid = np.meshgrid(N_values, alpha_values)\n",
    "ks_thrs = ks_threshold(N_grid, alpha_grid)\n",
    "\n",
    "# Create a 2D heatmap\n",
    "fig, ax = plt.subplots(figsize=(7,3.5))\n",
    "c = ax.pcolormesh(N_grid, alpha_grid, ks_thrs, shading='auto', cmap='RdYlBu')\n",
    "\n",
    "# Add a color bar\n",
    "fig.colorbar(c, ax=ax, label='KS Threshold')\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('Number of Samples N')\n",
    "ax.set_ylabel('Significance Threshold')\n",
    "ax.set_title('Heatmap of KS Threshold')\n",
    "# Set x-axis to logarithmic scale\n",
    "#ax.set_xscale('log')\n",
    "fig.tight_layout()\n",
    "fig.savefig('KS_thr_heatmap.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes score trends:**\n",
    "\n",
    "The Bayes score measured, at a cohort level, whether perturbing a feature A will shift the values of any other feature B up or down. The score is derived from the fraction of individuals or samples that recapitulate such behavior ($prob$). Here we plot the different scores as a function of the number of samples going up or down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "prob = np.arange(0,1.01,0.01)\n",
    "bayes_k = np.log(prob + 1e-8) - np.log(1-prob + 1e-8)\n",
    "bayes_abs = np.abs(bayes_k)\n",
    "bayes_p = np.exp(bayes_abs) / (1 + np.exp(bayes_abs))\n",
    "\n",
    "fig, axs = plt.subplots(3)\n",
    "axs[0].plot(prob,prob)\n",
    "axs[0].set_xlabel(\"prob\")\n",
    "axs[0].set_ylabel(\"prob\")\n",
    "axs[1].plot(prob, bayes_k)\n",
    "axs[1].set_xlabel(\"prob\")\n",
    "axs[1].set_ylabel(\"bayes_k\")\n",
    "axs[2].plot(prob, bayes_p)\n",
    "axs[2].set_xlabel(\"prob\")\n",
    "axs[2].set_ylabel(\"bayes_p\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residualization:**\n",
    "\n",
    "This function can be used if we want to remove the linear contributions of a number of covariates to the data before feeding the data to MOVE. Please be careful not to log2 transform residualized data, which will contain negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residualize(targets, covariates):\n",
    "    \"\"\"\n",
    "    This function trains a linear model to take into account the contributions\n",
    "    of age, sex and batch in the gene expression values (RNA counts).\n",
    "    \n",
    "    Args:\n",
    "        targets: Pandas dataframe of shape (N_samples x N_target_features) \n",
    "                 containing the target features to correct.\n",
    "        covariates: Pandas dataframe of shape (N_samples x N_covariates)\n",
    "                    containing the independent variables that we want to \n",
    "                    correct for.\n",
    "    Returns:\n",
    "        corrected_targets: Array with the linear contributions of the covariates \n",
    "                           removed.\n",
    "    \n",
    "    \"\"\"\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    # Add a constant term to the independent variables\n",
    "    covariates = sm.add_constant(covariates)\n",
    "    \n",
    "    # Fit a multivariate linear regression model\n",
    "    model = sm.OLS(targets, covariates).fit()\n",
    "    predictions = pd.DataFrame(data=model.predict(covariates))\n",
    "    predictions.columns = targets.columns\n",
    "    \n",
    "    # Get the residuals (corrected values) for each target variable\n",
    "    residuals = targets - predictions\n",
    "    \n",
    "    # Return the residuals (corrected targets)\n",
    "    return residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the reconstructed class after perturbing a categorical variable:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a, b = np.load('cat_class_baseline.npy'), np.load('cat_recon_baseline.npy')\n",
    "c, d = np.load('cat_class_perturb_0.npy'), np.load('cat_recon_perturb_0.npy')\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'class_baseline':a[:,1],'recon_baseline': b[:,1], 'class_perturb' : c[:,1], 'recon_perturb': d[:,1]})\n",
    "df.to_csv(\"Baseline_vs_reconstructed_Bayes_dysbiosis.tsv\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "move_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "596px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
