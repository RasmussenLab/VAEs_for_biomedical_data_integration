{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> On the use of VAEs for biomedical data integration <center> \n",
    "\n",
    "## Introduction\n",
    "    \n",
    ">In this project we will use MOVE (Multi-Omics Variational Autoencoder) to explore the power and potential pitfalls when using VAEs to perform multimodal data integration in biomedical settings.\n",
    ">\n",
    ">**We will cover:**\n",
    ">\n",
    ">- How the definition of the model's goal (loss) shapes its behavior.\n",
    ">- Patterns in the organization of the samples in latent space and emerging dynamics under perturbations.\n",
    ">- SHAP analyses to identify relevant features.\n",
    ">- Using _In silico_ perturbations of the inputs to identify associations between features.\n",
    ">\n",
    ">We will finally show how all of these concepts can be applied to a real biomedical setting, namely the study of the microbial ecosystem changes in patients with Inflammatory Bowel Diseases (IBD).\n",
    ">\n",
    ">**Papers of interest:**\n",
    ">    \n",
    ">- Allesøe, R.L., Lundgaard, A.T., Hernández Medina, R. *et al*. Discovery of\n",
    ">drug–omics associations in type 2 diabetes with generative deep-learning models.*Nat Biotechnol* (2023). https://www.nature.com/articles/s41587-022-01520-x\n",
    ">- Diederik P Kingma, Max Welling. Auto-Encoding Variational Bayes. https://arxiv.org/abs/1312.6114\n",
    ">- Lloyd-Price, J., Arze, C., Ananthakrishnan, A.N. et al. Multi-omics of the gut microbial ecosystem in inflammatory bowel diseases. Nature 569, 655–662 (2019). https://doi.org/10.1038/s41586-019-1237-9\n",
    ">\n",
    ">**About Variational AutoEncoders:**\n",
    ">\n",
    ">A Variational AutoEncoder (VAE) is a type of neural network aimed to compress the information stored in some inputs ($\\vec{x}$) and reconstruct it as accurately as possible in an unsupervised manner {$\\vec{x}'$}. A VAE consists in three parts: an encoder network, a decoder network, and a prior distribution that we set to be normal. The encoder will compress the input information and extract shared information between features.It is composed by the input layer, a vector of feature values for a given sample; hidden layers connecting the different features, and a latent layer. This latent layer will contain a compressed representation of our data (lower dimensionality feature vector). In VAEs, sample representations are not treated as data points, but rather distributions defined by a mean ($\\mu$) and a standard deviation ($\\sigma$) defined in a \"latent space\". However, once the model is trained we will use the mean vector as the sample's latent representation. The decoder will then reconstruct the input, recovering the high dimensional vector that constituted our sample. \n",
    ">\n",
    ">**About MOVE:**\n",
    ">\n",
    ">MOVE was developed as a software to build VAEs that can take as input multimodal data, both in the form of continuous feature values and also discrete categories. Despite the framework's flexibility, when we talk about MOVE as a network we refer to a simple MLP-based VAE containing a single hidden layer with a non-linear activation. The framework allows us to study the latent space appearance and dynamics, but also to apply *in silico* perturbations to the input in order to find associations between features. These are measured as changes in the outputs, i.e. the reconstructed inputs. \n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a designated environment\n",
    "\n",
    "We will first create an environment where we will store all the required packages, including MOVE. Make sure to run this notebook from said environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda create -n move_env python=3.9 --y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Note:_\n",
    ">\n",
    "> You might need to install ipykernel from the terminal to be able to run the cells in the notebook. You can do that as follows:\n",
    "> ```\n",
    "> pip install ipykernel\n",
    ">```\n",
    "\n",
    "**Installing MOVE:**\n",
    "\n",
    "MOVE can be found online at RasmussenLab's Github repository (https://github.com/RasmussenLab/MOVE). It can be downloaded as a pip package from the command line as follows:\n",
    "```bash\n",
    "pip install move-dl\n",
    "```\n",
    "\n",
    "> ⚠️ **MOVE source code edits**\n",
    ">\n",
    "> A few edits to MOVE's code were made for this project. These can be found in the python notebook ```MOVE_edits.ipynb``` and can be summarized as follows:\n",
    ">\n",
    "> 1) **Removing feature_mask:**\n",
    ">   When identifying associations (```identify_associations.py```) , feature_masks were removed and we only kept nan_masks.\n",
    ">\n",
    "> 2) **Corrected Cumulative distributions:**\n",
    "> Cumulative distributions obtained when using the KS method were not properly normalized. We now multiply by bin_width to obtain a distribution between 0-1. The plot_cumulative function is found in ```dataset_distributions.py```\n",
    ">\n",
    "> 3) **Check categorical reconstructions after perturbations:**\n",
    "> We noticed that categorical perturbations in the inputs induced tiny movements of the representations of the samples in latent space. Since samples did not abandon the cluster of origin, we suspected that the reconstructions would not recapitulate the perturbed state, but rather go back to the class of origin. To do that, we added a couple of lines to store the true targets and predicted targets for both baseline and perturbed inputs when identifying associations using the Bayes approach, and we now plot the categorical performance as the fraction of samples that were properly reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVE can be installed as a pip package:\n",
    "! pip install move-dl matplotlib_venn upsetplot biom-format wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages\n",
    "The first step is to load the third party packages required to perform the different tasks in this notebook. To ease readability and improve clarity, specific ```move``` functions are imported when they need to be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip\n",
    "import json\n",
    "# Main packages\n",
    "import os\n",
    "import random as rnd\n",
    "from pathlib import Path\n",
    "\n",
    "import biom\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import wget\n",
    "from matplotlib import cm\n",
    "from matplotlib_venn import venn2, venn3\n",
    "from scipy.cluster.hierarchy import leaves_list, linkage\n",
    "from sklearn.datasets import make_sparse_spd_matrix\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from upsetplot import UpSet\n",
    "\n",
    "# Reset all matplotlib settings to defaults\n",
    "plt.rcdefaults()\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sythetic datasets benchmark\n",
    "\n",
    "### Synthetic dataset creation\n",
    "\n",
    "A synthetic dataset is created as a multivariate gaussian, where different features are different components or dimensions of the Gaussian and each sample is a draw from the distribution, i.e. $x \\sim \\mathcal{N}(\\mu,\\sigma)$. Explicit stronger correlations can be added by defining some features to be linear combinations of others. Categorical variables (binary) can be obtained by setting values below the mean to zero and values above to one for a given feature.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "Given the way the dataset is constructed, associations lie in a spectrum (correlation coefficients span from 0 to 1). Adding strong and explicit relations between columns eases the classification between associated and non-associated pairs of features, as we can set a threshold below which we find no explicit correlations but only background correlations. In spite of this distinction in the nature of the correlations of the data, we can not fully consider background low correlations as negative examples of an association, since it is technically there but only fainter.\n",
    "\n",
    "**Dataset creation functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names(settings):\n",
    "    all_feature_names = [\n",
    "        f\"{key}_{i+1}\"\n",
    "        for key in settings.keys()\n",
    "        for i in range(settings[key][\"features\"])\n",
    "    ]\n",
    "    return all_feature_names\n",
    "\n",
    "\n",
    "def create_mean_profiles(settings):\n",
    "    feature_means = []\n",
    "    for key in settings.keys():\n",
    "        mean = settings[key][\"offset\"]\n",
    "        for freq, coef in zip(\n",
    "            settings[key][\"frequencies\"], settings[key][\"coefficients\"]\n",
    "        ):\n",
    "            mean += coef * (\n",
    "                np.sin(\n",
    "                    freq * np.arange(settings[key][\"features\"]) + settings[key][\"phase\"]\n",
    "                )\n",
    "                + 1\n",
    "            )\n",
    "        feature_means.extend(list(mean))\n",
    "    return feature_means\n",
    "\n",
    "\n",
    "def create_ground_truth_correlations_file(correlations):\n",
    "    sort_ids = np.argsort(abs(correlations), axis=None)[::-1]  # 1D: N x C\n",
    "    corr = np.take(correlations, sort_ids)  # 1D: N x C\n",
    "    sig_ids = sort_ids[abs(corr) > COR_THRES]\n",
    "    sig_ids = np.vstack(\n",
    "        (sig_ids // len(all_feature_names), sig_ids % len(all_feature_names))\n",
    "    ).T\n",
    "    associations = pd.DataFrame(sig_ids, columns=[\"feature_a_id\", \"feature_b_id\"])\n",
    "    a_df = pd.DataFrame(dict(feature_a_name=all_feature_names))\n",
    "    a_df.index.name = \"feature_a_id\"\n",
    "    a_df.reset_index(inplace=True)\n",
    "    b_df = pd.DataFrame(dict(feature_b_name=all_feature_names))\n",
    "    b_df.index.name = \"feature_b_id\"\n",
    "    b_df.reset_index(inplace=True)\n",
    "    associations = associations.merge(a_df, on=\"feature_a_id\", how=\"left\").merge(\n",
    "        b_df, on=\"feature_b_id\", how=\"left\"\n",
    "    )\n",
    "    associations[\"Correlation\"] = corr[abs(corr) > COR_THRES]\n",
    "    associations = associations[\n",
    "        associations.feature_a_id > associations.feature_b_id\n",
    "    ]  # Only one half of the matrix\n",
    "    return associations\n",
    "\n",
    "\n",
    "def plot_score_matrix(\n",
    "    array, feature_names, cmap=\"bwr\", vmin=None, vmax=None, label_step=5\n",
    "):\n",
    "    if vmin is None:\n",
    "        vmin = np.min(array)\n",
    "    elif vmax is None:\n",
    "        vmax = np.max(array)\n",
    "    # if ax is None:\n",
    "    fig = plt.figure(figsize=(3.5, 3.5))\n",
    "    im = plt.imshow(array, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    xticks = plt.xticks(\n",
    "        np.arange(0, len(feature_names), label_step),\n",
    "        feature_names[::label_step],\n",
    "        fontsize=10,\n",
    "        rotation=90,\n",
    "    )\n",
    "    yticks = plt.yticks(\n",
    "        np.arange(0, len(feature_names), label_step),\n",
    "        feature_names[::label_step],\n",
    "        fontsize=10,\n",
    "    )\n",
    "    layout = plt.tight_layout()\n",
    "    cbar = plt.colorbar(im, location='top')\n",
    "    cbar.set_ticks([-1,0,1], labels=[-1,0,1], size=16)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_feature_profiles(dataset, feature_means):\n",
    "    ## Plot profiles\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    plt.plot(\n",
    "        np.arange(len(feature_means)), feature_means, lw=1, marker=\".\", markersize=0\n",
    "    )\n",
    "    for sample in dataset:\n",
    "        plt.plot(\n",
    "            np.arange(len(feature_means)), sample, lw=0.1, marker=\".\", markersize=0\n",
    "        )\n",
    "    plt.xlabel(\"Feature number\")\n",
    "    plt.ylabel(\"Count number\")\n",
    "    plt.title(\"Patient specific profiles\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_feature_correlations(dataset, pairs_2_plot):\n",
    "    fig = plt.figure()\n",
    "    for f1, f2 in pairs_2_plot:\n",
    "        plt.plot(\n",
    "            dataset[:, f1],\n",
    "            dataset[:, f2],\n",
    "            lw=0,\n",
    "            marker=\".\",\n",
    "            markersize=1,\n",
    "            label=f\"{correlations[f1,f2]:.2f}\",\n",
    "        )\n",
    "    plt.xlabel(\"Feature A\")\n",
    "    plt.ylabel(\"Feature B\")\n",
    "    plt.legend(\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, -0.1),\n",
    "        fancybox=True,\n",
    "        shadow=True,\n",
    "        ncol=5,\n",
    "    )\n",
    "    plt.title(\"Feature correlations\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def save_splitted_datasets(\n",
    "    settings: dict, PROJECT_NAME, dataset, all_feature_names, n_samples, outpath\n",
    "):\n",
    "    # Save index file\n",
    "    index = pd.DataFrame({\"ID\": list(np.arange(1, n_samples + 1))})\n",
    "    index.to_csv(outpath / f\"random.{PROJECT_NAME}.ids.txt\", index=False, header=False)\n",
    "    # Save continuous files\n",
    "    df = pd.DataFrame(\n",
    "        dataset, columns=all_feature_names, index=list(np.arange(1, n_samples + 1))\n",
    "    )\n",
    "    cum_feat = 0\n",
    "    for key in settings.keys():\n",
    "        df_feat = settings[key][\"features\"]\n",
    "        df_cont = df.iloc[:, cum_feat : cum_feat + df_feat]\n",
    "        df_cont.insert(0, \"ID\", np.arange(1, n_samples + 1))\n",
    "        df_cont.to_csv(\n",
    "            outpath / f\"random.{PROJECT_NAME}.{key}.tsv\", sep=\"\\t\", index=False\n",
    "        )\n",
    "        cum_feat += df_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create high dimensional sythetic dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Hyperparameters ####################################\n",
    "PROJECT_NAME = \"random_all_sim\"\n",
    "MODE = \"linear\"  # \"non-linear\"\n",
    "HIGH_CORR = True  # Adding explicit correlations between columns\n",
    "SEED_1 = 1  # 1234\n",
    "np.random.seed(SEED_1)\n",
    "rnd.seed(SEED_1)\n",
    "\n",
    "COV_ALPHA = 0.99  # Covariance matrix sparsity\n",
    "N_SAMPLES = 5000\n",
    "\n",
    "SETTINGS = {\n",
    "    \"Continuous_A\": {\n",
    "        \"features\": 50,\n",
    "        \"frequencies\": [0.002, 0.01, 0.02],\n",
    "        \"coefficients\": [500, 100, 50],\n",
    "        \"phase\": 0,\n",
    "        \"offset\": 700,\n",
    "    },\n",
    "    \"Continuous_B\": {\n",
    "        \"features\": 100,\n",
    "        \"frequencies\": [0.001, 0.05, 0.08],\n",
    "        \"coefficients\": [80, 20, 10],\n",
    "        \"phase\": np.pi / 2,\n",
    "        \"offset\": 300,\n",
    "    },\n",
    "    \"Categorical_A\": {\n",
    "        \"features\": 1,\n",
    "        \"frequencies\": [0.1, 0.5, 0.8],\n",
    "        \"coefficients\": [0.2, 0.1, 0.05],\n",
    "        \"phase\": np.pi / 2,\n",
    "        \"offset\": 10,\n",
    "    },\n",
    "    \"Categorical_B\": {\n",
    "        \"features\": 1,\n",
    "        \"frequencies\": [0.01, 0.5, 0.08],\n",
    "        \"coefficients\": [10, 0.1, 0.05],\n",
    "        \"phase\": np.pi,\n",
    "        \"offset\": 10,\n",
    "    },\n",
    "}\n",
    "\n",
    "COR_THRES = 0.02\n",
    "PAIRS_OF_INTEREST = [(1, 2), (3, 4)]  # ,(77,75),(99,70),(38,2),(67,62)]\n",
    "\n",
    "# Path to store output files\n",
    "outpath = Path(\"./synthetic_data/\")\n",
    "outpath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "################################## Main script ##################################\n",
    "\n",
    "# Add all datasets in a single matrix:\n",
    "all_feature_names = get_feature_names(SETTINGS)\n",
    "feat_means = create_mean_profiles(SETTINGS)\n",
    "\n",
    "# Covariance matrix definition\n",
    "if MODE == \"linear\":\n",
    "    covariance_matrix = make_sparse_spd_matrix(\n",
    "        n_dim=len(all_feature_names),\n",
    "        alpha=COV_ALPHA,\n",
    "        smallest_coef=0,\n",
    "        largest_coef=1,\n",
    "        norm_diag=False,\n",
    "        random_state=SEED_1,\n",
    "    )\n",
    "elif MODE == \"non-linear\":\n",
    "    covariance_matrix = np.identity(len(all_feature_names))\n",
    "\n",
    "ABS_MAX = np.max(abs(covariance_matrix))\n",
    "fig = plot_score_matrix(\n",
    "    covariance_matrix, all_feature_names, vmin=-ABS_MAX, vmax=ABS_MAX\n",
    ")\n",
    "fig.savefig(outpath / f\"Covariance_matrix_{PROJECT_NAME}.svg\")\n",
    "\n",
    "# Create the dataset\n",
    "dataset = np.random.multivariate_normal(feat_means, covariance_matrix, N_SAMPLES)\n",
    "\n",
    "# Add non-linearities\n",
    "if MODE == \"non-linear\":\n",
    "    for i, j in PAIRS_OF_INTEREST:\n",
    "        freq = np.random.choice([4, 5, 6])\n",
    "        dataset[:, i] += np.sin(freq * dataset[:, j])\n",
    "\n",
    "# No scaling in the dataset creation! It will be handled in the data preprocessing of the move pipeline.\n",
    "scaled_dataset = dataset\n",
    "\n",
    "if HIGH_CORR:  # The last half of the features are combinations of the first half:\n",
    "    for i in range(scaled_dataset.shape[1] // 2):\n",
    "        col_1 = np.random.choice(range(scaled_dataset.shape[1] // 2))\n",
    "        col_2 = np.random.choice(range(scaled_dataset.shape[1] // 2))\n",
    "        scaled_dataset[:, i + scaled_dataset.shape[1] // 2] = (\n",
    "            scaled_dataset[:, col_1] + scaled_dataset[:, col_2]\n",
    "        ) / 2 + np.random.normal()\n",
    "\n",
    "# Binarize the categorical dataset\n",
    "NUM_CAT = SETTINGS[\"Categorical_A\"][\"features\"] + SETTINGS[\"Categorical_B\"][\"features\"]\n",
    "columns_to_binarize = scaled_dataset[:, -NUM_CAT:]\n",
    "\n",
    "# Compute the mean of each of the categorical columns\n",
    "means = columns_to_binarize.mean(axis=0)\n",
    "\n",
    "# Apply the binarization\n",
    "scaled_dataset[:, -NUM_CAT:] = (columns_to_binarize > means).astype(int)\n",
    "\n",
    "# Visualize the actual correlations in the data\n",
    "correlations = np.corrcoef(scaled_dataset, rowvar=False)\n",
    "fig = plot_score_matrix(correlations, all_feature_names, vmin=-1, vmax=1, label_step=20)\n",
    "fig.savefig(outpath / f\"Correlations_{PROJECT_NAME}.svg\", dpi=200)\n",
    "\n",
    "# Sort correlations by absolute value\n",
    "associations = create_ground_truth_correlations_file(correlations)\n",
    "associations.to_csv(outpath / f\"changes.{PROJECT_NAME}.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Plot feature profiles per sample\n",
    "fig = plot_feature_profiles(scaled_dataset, feat_means)\n",
    "fig.savefig(outpath / \"Multi-omic_profiles.svg\")\n",
    "\n",
    "## Plot correlations\n",
    "fig = plot_feature_correlations(dataset, PAIRS_OF_INTEREST)\n",
    "fig.savefig(outpath / \"Feature_correlations.svg\")\n",
    "\n",
    "fig = plot_feature_correlations(scaled_dataset, PAIRS_OF_INTEREST)\n",
    "fig.savefig(outpath / \"Feature_correlations_scaled.svg\")\n",
    "\n",
    "# Write tsv files with feature values for all samples in both datasets:\n",
    "save_splitted_datasets(\n",
    "    SETTINGS, PROJECT_NAME, scaled_dataset, all_feature_names, N_SAMPLES, outpath\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOVE \n",
    "\n",
    "The MOVE pipeline consists in the following steps: \n",
    "1) **Encoding the data**\n",
    "2) **Analyze latent space**\n",
    "3) **Identify associations between features**\n",
    "- Associations between a given categorical feature (e.g. Is the patient taking a drug? Yes / No) against the contiuous features.\n",
    "- Associations between a given continuous feature vs. the rest of continuous features.\n",
    "\n",
    "The following description of the different steps is mostly taken from the [NNFC workshop](https://github.com/RasmussenLab/MOVE/tree/nnfc-workshop), feel free to look at that as well!\n",
    "\n",
    "### 1) Encode data\n",
    "\n",
    "The first step in order to run MOVE is to encode the data in a format that the model can understand.\n",
    "\n",
    "To encode the data we store all datasets in a TSV format. Each table needs to have a shape `N` &times; `M`, i.e. `N` rows and `M` columns where `N` is the number of samples/individuals and `M` is the number of features.\n",
    "\n",
    "> 📡 **How is data encoded?**\n",
    ">\n",
    "> **_Categorical data is one-hot encoded._** _For a feature like drug status , which has discrete values/categories (e.g., yes or no), we encode these categories as_ binary bit flags. _This means each category is assigned a value starting with one, and then represented in binary format (with zeros and ones)._\n",
    ">\n",
    ">_A useful property of flags is that they do not have hierarchy; they are incompatible with \"<\" or \">\" operators. So (in our example), yes would not be considered more or less important than no._\n",
    ">\n",
    "> **_Continuous data can be z-score normalized_**, _meaning that each feature can be rescaled to have zero mean and unit variance:_\n",
    ">\n",
    ">  $$ Z = \\frac{x-\\mu}{\\sigma} $$\n",
    "> Where $x$ is the vector of feature values for all samples, $\\mu$ and $\\sigma$ its mean and standard deviation, respectively.\n",
    "> We can either transform the data beforehand or set the config flags for log2 transformation and scaling to True.\n",
    "\n",
    "\n",
    "The first step is to read the configuration called `random_continuous_paper` and specify the pre-defined task called `encode_data`.\n",
    "\n",
    "⚠️ Remember that the notebook takes user-defined configs in a `config/data` directory located in the current working directory.\n",
    "\n",
    "\n",
    "### 2) Latent space analyses:\n",
    "\n",
    "This section trains MOVE to integrate the data into a latent space. We will then plot the results and find the important variables for the integration using SHAP analysis.\n",
    "\n",
    "> ℹ️ About SHAP analysis.\n",
    ">\n",
    "> There are many ways to identify the most important features in the data, or the set of features that the model will use the most when encoding the data into a compressed/latent representation. One of them is SHAP (SHapley Additive exPlanations) analysis.\n",
    ">\n",
    "> This method measures how much do samples move in latent space when removing one variable at a time from the input. If the model gives a lot of importance to an input variable, e.g. the concentration of a metabolite, removing it from the input will lead to a significant movement of the samples in latent space (i.e., wide band in the SHAP plots, impact on latent space). On the other hand, if an input variable is not really needed, the model will \"ignore\" it and hence the latent space representation of the samples will not change much when that feature is not there anymore (impact on latent space close to 0).\n",
    "\n",
    "\n",
    "\n",
    "As in previous examples, first we need to read our configuration files and then we can run the `random_continuous_paper__latent` task. You can have a look at the file in ```config/task/random_continuous_paper__latent.yaml```.\n",
    "\n",
    "\n",
    ">_📖 For the advanced reader:_ _This config file, in addition, explicitly sets the learning rate to be 1e-4 and introduces the factors beta (controlling how variational do we make the autoencoder) and the Kullback-Leibler divergence warmup steps, which will gradually introduce the loss term that pushes the samples towards the center of the latent space._\n",
    "\n",
    "\n",
    "⚠️  If you get a similar error:\n",
    "\n",
    "```\n",
    "RuntimeError: Error(s) in loading state_dict for VAE:\n",
    "size mismatch for encoderlayers.0.weight: copying a param with shape torch.Size([720, 1342]) from checkpoint, the shape in current model is torch.Size([900, 1342]).\n",
    "```\n",
    "\n",
    "Erase the model.pt file in the results folder, e.g. ```results_cont_paper/latent_space```\n",
    "\n",
    "### 3) Identify associations between features: \n",
    "\n",
    "Lastly, we will use what the model has learnt to identify entangled or associated variables.\n",
    "\n",
    "When perturbing the input value of a certain feature for all samples, the latent representation of said samples will change, and so will their reconstructions. We can track the induced shifts in output values to identify the features that were affected the most when perturbing the original feature, at a cohort level.\n",
    "\n",
    "> ℹ️ We will use use the [Bayesian decision theory-based approach](https://www.nature.com/articles/s41587-022-01520-x#Sec15), presented in the Methods section of the original paper. We will also use an approach based on Kolmogorov-Smirnov distances between the feature reconstruction distributions, comparing them before and after perturbing an input feature.\n",
    "\n",
    "\n",
    "MOVE can be run in a jupyter-notebook friendly manner by importing specific functions from the package as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running MOVE on synthetic data\n",
    "from move.data import io\n",
    "from move.tasks import analyze_latent, encode_data, identify_associations\n",
    "\n",
    "# Configuration mapping\n",
    "configs = {\n",
    "    \"encode\": (\"random_continuous_paper\", \"encode_data\"),\n",
    "    \"latent\": (\"random_continuous_paper\", \"random_continuous_paper__latent\"),\n",
    "    \"ttest\": (\"random_continuous_paper\", \"random_continuous_paper__id_assoc_ttest\"),\n",
    "    \"bayes\": (\"random_continuous_paper\", \"random_continuous_paper__id_assoc_bayes\"),\n",
    "    \"ks\": (\"random_continuous_paper\", \"random_continuous_paper__id_assoc_ks\"),\n",
    "}\n",
    "\n",
    "# Read all configs at once\n",
    "config_data = {name: io.read_config(*params) for name, params in configs.items()}\n",
    "\n",
    "# Execute tasks\n",
    "encode_data(config_data[\"encode\"].data)\n",
    "analyze_latent(config_data[\"latent\"])\n",
    "\n",
    "# Run all association identification methods\n",
    "for method in [\"ttest\", \"bayes\", \"ks\"]:\n",
    "    identify_associations(config_data[method])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, move can be run using a command line style as follows. Please note that the folder structure must be such that move can find the config files.\n",
    "\n",
    "```python\n",
    "# Encode data\n",
    "! move-dl task=encode_data data=random_continuous_paper \n",
    "# Identify assoc bayes\n",
    "! move-dl task=random_continuous_paper__id_assoc_bayes data=random_continuous_paper\n",
    "# Identify assoc KS\n",
    "! move-dl task=random_continuous_paper__id_assoc_ks data=random_continuous_paper \n",
    "# Identify assoc t-test\n",
    "! move-dl task=random_continuous_paper__id_assoc_ttest data=random_continuous_paper \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing methods to identify associations\n",
    "\n",
    "We just run MOVE using the three methods to identify associations between features (t-test, Bayes and KS). The resulting tsv files can be compared with the ground truths file to benchmark their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a colormap from seaborn\n",
    "cmap = sns.color_palette(\"Dark2\", 3, as_cmap=False)  # False for a list of colors\n",
    "\n",
    "# Set the color cycle for matplotlib using the colormap\n",
    "plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(color=cmap)\n",
    "\n",
    "\n",
    "##################################### Functions #############################################\n",
    "def plot_confusion_matrix(cm, target_names, cmap=None, normalize=False):\n",
    "    \"\"\"Function that plots the confusion matrix given cm.\"\"\"\n",
    "\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap(\"Blues\")\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 3))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=0, fontsize=12)\n",
    "        plt.yticks(tick_marks, target_names, fontsize=12)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                \"{:0.4f}\".format(cm[i, j]),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                fontsize=14,\n",
    "            )\n",
    "        else:\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                \"{:,}\".format(cm[i, j]),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                fontsize=14,\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\", fontsize=14)\n",
    "    plt.xlabel(\n",
    "        \"Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}\".format(\n",
    "            accuracy, misclass\n",
    "        ),\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def classify_associations(target_file, assoc_tuples):\n",
    "    self_assoc = 0  # Self associations\n",
    "    found_assoc_dict = {}\n",
    "    false_assoc_dict = {}\n",
    "    tp_fp = np.array([[0, 0]])\n",
    "    with open(target_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line[0] != \"f\":\n",
    "                splitline = line.strip().split(\"\\t\")\n",
    "                feat_a = splitline[2]\n",
    "                feat_b = splitline[3]\n",
    "                score = abs(float(splitline[5]))\n",
    "                if feat_a == feat_b:  # Self associations will not be counted\n",
    "                    self_assoc += 1\n",
    "                else:\n",
    "                    if (feat_a, feat_b) in assoc_tuples:\n",
    "                        found_assoc_dict[(feat_a, feat_b)] = score\n",
    "                        if (\n",
    "                            feat_b,\n",
    "                            feat_a,\n",
    "                        ) not in found_assoc_dict.keys():  # If we had not found it yet\n",
    "                            tp_fp = np.vstack((tp_fp, tp_fp[-1] + [0, 1]))\n",
    "                    elif (feat_a, feat_b) not in assoc_tuples:\n",
    "                        false_assoc_dict[(feat_a, feat_b)] = score\n",
    "                        if (feat_b, feat_a) not in false_assoc_dict.keys():\n",
    "                            tp_fp = np.vstack((tp_fp, tp_fp[-1] + [1, 0]))\n",
    "\n",
    "    # Remove duplicated associations:\n",
    "    for i, j in list(found_assoc_dict.keys()):\n",
    "        if (j, i) in found_assoc_dict.keys():\n",
    "            del found_assoc_dict[\n",
    "                (j, i)\n",
    "            ]  # remove the weakest direction for the association\n",
    "\n",
    "    for i, j in list(false_assoc_dict.keys()):\n",
    "        if (j, i) in false_assoc_dict.keys():\n",
    "            del false_assoc_dict[(j,i)]\n",
    "\n",
    "    return self_assoc, found_assoc_dict, false_assoc_dict, tp_fp\n",
    "\n",
    "\n",
    "def create_confusion_matrix(n_feat, associations, real_assoc, false_assoc):\n",
    "    cm = np.empty((2, 2))\n",
    "    # TN: only counting the upper half matrix (non doubled associations)\n",
    "    cm[0, 0] = (n_feat * n_feat - n_feat) / 2 - (\n",
    "        associations + false_assoc\n",
    "    )  # Diagonal is discarded\n",
    "    cm[0, 1] = false_assoc\n",
    "    cm[1, 0] = associations - real_assoc\n",
    "    cm[1, 1] = real_assoc\n",
    "\n",
    "    return cm\n",
    "\n",
    "\n",
    "def get_precision_recall(found_assoc_dict, false_assoc_dict, associations):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # True Positives\n",
    "    for score in found_assoc_dict.values():\n",
    "        y_true.append(1)\n",
    "        y_pred.append(score)\n",
    "    # False Positives\n",
    "    for score in false_assoc_dict.values():\n",
    "        y_true.append(0)\n",
    "        y_pred.append(score)\n",
    "    # False negatives\n",
    "    for _ in range(associations - len(found_assoc_dict)):\n",
    "        y_true.append(1)\n",
    "        y_pred.append(0)\n",
    "\n",
    "    precision, recall, thr = precision_recall_curve(\n",
    "        y_true, y_pred\n",
    "    )  # thr will tell us score values\n",
    "    avg_prec = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    return precision, recall, thr, avg_prec\n",
    "\n",
    "\n",
    "def plot_precision_recall(precision, recall, avg_prec, label, ax):\n",
    "    ax.scatter(\n",
    "        recall,\n",
    "        precision,\n",
    "        lw=0,\n",
    "        marker=\".\",\n",
    "        s=5,\n",
    "        edgecolors=\"none\",\n",
    "        label=f\"{label} - APS:{avg_prec:.2f}\",\n",
    "    )\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_thr_recall(thr, recall, label, ax):\n",
    "    ax.scatter(recall[:-1], thr, lw=0, marker=\".\", s=5, edgecolors=\"none\", label=label)\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_TP_vs_FP(tp_fp, label, ax):\n",
    "    ax.scatter(tp_fp[:, 0], tp_fp[:, 1], s=2, label=label, edgecolors=\"none\")\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_filling_order(order_list, last_rank=None):\n",
    "\n",
    "    if last_rank is None:\n",
    "        last_rank = len(order_list)\n",
    "    fig = plt.figure()\n",
    "    order_img = np.zeros((np.max(order_list), len(order_list)))\n",
    "    for i, element in enumerate(order_list):\n",
    "        order_img[element - 1, i:] = 1\n",
    "\n",
    "    plt.imshow(order_img[:last_rank, :], cmap=\"binary\")\n",
    "    plt.xlabel(\"Correct prediction number\")\n",
    "    plt.ylabel(\"Association ranking\")\n",
    "    plt.plot(np.arange(last_rank), np.arange(last_rank))\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_effect_size_matching(\n",
    "    assoc_tuples_dict, found_assoc_dict, label, ALGORITHM, ax\n",
    "):\n",
    "\n",
    "    ground_truth_effects = [\n",
    "        assoc_tuples_dict[key] for key in list(found_assoc_dict.keys())\n",
    "    ]\n",
    "    predicted_effects = np.array(list(found_assoc_dict.values()))\n",
    "\n",
    "    if ALGORITHM == \"ttest\":\n",
    "        # Eq 15 on https://doi.org/10.1146/annurev-statistics-031017-100307\n",
    "        predicted_effects = [-np.log10(p) if p != 0 else -1 for p in predicted_effects]\n",
    "        predicted_effects[predicted_effects == -1] = np.max(\n",
    "            predicted_effects\n",
    "        )  # Change zeros for max likelihood, -1 as dummy value\n",
    "        predicted_effects = np.array(predicted_effects)\n",
    "\n",
    "    max, min = np.max(predicted_effects), np.min(predicted_effects)\n",
    "    standarized_pred_effects = (predicted_effects - min) / (max - min)\n",
    "    ax.scatter(\n",
    "        ground_truth_effects,\n",
    "        standarized_pred_effects,\n",
    "        s=12,\n",
    "        edgecolors=\"none\",\n",
    "        label=label,\n",
    "    )\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_venn_diagram(venn, ax, mode=\"all\", scale=\"log\"):\n",
    "    sets = [set(venn[key][mode]) for key in list(venn.keys())]\n",
    "    labels = (key for key in list(venn.keys()))\n",
    "\n",
    "    if len(venn) == 2:\n",
    "        venn2(sets, labels, ax=ax)\n",
    "    elif len(venn) == 3:\n",
    "        venn3(sets, labels, ax=ax)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported number of input files.\")\n",
    "\n",
    "\n",
    "def plot_upsetplot(venn, assoc_tuples):\n",
    "\n",
    "    all_assoc = set(\n",
    "        [\n",
    "            association\n",
    "            for ALGORITHM in venn.keys()\n",
    "            for association in venn[ALGORITHM][\"all\"]\n",
    "        ]\n",
    "    )\n",
    "    columns = [\"ground truth\"]\n",
    "    columns.extend([ALGORITHM for ALGORITHM in list(venn.keys())])\n",
    "\n",
    "    df = {}\n",
    "    for association in all_assoc:\n",
    "        df[association] = []\n",
    "\n",
    "        if association in assoc_tuples:\n",
    "            df[association].append(\"TP\")\n",
    "        else:\n",
    "            df[association].append(\"FP\")\n",
    "\n",
    "        for ALGORITHM in list(venn.keys()):\n",
    "            if association in venn[ALGORITHM][\"all\"]:\n",
    "                df[association].append(1)\n",
    "            else:\n",
    "                df[association].append(0)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(df, orient=\"index\", columns=columns)\n",
    "    df = df.set_index([pd.Index(df[ALGORITHM] == 1) for ALGORITHM in list(venn.keys())])\n",
    "    upset = UpSet(df, intersection_plot_elements=0, show_counts=True)\n",
    "\n",
    "    upset.add_stacked_bars(\n",
    "        by=\"ground truth\",\n",
    "        colors=cm.Pastel1,\n",
    "        title=\"Count by ground truth value\",\n",
    "        elements=10,\n",
    "    )\n",
    "\n",
    "    return upset\n",
    "\n",
    "\n",
    "###################################### Main code ################################################\n",
    "\n",
    "\n",
    "def main(args_list):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Read two files with ground truth associations and predicted associations.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--perturbed\",\n",
    "        metavar=\"pert\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"perturbed feature names\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-n\",\n",
    "        \"--features\",\n",
    "        metavar=\"feat\",\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help=\"total number of features\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-r\",\n",
    "        \"--reference\",\n",
    "        metavar=\"ref\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"path to the ground truth associations file\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-o\",\n",
    "        \"--outpath\",\n",
    "        metavar=\"outpath\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"path where figures will be saved\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-t\",\n",
    "        \"--targets\",\n",
    "        metavar=\"tar\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        nargs=\"+\",\n",
    "        help=\"path to the predicted associations files\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(args_list)\n",
    "\n",
    "    # Defining main performance evaluation figures:\n",
    "    fig_0, ax_0 = plt.subplots(figsize=(5, 5))\n",
    "    fig_1, ax_1 = plt.subplots(figsize=(5, 5))\n",
    "    fig_2, ax_2 = plt.subplots(figsize=(5, 5))\n",
    "    fig_3, ax_3 = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    assoc_tuples_dict = {}\n",
    "\n",
    "    # Reading the file with the ground truth changes:\n",
    "    with open(args.reference, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line[0] != \"f\" and line[0] != \"n\":\n",
    "                splitline = line.strip().split(\"\\t\")\n",
    "                feat_a = splitline[2]\n",
    "                feat_b = splitline[3]\n",
    "                assoc_strength = abs(float(splitline[4]))\n",
    "                # Only can detect associations with perturbed features\n",
    "                if args.perturbed in feat_a or args.perturbed in feat_b:\n",
    "                    assoc_tuples_dict[(feat_a, feat_b)] = assoc_strength\n",
    "                    assoc_tuples_dict[(feat_b, feat_a)] = assoc_strength\n",
    "\n",
    "    associations = int(len(assoc_tuples_dict) / 2)\n",
    "    venn = {}\n",
    "\n",
    "    # Count and save found associations\n",
    "    for target_file in args.targets:\n",
    "        ALGORITHM = target_file.split(\"/\")[-1].split(\"_\")[3][:-4]\n",
    "        self_assoc, found_assoc_dict, false_assoc_dict, tp_fp = classify_associations(\n",
    "            target_file, list(assoc_tuples_dict.keys())\n",
    "        )\n",
    "        real_assoc = len(found_assoc_dict)  # True predicted associations\n",
    "        false_assoc = len(false_assoc_dict)  # False predicted associations\n",
    "        total_assoc = real_assoc + false_assoc\n",
    "\n",
    "        venn[ALGORITHM] = {}\n",
    "        venn[ALGORITHM][\"correct\"] = list(found_assoc_dict.keys())\n",
    "        venn[ALGORITHM][\"all\"] = list(found_assoc_dict.keys()) + list(\n",
    "            false_assoc_dict.keys()\n",
    "        )\n",
    "\n",
    "        # Assess ranking of associations (they are doubled in assoc_tuples):\n",
    "        order_list = [\n",
    "            list(assoc_tuples_dict.keys()).index((feat_a, feat_b)) // 2\n",
    "            for (feat_a, feat_b) in list(found_assoc_dict.keys())\n",
    "        ]\n",
    "        fig = plot_filling_order(order_list)\n",
    "        fig.savefig(f\"Order_image_{ALGORITHM}.png\", dpi=200)\n",
    "\n",
    "        ax_0 = plot_effect_size_matching(\n",
    "            assoc_tuples_dict, found_assoc_dict, ALGORITHM, ALGORITHM, ax_0\n",
    "        )\n",
    "\n",
    "        # Plot confusion matrix:\n",
    "        cm = create_confusion_matrix(\n",
    "            args.features, associations, real_assoc, false_assoc\n",
    "        )\n",
    "        fig = plot_confusion_matrix(\n",
    "            cm, [\"No assoc\", \"Association\"], cmap=None, normalize=False\n",
    "        )\n",
    "\n",
    "        fig.savefig(f\"Confusion_matrix_{ALGORITHM}.png\", dpi=100, bbox_inches=\"tight\")\n",
    "\n",
    "        # Plot precision-recall and TP-FP curves\n",
    "        precision, recall, thr, avg_prec = get_precision_recall(\n",
    "            found_assoc_dict, false_assoc_dict, associations\n",
    "        )\n",
    "\n",
    "        ax_1 = plot_precision_recall(precision, recall, avg_prec, ALGORITHM, ax_1)\n",
    "        ax_2 = plot_TP_vs_FP(tp_fp, ALGORITHM, ax_2)\n",
    "        ax_3 = plot_thr_recall(thr, recall, ALGORITHM, ax_3)\n",
    "\n",
    "        # Write results:\n",
    "        with open(\"Performance_evaluation_summary_results.txt\", \"a\") as f:\n",
    "            f.write(f\" File:  {target_file}\\n\")\n",
    "            f.write(\n",
    "                f\"Ground truth detectable associations (i.e. involving perturbed feature,{args.perturbed}):{associations}\\n\"\n",
    "            )\n",
    "            f.write(\n",
    "                f\"{total_assoc} unique associations found\\n{self_assoc} self-associations were found before filtering\\n{real_assoc} were real associations\\n{false_assoc} were either false or below the significance threshold\\n\"\n",
    "            )\n",
    "            # print(\"Correct associations:\\n\", found_assoc_tuples, \"\\n\")\n",
    "            f.write(\n",
    "                f\"Sensitivity:{real_assoc}/{associations} = {real_assoc/associations}\\n\"\n",
    "            )\n",
    "            f.write(\n",
    "                f\"Precision:{real_assoc}/{total_assoc} = {(real_assoc)/total_assoc}\\n\"\n",
    "            )\n",
    "            f.write(f\"Order list:{order_list}\\n\\n\")\n",
    "            f.write(\"______________________________________________________\\n\")\n",
    "\n",
    "    # Edit figures: layout\n",
    "    ax_0.set_xlabel(\"Real effect\")\n",
    "    ax_0.set_ylabel(\"Predicted effect\")\n",
    "    ax_0.set_ylim((-0.02, 1.02))\n",
    "    ax_0.set_xlim((0, 1.02))\n",
    "    ax_0.legend(\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 1.1),\n",
    "        ncol=3,\n",
    "        fancybox=True,\n",
    "        shadow=True,\n",
    "    )\n",
    "\n",
    "    ax_1.set_xlabel(\"Recall\")\n",
    "    ax_1.set_ylabel(\"Precision\")\n",
    "    ax_1.legend()\n",
    "    ax_1.set_ylim((0, 1.05))\n",
    "    ax_1.set_xlim((0, 1.05))\n",
    "\n",
    "    ax_2.set_xlabel(\"False Positives\")\n",
    "    ax_2.set_ylabel(\"True Positives\")\n",
    "    ax_2.set_aspect(\"auto\")\n",
    "\n",
    "    ax_3.set_ylabel(\"Threshold\")\n",
    "    ax_3.set_xlabel(\"Recall\")\n",
    "\n",
    "    # Save main figures:\n",
    "    fig_0.savefig(args.outpath + \"Effect_size_matchin.svg\", dpi=200)\n",
    "    fig_1.savefig(args.outpath + \"Precision_recall.svg\", dpi=200)\n",
    "    fig_2.savefig(args.outpath + \"TP_vs_FP.svg\", dpi=200)\n",
    "    fig_3.savefig(args.outpath + \"thr_vs_recall.svg\", dpi=200)\n",
    "\n",
    "    # Plotting venn diagram:\n",
    "    if len(venn) == 2 or len(venn) == 3:\n",
    "        fig_v, ax_v = plt.subplots(figsize=(4, 4))\n",
    "        ax_v = plot_venn_diagram(venn, ax_v, mode=\"correct\")\n",
    "        fig_v.savefig(args.outpath + \"Venn_diagram.svg\", dpi=200)\n",
    "\n",
    "    # Plotting UpSet plot\n",
    "    upset = plot_upsetplot(venn, list(assoc_tuples_dict.keys()))\n",
    "    upset.plot()\n",
    "    plt.savefig(args.outpath + \"UpSet.svg\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_list = [\n",
    "    \"-p\",\n",
    "    \"Continuous_A_2\",\n",
    "    \"-n\",\n",
    "    \"36\",\n",
    "    \"-r\",\n",
    "    \"./synthetic_data/changes.random_all_sim.txt\",\n",
    "    \"-o\",\n",
    "    \"./synthetic_data/\",\n",
    "    \"-t\",\n",
    "    \"./results_cont_paper/identify_associations/results_sig_assoc_ttest.tsv\",\n",
    "    \"./results_cont_paper/identify_associations/results_sig_assoc_bayes.tsv\",\n",
    "    \"./results_cont_paper/identify_associations/results_sig_assoc_ks.tsv\",\n",
    "]\n",
    "\n",
    "main(args_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing Bayes and KS score distributions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bayes = pd.read_csv(\n",
    "    \"./results_cont_paper/identify_associations/results_sig_assoc_bayes.tsv\", sep=\"\\t\"\n",
    ")\n",
    "df_ks = pd.read_csv(\n",
    "    \"./results_cont_paper/identify_associations/results_sig_assoc_ks.tsv\", sep=\"\\t\"\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Plot histograms using ax.hist() instead of np.histogram()\n",
    "axs[0].hist(abs(df_bayes[\"bayes_k\"]), bins=80)\n",
    "axs[1].hist(abs(df_ks[\"ks_distance\"]), bins=80)\n",
    "\n",
    "# Add labels and titles\n",
    "axs[0].set_title(\"Bayes K Distribution\")\n",
    "axs[0].set_xlabel(\"|Bayes K|\")\n",
    "axs[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axs[1].set_title(\"KS Distance Distribution\")\n",
    "axs[1].set_xlabel(\"|KS Distance|\")\n",
    "axs[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(\"./results_cont_paper/identify_associations/KS_vs_Bayes.svg\", dpi=200)\n",
    "\n",
    "df_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synthetic data 2:**\n",
    "\n",
    "\n",
    "Here we will create a simplified version of the datasets in order to:\n",
    "-  explore the behavior of MOVE at high and low sample regimes (Low N: 50 / High N: 1000).\n",
    "-  visualize the latent representations in a 3D latent space and their movement after the perturbations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r synthetic_data_II/\n",
    "! rm -r results_cont_paper_II/\n",
    "! rm -r results_cont_paper_II_high_N/\n",
    "! rm -r results_cont_paper_II_low_N/\n",
    "! rm -r interim_data_cont_paper_II/\n",
    "! rm -r interim_data_cont_paper_II_high_N/\n",
    "! rm -r interim_data_cont_paper_II_low_N/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Hyperparameters ####################################\n",
    "PROJECT_NAME = \"random_all_sim\"\n",
    "MODE = \"linear\"  # \"non-linear\"\n",
    "SEED_1 = 123  # 1234\n",
    "np.random.seed(SEED_1)\n",
    "rnd.seed(SEED_1)\n",
    "\n",
    "COV_ALPHA = 0.97\n",
    "\n",
    "data_regimes_dict = {\n",
    "    \"synthetic_data_II\": {\n",
    "        \"N_SAMPLES\": 1000,\n",
    "        \"HIGH_CORR\": True,\n",
    "        \"interim_path\": \"interim_data_cont_paper_II\",\n",
    "        \"results_path\": \"results_cont_paper_II\",\n",
    "    },  # high N, added explicit corr\n",
    "    \"low_N_gaussian_corr\": {\n",
    "        \"N_SAMPLES\": 50,\n",
    "        \"HIGH_CORR\": False,\n",
    "        \"interim_path\": \"interim_data_cont_paper_II_low_N\",\n",
    "        \"results_path\": \"results_cont_paper_II_low_N\",\n",
    "    },\n",
    "    \"high_N_gaussian_corr\": {\n",
    "        \"N_SAMPLES\": 1000,\n",
    "        \"HIGH_CORR\": False,\n",
    "        \"interim_path\": \"interim_data_cont_paper_II_high_N\",\n",
    "        \"results_path\": \"results_cont_paper_II_high_N\",\n",
    "    },\n",
    "}\n",
    "\n",
    "SETTINGS = {\n",
    "    \"Continuous_A\": {\n",
    "        \"features\": 10,\n",
    "        \"frequencies\": [0.002, 0.01, 0.02],\n",
    "        \"coefficients\": [500, 100, 50],\n",
    "        \"phase\": 0,\n",
    "        \"offset\": 500,\n",
    "    },\n",
    "    \"Continuous_B\": {\n",
    "        \"features\": 10,\n",
    "        \"frequencies\": [0.001, 0.05, 0.08],\n",
    "        \"coefficients\": [80, 20, 10],\n",
    "        \"phase\": np.pi / 2,\n",
    "        \"offset\": 400,\n",
    "    },\n",
    "    \"Categorical_A\": {\n",
    "        \"features\": 1,\n",
    "        \"frequencies\": [0.1, 0.5, 0.8],\n",
    "        \"coefficients\": [0.2, 0.1, 0.05],\n",
    "        \"phase\": np.pi / 2,\n",
    "        \"offset\": 10,\n",
    "    },\n",
    "    \"Categorical_B\": {\n",
    "        \"features\": 1,\n",
    "        \"frequencies\": [0.01, 0.5, 0.08],\n",
    "        \"coefficients\": [10, 0.1, 0.05],\n",
    "        \"phase\": np.pi,\n",
    "        \"offset\": 1,\n",
    "    },\n",
    "}\n",
    "\n",
    "for path, hyperparams in data_regimes_dict.items():\n",
    "\n",
    "    COR_THRES = 0.02\n",
    "    PAIRS_OF_INTEREST = [(1, 2), (3, 4)]\n",
    "    HIGH_CORR = hyperparams[\"HIGH_CORR\"]\n",
    "    N_SAMPLES = hyperparams[\"N_SAMPLES\"]\n",
    "\n",
    "    # Path to store output files\n",
    "    outpath = Path(f\"{path}\")\n",
    "    outpath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # %%\n",
    "    # Add all datasets in a single matrix:\n",
    "    all_feature_names = get_feature_names(SETTINGS)\n",
    "    feat_means = create_mean_profiles(SETTINGS)\n",
    "\n",
    "    # %%\n",
    "    ###### Covariance matrix definition ######\n",
    "    if MODE == \"linear\":\n",
    "        covariance_matrix = make_sparse_spd_matrix(\n",
    "            n_dim=len(all_feature_names),\n",
    "            alpha=COV_ALPHA,\n",
    "            smallest_coef=0,\n",
    "            largest_coef=1,\n",
    "            norm_diag=True,\n",
    "            random_state=SEED_1,\n",
    "        )\n",
    "    elif MODE == \"non-linear\":\n",
    "        covariance_matrix = np.identity(len(all_feature_names))\n",
    "\n",
    "    ABS_MAX = np.max(abs(covariance_matrix))\n",
    "    fig = plot_score_matrix(\n",
    "        covariance_matrix, all_feature_names, vmin=-ABS_MAX, vmax=ABS_MAX\n",
    "    )\n",
    "    fig.savefig(outpath / f\"Covariance_matrix_{PROJECT_NAME}.svg\")\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = np.random.multivariate_normal(feat_means, covariance_matrix, N_SAMPLES)\n",
    "\n",
    "    # Add non-linearities\n",
    "    if MODE == \"non-linear\":\n",
    "        for i, j in PAIRS_OF_INTEREST:\n",
    "            freq = np.random.choice([4, 5, 6])\n",
    "            dataset[:, i] += np.sin(freq * dataset[:, j])\n",
    "\n",
    "    # scaled_dataset, _ = scale(dataset)\n",
    "    # No scaling in the dataset creation! It will be handled in preprocessing.\n",
    "    scaled_dataset = dataset\n",
    "\n",
    "    if HIGH_CORR:  # The last half of the features are combinations of the first half:\n",
    "        for i in range(scaled_dataset.shape[1] // 2):\n",
    "            col_1 = np.random.choice(range(scaled_dataset.shape[1] // 2))\n",
    "            col_2 = np.random.choice(range(scaled_dataset.shape[1] // 2))\n",
    "            scaled_dataset[:, i + scaled_dataset.shape[1] // 2] = (\n",
    "                scaled_dataset[:, col_1] + scaled_dataset[:, col_2]\n",
    "            ) / 2 + np.random.normal()\n",
    "\n",
    "    # Binarize the categorical dataset\n",
    "    NUM_CAT = (\n",
    "        SETTINGS[\"Categorical_A\"][\"features\"] + SETTINGS[\"Categorical_B\"][\"features\"]\n",
    "    )\n",
    "    columns_to_binarize = scaled_dataset[:, -NUM_CAT:]\n",
    "\n",
    "    # Compute the mean of each of the categorical columns\n",
    "    means = columns_to_binarize.mean(axis=0)\n",
    "\n",
    "    # Apply the binarization\n",
    "    scaled_dataset[:, -NUM_CAT:] = (columns_to_binarize > means).astype(int)\n",
    "\n",
    "    # Correlations\n",
    "    correlations = np.corrcoef(scaled_dataset, rowvar=False)\n",
    "    fig = plot_score_matrix(\n",
    "        correlations, all_feature_names, vmin=-1, vmax=1, label_step=5\n",
    "    )\n",
    "    fig.savefig(outpath / f\"Correlations_{PROJECT_NAME}.svg\", dpi=200)\n",
    "\n",
    "    # Sort correlations by absolute value\n",
    "    associations = create_ground_truth_correlations_file(correlations)\n",
    "    associations.to_csv(outpath / f\"changes.{PROJECT_NAME}.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "    # Plot feature profiles per sample\n",
    "    fig = plot_feature_profiles(scaled_dataset, feat_means)\n",
    "    fig.savefig(outpath / \"Multi-omic_profiles.svg\")\n",
    "\n",
    "    ## Plot correlations\n",
    "    fig = plot_feature_correlations(dataset, PAIRS_OF_INTEREST)\n",
    "    fig.savefig(outpath / \"Feature_correlations.svg\")\n",
    "\n",
    "    fig = plot_feature_correlations(scaled_dataset, PAIRS_OF_INTEREST)\n",
    "    fig.savefig(outpath / \"Feature_correlations_scaled.svg\")\n",
    "\n",
    "    # Write tsv files with feature values for all samples in both datasets:\n",
    "    save_splitted_datasets(\n",
    "        SETTINGS, PROJECT_NAME, scaled_dataset, all_feature_names, N_SAMPLES, outpath\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running MOVE on simple synthetic data (command line style)\n",
    "data_regimes_dict = {\n",
    "    \"synthetic_data_II\": {\n",
    "        \"N_SAMPLES\": 1000,\n",
    "        \"HIGH_CORR\": True,\n",
    "        \"interim_path\": \"interim_data_cont_paper_II\",\n",
    "        \"results_path\": \"results_cont_paper_II\",\n",
    "    },  # high N, added explicit corr\n",
    "    \"low_N_gaussian_corr\": {\n",
    "        \"N_SAMPLES\": 50,\n",
    "        \"HIGH_CORR\": False,\n",
    "        \"interim_path\": \"interim_data_cont_paper_II_low_N\",\n",
    "        \"results_path\": \"results_cont_paper_II_low_N\",\n",
    "    },\n",
    "    \"high_N_gaussian_corr\": {\n",
    "        \"N_SAMPLES\": 1000,\n",
    "        \"HIGH_CORR\": False,\n",
    "        \"interim_path\": \"interim_data_cont_paper_II_high_N\",\n",
    "        \"results_path\": \"results_cont_paper_II_high_N\",\n",
    "    },\n",
    "}\n",
    "\n",
    "for data_regime, hyperparams in data_regimes_dict.items():\n",
    "\n",
    "    # Encode data\n",
    "    ! move-dl task=encode_data data=random_continuous_paper_II data.raw_data_path={data_regime} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]}     # where result files will be placed\n",
    "\n",
    "    # Analyze latent\n",
    "    ! move-dl task=random_continuous_paper_II__latent data=random_continuous_paper_II data.raw_data_path={data_regime} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]}\n",
    "\n",
    "    # Copy model trained on analyze latent to id_assoc directory\n",
    "    ! mkdir -p {hyperparams[\"interim_path\"]}/models\n",
    "    ! mv {hyperparams[\"results_path\"]}/latent_space/model.pt {hyperparams[\"interim_path\"]}/models/model_3_0.pt\n",
    "\n",
    "    # Identify assoc ks\n",
    "    ! move-dl task=random_continuous_paper_II__id_assoc_ks data=random_continuous_paper_II data.raw_data_path={data_regime} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot reconstruction metrics separately:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot categorical box-plots separately\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"font.family\": \"Times New Roman\",\n",
    "        \"font.size\": 16,\n",
    "        \"xtick.labelsize\": 14,\n",
    "        \"ytick.labelsize\": 14,\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"legend.fontsize\": 14,\n",
    "    }\n",
    ")\n",
    "\n",
    "folders = [\n",
    "    \"results_cont_paper_II\",\n",
    "    \"results_cont_paper_II_high_N\",\n",
    "    \"results_cont_paper_II_low_N\",\n",
    "]\n",
    "dfs = [\n",
    "    pd.read_csv(f\"{folder}/latent_space/reconstruction_metrics.tsv\", sep=\"\\t\")\n",
    "    for folder in folders\n",
    "]\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "\n",
    "    # Scatter points for categorical data\n",
    "    plt.scatter(\n",
    "        [\n",
    "            df[\"random.random_all_sim.Categorical_A\"].mean(),\n",
    "            df[\"random.random_all_sim.Categorical_B\"].mean(),\n",
    "        ],\n",
    "        [0.8, 0.6],\n",
    "        marker=\"d\",\n",
    "        s=128,\n",
    "        color=\"#3e4297\",\n",
    "    )\n",
    "\n",
    "    # Horizontal boxplots for continuous data\n",
    "    bp = plt.boxplot(\n",
    "        [\n",
    "            df[\"random.random_all_sim.Continuous_A\"],\n",
    "            df[\"random.random_all_sim.Continuous_B\"],\n",
    "        ],\n",
    "        positions=[0.4, 0.2],\n",
    "        vert=False,\n",
    "        widths=0.10,\n",
    "        patch_artist=True,\n",
    "        medianprops=dict(color=\"black\"),\n",
    "    )\n",
    "\n",
    "    bp[\"boxes\"][0].set_facecolor(\"#980000\")\n",
    "    bp[\"boxes\"][1].set_facecolor(\"#980000\")\n",
    "\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(0.05, 0.95)\n",
    "    plt.yticks(\n",
    "        [0.8, 0.6, 0.4, 0.2],\n",
    "        [\"Categorical_A\", \"Categorical_B\", \"Continuous_A\", \"Continuous_B\"],\n",
    "        size=14,\n",
    "    )\n",
    "    plt.xticks(size=14)\n",
    "    plt.xlabel(\"Score\", size=16)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\n",
    "        f\"./{folders[i]}/latent_space/Categorical_reconstructions_{i}.svg\", dpi=200\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting continuous importance:**\n",
    "\n",
    "Here we will train MOVE by setting a high weight (w=10) on the loss associated to the continuous datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"N_SAMPLES\": 1000,\n",
    "    \"HIGH_CORR\": True,\n",
    "    \"data_path\": \"synthetic_data_II\",\n",
    "    \"interim_path\": \"interim_data_cont_paper_II_10x_cont\",\n",
    "    \"results_path\": \"results_cont_paper_II_10x_cont\",\n",
    "    \"w\": 10,\n",
    "}\n",
    "\n",
    "! move-dl task=encode_data data=random_continuous_paper_II data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} +data.continuous_inputs.0.weight={hyperparams[\"w\"]} +data.continuous_inputs.1.weight={hyperparams[\"w\"]}\n",
    "! move-dl task=random_continuous_paper_II__latent data=random_continuous_paper_II data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} +data.continuous_inputs.0.weight={hyperparams[\"w\"]} +data.continuous_inputs.1.weight={hyperparams[\"w\"]}\n",
    "# Copy model trained on analyze latent to id_assoc directory\n",
    "! mkdir -p {hyperparams[\"interim_path\"]}/models\n",
    "! mv {hyperparams[\"results_path\"]}/latent_space/model.pt {hyperparams[\"interim_path\"]}/models/model_3_0.pt\n",
    "! move-dl task=random_continuous_paper_II__id_assoc_ks data=random_continuous_paper_II data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} +data.continuous_inputs.0.weight={hyperparams[\"w\"]} +data.continuous_inputs.1.weight={hyperparams[\"w\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Latent space.\n",
    "\n",
    "We will now represent the simplified dataset in a 3D latent space. This will allow us to immediately and intuitively understand the emerging behavior of MOVE given its architecture and loss function. We will also be able to check how the perturbations impact the latent location of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D_latent_and_displacement(\n",
    "    mu_baseline,\n",
    "    mu_perturbed,\n",
    "    feature_values,\n",
    "    feature_name,\n",
    "    show_baseline=True,\n",
    "    show_perturbed=True,\n",
    "    show_arrows=True,\n",
    "    step: int = 1,\n",
    "    altitude: int = 30,\n",
    "    azimuth: int = 45,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the movement of the samples in the 3D latent space after perturbing one\n",
    "    input variable.\n",
    "\n",
    "    Args:\n",
    "        mu_baseline:\n",
    "            ND array with dimensions n_samples x n_latent_nodes containing\n",
    "            the latent representation of each sample\n",
    "        mu_perturbed:\n",
    "            ND array with dimensions n_samples x n_latent_nodes containing\n",
    "            the latent representation of each sample after perturbing the input\n",
    "        feature_values:\n",
    "            1D array with feature values to map to a colormap (\"bwr\"). Each sample is\n",
    "            colored according to its value for the feature of interest.\n",
    "        feature_name:\n",
    "            name of the feature mapped to a colormap\n",
    "        show_baseline:\n",
    "            plot orginal location of the samples in the latent space\n",
    "        show_perturbed:\n",
    "            plot final location (after perturbation) of the samples in latent space\n",
    "        show_arrows:\n",
    "            plot arrows from original to final location of each sample\n",
    "        angle:\n",
    "            elevation from dim1-dim2 plane for the visualization of latent space.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If latent space is not 3-dimensional (3 hidden nodes).\n",
    "    Returns:\n",
    "        Figure\n",
    "    \"\"\"\n",
    "    # construct cmap\n",
    "    my_cmap = sns.color_palette(\"RdYlBu\", as_cmap=True)\n",
    "\n",
    "    eps = 1e-16\n",
    "    if [np.shape(mu_baseline)[1], np.shape(mu_perturbed)[1]] != [3, 3]:\n",
    "        raise ValueError(\n",
    "            \" The latent space must be 3-dimensional. Redefine num_latent to 3.\"\n",
    "        )\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(5, 5))\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    ax.view_init(altitude, azimuth)\n",
    "\n",
    "    if show_baseline:\n",
    "        vmin, vmax = np.min(feature_values[::step]), np.max(feature_values[::step])\n",
    "        abs_max = np.max([abs(vmin), abs(vmax)])\n",
    "        plot = ax.scatter(\n",
    "            mu_baseline[::step, 0],\n",
    "            mu_baseline[::step, 1],\n",
    "            mu_baseline[::step, 2],\n",
    "            marker=\"o\",\n",
    "            c=feature_values[::step],\n",
    "            s=10,\n",
    "            lw=0,\n",
    "            cmap=my_cmap,\n",
    "        )\n",
    "        fig.colorbar(plot)\n",
    "\n",
    "    if show_perturbed:\n",
    "        ax.scatter(\n",
    "            mu_perturbed[::step, 0],\n",
    "            mu_perturbed[::step, 1],\n",
    "            mu_perturbed[::step, 2],\n",
    "            marker=\"o\",\n",
    "            c=feature_values[::step],\n",
    "            s=10,\n",
    "            label=\"perturbed\",\n",
    "            lw=0,\n",
    "        )\n",
    "    if show_arrows:\n",
    "        u = mu_perturbed[::step, 0] - mu_baseline[::step, 0]\n",
    "        v = mu_perturbed[::step, 1] - mu_baseline[::step, 1]\n",
    "        w = mu_perturbed[::step, 2] - mu_baseline[::step, 2]\n",
    "\n",
    "        module = np.sqrt(u * u + v * v + w * w)\n",
    "        mask = module > eps\n",
    "        max_u, max_v, max_w = np.max(abs(u)), np.max(abs(v)), np.max(abs(w))\n",
    "\n",
    "        # Arrow colors will be weighted contributions of red -> dim1, green -> dim2, and blue-> dim3. I.e. purple arrow means movement in dims 1 and 3\n",
    "        colors = [\n",
    "            (abs(du) / max_u, abs(dv) / max_v, abs(dw) / max_w, 0.7)\n",
    "            for du, dv, dw in zip(u, v, w)\n",
    "        ]\n",
    "        ax.quiver(\n",
    "            mu_baseline[::step, 0][mask],\n",
    "            mu_baseline[::step, 1][mask],\n",
    "            mu_baseline[::step, 2][mask],\n",
    "            u[mask],\n",
    "            v[mask],\n",
    "            w[mask],\n",
    "            color=colors,\n",
    "            lw=0.6,\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Dim 1\")\n",
    "    ax.set_ylabel(\"Dim 2\")\n",
    "    ax.set_zlabel(\"Dim 3\")\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"classic\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"font.family\": \"Times New Roman\",\n",
    "        \"font.size\": 14,\n",
    "        \"xtick.labelsize\": 14,\n",
    "        \"ytick.labelsize\": 14,\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"legend.fontsize\": 14,\n",
    "    }\n",
    ")\n",
    "\n",
    "DATASET = \"Continuous_B\"  # \"Continuous_A\"\n",
    "feature_list = pd.read_csv(\n",
    "    f\"./interim_data_cont_paper_II/random.random_all_sim.{DATASET}.txt\", header=None\n",
    ")\n",
    "feature_list = feature_list.values.flatten()\n",
    "\n",
    "figure_path = Path(\"./synthetic_data_II/figures_synthetic_small/\")\n",
    "! mkdir -p {figure_path}\n",
    "\n",
    "latent_space = np.load(\n",
    "    \"./results_cont_paper_II/identify_associations/latent_location.npy\"\n",
    ")\n",
    "\n",
    "# Main code:\n",
    "for i, feature in enumerate(feature_list):\n",
    "    print(i)\n",
    "    feature_values = np.load(\n",
    "        f\"./interim_data_cont_paper_II/random.random_all_sim.{DATASET}.npy\"\n",
    "    )\n",
    "    CONDITION = \"Categorical\" in DATASET\n",
    "    if CONDITION:\n",
    "        feature_values = np.argmax(feature_values, axis=2)[:, i]\n",
    "    else:\n",
    "        feature_values = feature_values[:, i]\n",
    "\n",
    "    print(feature_values)\n",
    "    latent_space_baseline = latent_space[:, :, -1]\n",
    "    latent_space_perturbed = latent_space[:, :, i]\n",
    "\n",
    "    # # Plot latent space:\n",
    "    n_pictures = 10\n",
    "    PLOT = [0] if CONDITION else [0, 2, 3, 9]  # Plot Categorical A1\n",
    "    if i in PLOT:\n",
    "        print(i)\n",
    "        # for pic_num, (azimuth, altitude) in enumerate(zip(\n",
    "        #    np.linspace(90, 180, n_pictures), np.linspace(35, 55, n_pictures)\n",
    "        # )):\n",
    "\n",
    "        for pic_num, (azimuth, altitude) in enumerate([[120, 44]]):\n",
    "            print(pic_num, azimuth, altitude)\n",
    "            title = feature\n",
    "            fig = plot_3D_latent_and_displacement(\n",
    "                latent_space_baseline,\n",
    "                latent_space_perturbed,\n",
    "                feature_values=feature_values,\n",
    "                feature_name=f\"{title}\",\n",
    "                show_baseline=True,\n",
    "                show_perturbed=False,\n",
    "                show_arrows=False,\n",
    "                altitude=altitude,\n",
    "                azimuth=azimuth,\n",
    "            )\n",
    "            fig.savefig(figure_path / f\"3D_latent_{pic_num}_perturbed_{feature}.svg\")\n",
    "            plt.close(fig)\n",
    "\n",
    "            fig = plot_3D_latent_and_displacement(\n",
    "                latent_space_baseline,\n",
    "                latent_space_perturbed,\n",
    "                feature_values=feature_values,\n",
    "                feature_name=f\"{title}\",\n",
    "                show_baseline=False,\n",
    "                show_perturbed=False,\n",
    "                show_arrows=True,\n",
    "                altitude=altitude,\n",
    "                azimuth=azimuth,\n",
    "            )\n",
    "            fig.savefig(\n",
    "                figure_path / f\"3D_latent_{pic_num}_perturbed_{feature}_arrows_max.svg\"\n",
    "            )\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real world applications: \n",
    "### A) A Parkinsons dataset\n",
    "\n",
    "Download the csv file The original table is provided as a Supplementary Dataset in the original article: [Parkinsons disease dataset](https://static-content.springer.com/esm/art%3A10.1038%2Fs41598-017-00047-5/MediaObjects/41598_2017_47_MOESM3_ESM.xls) by clicking on the link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "output_path = Path(\"./parkinsons_data/\")\n",
    "output_path.mkdir(exist_ok=True)\n",
    "df = pd.read_csv(output_path / \"dataset.csv\", sep=\",\")\n",
    "\n",
    "# Setting the index\n",
    "df[\"ID\"] = pd.RangeIndex(1, len(df) + 1)\n",
    "df = df.set_index(\"ID\")\n",
    "\n",
    "\n",
    "# Function to convert columns to proper data types and handle missing values\n",
    "def process_dataframe(dataframe, clean_medications=False):\n",
    "    \"\"\"Convert numerical columns to proper types and replace missing values with 'NA'\"\"\"\n",
    "    processed_df = dataframe.copy()\n",
    "\n",
    "    for col in processed_df.columns:\n",
    "        # Skip columns that are categorical\n",
    "        if col in [\n",
    "            \" Gender \",\n",
    "            \" Positive  history  of  Parkinson  disease  in  family \",\n",
    "            \" Antidepressant  therapy \",\n",
    "            \" Antiparkinsonian  medication \",\n",
    "            \" Antipsychotic  medication \",\n",
    "            \" Benzodiazepine  medication \",\n",
    "        ]:\n",
    "            # For categorical columns, just replace empty strings and various missing indicators\n",
    "            processed_df[col] = processed_df[col].replace(\n",
    "                [\"\", \" \", \"nan\", \"NaN\", np.nan, None], \"NA\"\n",
    "            )\n",
    "\n",
    "            if clean_medications and \"therapy\" in col or \"medication\" in col:\n",
    "                # Clean medication columns to keep only Yes/No\n",
    "                processed_df[col] = processed_df[col].astype(str).str.strip()\n",
    "                processed_df[col] = processed_df[col].apply(\n",
    "                    lambda x: (\n",
    "                        \"Yes\"\n",
    "                        if x.startswith(\"Yes\") or x.startswith(\" Yes\")\n",
    "                        else \"No\" if x.startswith(\"No\") or x.startswith(\" No\") else \"NA\"\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # For other categorical columns, just replace empty strings and various missing indicators\n",
    "                processed_df[col] = processed_df[col].replace(\n",
    "                    [\"\", \" \", \"nan\", \"NaN\", np.nan, None], \"NA\"\n",
    "                )\n",
    "        else:\n",
    "            # For numerical columns, convert to numeric and handle missing values\n",
    "            # First, replace various missing value indicators with NaN\n",
    "            processed_df[col] = processed_df[col].replace(\n",
    "                [\"\", \" \", \"-\", \"nan\", \"NaN\"], np.nan\n",
    "            )\n",
    "\n",
    "            # Convert to numeric, coercing errors to NaN\n",
    "            processed_df[col] = pd.to_numeric(processed_df[col], errors=\"coerce\")\n",
    "\n",
    "            # Replace NaN with 'NA' string for output\n",
    "            processed_df[col] = processed_df[col].fillna(\"NA\")\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "# 1. parkinson.ids.txt - Only the index numbers, no header\n",
    "with open(output_path / \"parkinsons.ids.txt\", \"w\") as f:\n",
    "    for idx in df.index:\n",
    "        f.write(f\"{idx}\\n\")\n",
    "\n",
    "# 2. demographics.tsv - Index + Gender and Positive history of Parkinson disease in family\n",
    "demographics_cols = [\n",
    "    \"Gender\",\n",
    "    \" Positive  history  of  Parkinson  disease  in  family \",\n",
    "]\n",
    "demographics_df = df[demographics_cols]\n",
    "demographics_df.to_csv(output_path / \"demographics.tsv\", sep=\"\\t\", na_rep=\"NA\")\n",
    "\n",
    "# 3. age_related.tsv - Index + Age, Age of disease onset, Duration of disease\n",
    "age_cols = [\n",
    "    \" Age  (years) \",\n",
    "    \" Age  of  disease  onset  (years) \",\n",
    "    \" Duration  of  disease  from  first  symptoms  (years) \",\n",
    "]\n",
    "age_df = process_dataframe(df[age_cols])\n",
    "age_df.to_csv(output_path / \"age_related.tsv\", sep=\"\\t\", na_rep=\"NA\")\n",
    "\n",
    "# 4. medication.tsv (without Levodopa and Clonazepam)\n",
    "medication_cols = df.columns[6:10].tolist()\n",
    "medication_df = process_dataframe(df[medication_cols], clean_medications=True)\n",
    "medication_df.to_csv(output_path / \"medication.tsv\", sep=\"\\t\", na_rep=\"NA\")\n",
    "\n",
    "# 5. overview_of_motor_exam.tsv\n",
    "motor_overview_cols = df.columns[12:14].tolist()\n",
    "motor_overview_df = process_dataframe(df[motor_overview_cols])\n",
    "motor_overview_df.to_csv(\n",
    "    output_path / \"overview_of_motor_exam.tsv\", sep=\"\\t\", na_rep=\"NA\"\n",
    ")\n",
    "\n",
    "# 6. motor_scale.tsv\n",
    "motor_scale_cols = df.columns[14:41].tolist()\n",
    "motor_scale_df = process_dataframe(df[motor_scale_cols])\n",
    "motor_scale_df.to_csv(output_path / \"motor_scale.tsv\", sep=\"\\t\", na_rep=\"NA\")\n",
    "\n",
    "# 7. speech_1.tsv\n",
    "speech1_cols = df.columns[41:53].tolist()\n",
    "speech1_df = process_dataframe(df[speech1_cols])\n",
    "speech1_df.to_csv(output_path / \"speech_1.tsv\", sep=\"\\t\", na_rep=\"NA\")\n",
    "\n",
    "# 8. speech_2.tsv\n",
    "speech2_cols = df.columns[53:].tolist()\n",
    "speech2_df = process_dataframe(df[speech2_cols])\n",
    "speech2_df.to_csv(output_path / \"speech2.tsv\", sep=\"\\t\", na_rep=\"NA\")\n",
    "\n",
    "print(\"Files created successfully!\")\n",
    "print(f\"Total columns in original dataset: {len(df.columns)}\")\n",
    "print(\"\\nFiles created:\")\n",
    "for file_name in [\n",
    "    \"parkinson.ids.txt\",\n",
    "    \"demographics.tsv\",\n",
    "    \"age_related.tsv\",\n",
    "    \"medication.tsv\",\n",
    "    \"overview_of_motor_exam.tsv\",\n",
    "    \"motor_scale.tsv\",\n",
    "    \"speech_1.tsv\",\n",
    "    \"speech2.tsv\",\n",
    "]:\n",
    "    file_path = output_path / file_name\n",
    "    if file_path.exists():\n",
    "        print(f\"✓ {file_name}\")\n",
    "    else:\n",
    "        print(f\"✗ {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run move on parkinson's data:\n",
    "# Encode data\n",
    "! move-dl task=encode_data data=parkinsons\n",
    "\n",
    "# Analyze latent\n",
    "! move-dl task=parkinsons__latent data=parkinsons\n",
    "\n",
    "# Identify assoc ks\n",
    "! move-dl task=parkinsons__id_assoc_bayes_cont data=parkinsons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Machine Learning Omics: \n",
    "\n",
    "This is a multimodal dataset on diverse cancers, covering CNV, methylation, miRNA and mRNA. Please note that all of these data modalities are described by continuous features.\n",
    "\n",
    "https://www.nature.com/articles/s41597-025-05235-x\n",
    "\n",
    "From all MLOmics data, I used the files for clustering on LUAD data, the aligned files to ease integration.\n",
    "\n",
    "```\n",
    "MLOmics -> Clustering -> LUAD -> aligned\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"./MLOmics_Clustering_LUAD_aligned/\")\n",
    "# df = pd.read_csv(data_path / 'LUAD_CNV_aligned.csv', sep=',', header=0, index_col=0).T\n",
    "\n",
    "\n",
    "def transpose_luad_files(folder_path):\n",
    "    \"\"\"Transpose LUAD CSV files and save with _MOVE suffix. Create LUAD.ids.txt.\"\"\"\n",
    "\n",
    "    folder = Path(folder_path)\n",
    "    luad_files = list(folder.glob(\"LUAD*.csv\"))\n",
    "\n",
    "    if not luad_files:\n",
    "        print(f\"No LUAD files found in {folder_path}\")\n",
    "        return\n",
    "\n",
    "    all_ids = set()\n",
    "\n",
    "    for file_path in luad_files:\n",
    "        # Read, transpose, save\n",
    "        df = pd.read_csv(file_path, index_col=0).T\n",
    "        df.index.name = \"ID\"  # Add \"ID\" as header for index column\n",
    "\n",
    "        # Save with _MOVE suffix\n",
    "        output_name = file_path.stem + \"_MOVE.tsv\"\n",
    "        output_path = folder / output_name\n",
    "        df.to_csv(output_path, sep=\"\\t\")\n",
    "\n",
    "        # Collect sample IDs\n",
    "        all_ids.update(df.index)\n",
    "\n",
    "        print(f\"Transposed: {file_path.name} -> {output_name}\")\n",
    "\n",
    "    # Save all unique IDs to LUAD.ids.txt\n",
    "    ids_file = folder / \"LUAD.ids.txt\"\n",
    "    with open(ids_file, \"w\") as f:\n",
    "        for sample_id in sorted(all_ids):\n",
    "            f.write(f\"{sample_id}\\n\")\n",
    "\n",
    "    print(f\"Created {ids_file.name} with {len(all_ids)} unique IDs\")\n",
    "\n",
    "\n",
    "transpose_luad_files(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run move on MLOmics data:\n",
    "# Encode data\n",
    "! move-dl task=encode_data data=MLOmics\n",
    "\n",
    "# Analyze latent\n",
    "! move-dl task=MLOmics__latent data=MLOmics\n",
    "\n",
    "# Identify assoc ks\n",
    "! move-dl task=MLOmics__id_assoc_bayes data=MLOmics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Inflammatory Bowel Disease (IBD)\n",
    "\n",
    "This is the main real world, biological dataset that we will use for the manuscript. The following analyses and cells are an updated version of our [NNFC workshop](https://github.com/RasmussenLab/MOVE/tree/nnfc-workshop). \n",
    "\n",
    "**About IBD:**\n",
    "\n",
    " IBDs are a group of conditions that debilitate and inflammate the gastrointestinal tract or the colon. We will use part of the data presented in the paper [_Multi-omics of the gut microbial ecosystem in inflammatory bowel diseases_](https://www.nature.com/articles/s41586-019-1237-9) by Lloyd-Price, J., Arze, C., Ananthakrishnan, A.N. et al. Their results and conclusions will guide our data analysis. The data can be obtained from [The Inflammatory Bowel Disease Multiomics Database (IBDMDB)](https://ibdmdb.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD = False\n",
    "ibdmdb_data_path = Path(\"./ibdmdb_data/\")\n",
    "ibdmdb_data_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "ibdmdb_files = {\n",
    "    \"ibdmdb_metadata.csv\": \"https://g-227ca.190ebd.75bc.data.globus.org/ibdmdb/metadata/hmp2_metadata_2018-08-20.csv\",  # For diagnostics\n",
    "    \"ibdmdb_mbx_w_metadata.biom.gz\": \"https://g-227ca.190ebd.75bc.data.globus.org/ibdmdb/products/HMP2/MBX/HMP2_metabolomics_w_metadata.biom.gz\",  # Metabolomics with metabolite names\n",
    "    \"ibdmdb_mgx.tsv.gz\": \"https://g-227ca.190ebd.75bc.data.globus.org/ibdmdb/products/HMP2/MGX/2018-05-04/taxonomic_profiles.tsv.gz\",  # Metagenomics\n",
    "    \"ibdmdb_mtx.tsv.gz\": \"https://g-227ca.190ebd.75bc.data.globus.org/ibdmdb/products/HMP2/MTX/2017-12-14/ecs_3.tsv.gz\",  # Meta transcriptomics\n",
    "    \"ibdmdb_dysbiosis.tsv\": \"https://forum.biobakery.org/uploads/short-url/umwfR0kDJ6s5RXHwtIMgLaOEoOI.tsv\",\n",
    "}  # Diagnosis\n",
    "\n",
    "# Download the files the first time:\n",
    "if DOWNLOAD:\n",
    "    for filename, file in ibdmdb_files.items():\n",
    "        filename = wget.download(file, str(ibdmdb_data_path) + \"/\" + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing the data:** \n",
    "- Process data into a MOVE-friendly format.\n",
    "- Drop columns with high missingness (more than 50% missing).\n",
    "- Keep only named metabolites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a missingness threshold\n",
    "MISSING_THRESHOLD = 0.5  # Drop columns with >50% missing\n",
    "na_value = 0.0\n",
    "\n",
    "# Create index file from dysbiosis.tsv (first column)\n",
    "print(\"Creating index file...\")\n",
    "dysbiosis_df = pd.read_csv(ibdmdb_data_path / \"ibdmdb_dysbiosis.tsv\", sep=\"\\t\")\n",
    "index_ids = dysbiosis_df.iloc[:, 0].tolist()  # First column IDs\n",
    "index_df = pd.DataFrame({\"ID\": index_ids})\n",
    "index_df.to_csv(ibdmdb_data_path / \"ibdmdb.ids.txt\", sep=\"\\t\", index=False, header=None)\n",
    "print(f\"Index file created with {len(index_ids)} IDs\")\n",
    "\n",
    "# Create dysbiosis.tsv with first and third columns\n",
    "print(\"Creating dysbiosis.tsv...\")\n",
    "dysbiosis_subset = dysbiosis_df.iloc[:, [0, 2]].copy()  # First and third columns\n",
    "dysbiosis_subset.columns = [\"ID\", \"dysbiosis\"]\n",
    "dysbiosis_subset.to_csv(ibdmdb_data_path / \"dysbiosis.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Create mbx.tsv from BIOM file\n",
    "# Extract valid metabolite IDs and names\n",
    "with gzip.open(ibdmdb_data_path / \"ibdmdb_mbx_w_metadata.biom.gz\", \"rt\") as f:\n",
    "    biom_data = json.load(f)\n",
    "\n",
    "# Keep only the named metabolites\n",
    "valid_metabolites = {}\n",
    "for row in biom_data[\"rows\"]:\n",
    "    metabolite = row[\"metadata\"].get(\"Metabolite\")\n",
    "    if metabolite and metabolite != \"\":\n",
    "        valid_metabolites[row[\"id\"]] = metabolite\n",
    "\n",
    "# Load biom table and filter columns\n",
    "print(\"Creating mbx.tsv from BIOM file...\")\n",
    "table = biom.load_table(str(ibdmdb_data_path / \"ibdmdb_mbx_w_metadata.biom.gz\"))\n",
    "mbx_df = table.to_dataframe().T\n",
    "\n",
    "# Keep only valid metabolite columns and rename them\n",
    "valid_cols = [col for col in mbx_df.columns if col in valid_metabolites]\n",
    "mbx_df = mbx_df[valid_cols]\n",
    "mbx_df.rename(columns=valid_metabolites, inplace=True)\n",
    "\n",
    "# Reset index to get sample IDs as a column\n",
    "mbx_df.reset_index(inplace=True)\n",
    "mbx_df.rename(columns={\"index\": \"ID\"}, inplace=True)\n",
    "mbx_df.to_csv(ibdmdb_data_path / \"mbx.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "def process_microbiome_data(\n",
    "    file_path,\n",
    "    output_path,\n",
    "    data_type=\"mtx\",\n",
    "    na_value=na_value,\n",
    "    MISSING_THRESHOLD=MISSING_THRESHOLD,\n",
    "):\n",
    "    \"\"\"Process MTX or MGX data with simplified column filtering\"\"\"\n",
    "\n",
    "    print(f\"Creating {data_type}.tsv...\")\n",
    "    with gzip.open(file_path, \"rt\") as f:\n",
    "        df = pd.read_csv(\n",
    "            f,\n",
    "            sep=\"\\t\",\n",
    "            index_col=0,\n",
    "            low_memory=False,\n",
    "            dtype=str if data_type == \"mtx\" else None,\n",
    "        )\n",
    "        df = df.T.reset_index().rename(columns={\"index\": \"ID\"})\n",
    "\n",
    "        # Clean ID column\n",
    "        df[\"ID\"] = df[\"ID\"].str.split(\"_\").str[0]\n",
    "\n",
    "        # Get data columns\n",
    "        data_cols = df.columns[1:]  # All except ID\n",
    "\n",
    "        # Convert to numeric and handle zeros\n",
    "        for col in data_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        if data_type == \"mtx\":\n",
    "            # Filter species columns (UNGROUPED with s__)\n",
    "            species_cols = [\n",
    "                col\n",
    "                for col in data_cols\n",
    "                if col.startswith(\"UNGROUPED|\") and \".s__\" in col\n",
    "            ]\n",
    "            # Rename columns\n",
    "            rename_dict = {col: f\"s__{col.split('.s__')[1]}\" for col in species_cols}\n",
    "        else:\n",
    "            # MGX: Keep 0.0 as is, filter by taxonomic levels\n",
    "            species_cols = [\n",
    "                col\n",
    "                for col in data_cols\n",
    "                if col.startswith(\"k__\")\n",
    "                and col.count(\"|\") == 6\n",
    "                and col.endswith(col.split(\"|s__\")[1])\n",
    "            ]\n",
    "            # Rename columns\n",
    "            rename_dict = {col: f\"s__{col.split('|s__')[1]}\" for col in species_cols}\n",
    "\n",
    "        # Filter and rename\n",
    "        df_filtered = df[[\"ID\"] + species_cols].rename(columns=rename_dict)\n",
    "\n",
    "        # Filter by missingness\n",
    "        final_cols = df_filtered.columns[1:]  # Exclude ID\n",
    "        cols_to_keep = [\n",
    "            col\n",
    "            for col in final_cols\n",
    "            if (df_filtered[col] == na_value).mean() <= MISSING_THRESHOLD\n",
    "        ]\n",
    "\n",
    "        df_final = df_filtered[[\"ID\"] + cols_to_keep]\n",
    "\n",
    "        print(\n",
    "            f\"{data_type.upper()}: {len(data_cols)} → {len(species_cols)} species → {len(cols_to_keep)} kept\"\n",
    "        )\n",
    "        print(f\"Final shape: {df_final.shape}\")\n",
    "\n",
    "    df_final.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "    return df_final\n",
    "\n",
    "\n",
    "# Use the function for both data modalities:\n",
    "mtx_df = process_microbiome_data(\n",
    "    ibdmdb_data_path / \"ibdmdb_mtx.tsv.gz\", ibdmdb_data_path / \"mtx.tsv\", \"mtx\"\n",
    ")\n",
    "\n",
    "mgx_df = process_microbiome_data(\n",
    "    ibdmdb_data_path / \"ibdmdb_mgx.tsv.gz\", ibdmdb_data_path / \"mgx.tsv\", \"mgx\"\n",
    ")\n",
    "\n",
    "# Create ibd.tsv from metadata\n",
    "print(\"Creating ibd.tsv...\")\n",
    "metadata_df = pd.read_csv(ibdmdb_data_path / \"ibdmdb_metadata.csv\")\n",
    "\n",
    "# Extract External ID and diagnosis columns\n",
    "ibd_df = metadata_df[[\"External ID\", \"diagnosis\"]].copy()\n",
    "ibd_df.columns = [\"ID\", \"IBD_status\"]\n",
    "\n",
    "\n",
    "# Clean up IBD status - map to UC, CD, or Control\n",
    "def map_ibd_status(diagnosis):\n",
    "    if pd.isna(diagnosis):\n",
    "        return \"Control\"\n",
    "    diagnosis_str = str(diagnosis).upper()\n",
    "    if \"UC\" in diagnosis_str or \"ULCERATIVE\" in diagnosis_str:\n",
    "        return \"UC\"\n",
    "    elif \"CD\" in diagnosis_str or \"CROHN\" in diagnosis_str:\n",
    "        return \"CD\"\n",
    "    else:\n",
    "        return \"Control\"\n",
    "\n",
    "\n",
    "ibd_df[\"IBD_status\"] = ibd_df[\"IBD_status\"].apply(map_ibd_status)\n",
    "ibd_df.to_csv(ibdmdb_data_path / \"ibd.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Align the different files to share the same IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tsv_files(folder_path=\"./ibdmdb_data/\"):\n",
    "    folder = Path(folder_path)\n",
    "    tsv_files = [f for f in folder.glob(\"*.tsv\") if \"ibdmdb\" not in f.name]\n",
    "\n",
    "    # Load files and find shared IDs\n",
    "    dataframes = {}\n",
    "    all_ids = []\n",
    "\n",
    "    for file in tsv_files:\n",
    "        df = pd.read_csv(file, sep=\"\\t\")\n",
    "        df[\"ID\"] = df[\"ID\"].astype(str).str.strip()\n",
    "        dataframes[file.name] = df\n",
    "        all_ids.append(set(df[\"ID\"]))\n",
    "\n",
    "    shared_ids = set.intersection(*all_ids)\n",
    "    print(f\"Shared IDs: {len(shared_ids)}\")\n",
    "\n",
    "    # Overwrite older ids file.\n",
    "    with open(folder_path + \"ibdmdb.ids.txt\", \"w\") as f:\n",
    "        for id_val in sorted(shared_ids):\n",
    "            f.write(f\"{id_val}\\n\")\n",
    "\n",
    "    # Filter to shared IDs only, removing duplicates\n",
    "    for filename, df in dataframes.items():\n",
    "        # Keep only shared IDs and drop duplicates\n",
    "        aligned_df = df[df[\"ID\"].isin(shared_ids)].drop_duplicates(subset=[\"ID\"]).copy()\n",
    "        output_path = folder / f\"aligned_{filename}\"\n",
    "        aligned_df.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "        print(f\"Created {output_path.name}: {len(aligned_df)} rows\")\n",
    "\n",
    "\n",
    "shared_ids = align_tsv_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the correlational structure in the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./ibdmdb_data/\")\n",
    "savepath = Path(\"./results_ibdmdb/\")\n",
    "\n",
    "# Read and merge dataframes\n",
    "mgx_df = pd.read_csv(path / \"aligned_mgx.tsv\", sep=\"\\t\")\n",
    "mtx_df = pd.read_csv(path / \"aligned_mtx.tsv\", sep=\"\\t\")\n",
    "mbx_df = pd.read_csv(path / \"aligned_mbx.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Store original mbx columns (excluding ID)\n",
    "mbx_original_columns = [col for col in mbx_df.columns if col != \"ID\"]\n",
    "\n",
    "# Add suffixes to all non-ID columns\n",
    "mgx_df = mgx_df.rename(\n",
    "    columns={col: f\"{col}_mgx\" for col in mgx_df.columns if col != \"ID\"}\n",
    ")\n",
    "mtx_df = mtx_df.rename(\n",
    "    columns={col: f\"{col}_mtx\" for col in mtx_df.columns if col != \"ID\"}\n",
    ")\n",
    "mbx_df = mbx_df.rename(\n",
    "    columns={col: f\"{col}_mbx\" for col in mbx_df.columns if col != \"ID\"}\n",
    ")\n",
    "\n",
    "df = mgx_df.merge(mtx_df, on=\"ID\").merge(mbx_df, on=\"ID\").set_index(\"ID\")\n",
    "\n",
    "# Calculate correlation matrix with hierarchical clustering order\n",
    "corr_matrix = df.corr()\n",
    "columns = corr_matrix.columns\n",
    "linkage_matrix = linkage(corr_matrix, method=\"average\")\n",
    "ordered_indices = leaves_list(linkage_matrix)\n",
    "corr_matrix = corr_matrix.iloc[ordered_indices, ordered_indices]\n",
    "columns = columns[ordered_indices]\n",
    "\n",
    "# Large heatmap\n",
    "STEP = 1\n",
    "fig = plt.figure(figsize=(50, 50))\n",
    "plt.imshow(corr_matrix, cmap=\"seismic\", vmin=-1, vmax=1)\n",
    "plt.xticks(np.arange(0, len(columns), STEP), columns[::STEP], rotation=90, fontsize=5)\n",
    "plt.yticks(np.arange(0, len(columns), STEP), columns[::STEP], fontsize=5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "fig.savefig(savepath / \"Correlation_matrix.png\", dpi=200)\n",
    "\n",
    "# Distribution plot\n",
    "upper_triangle_indices = np.triu_indices_from(corr_matrix, k=1)\n",
    "upper_triangle_values = corr_matrix.values[upper_triangle_indices]\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "sns.kdeplot(abs(upper_triangle_values), bw_adjust=0.5)\n",
    "plt.xlabel(\"Correlation Coefficient\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Density Plot of Correlation Coefficients\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(savepath / \"Correlation_distribution.png\", dpi=200)\n",
    "\n",
    "# Three-panel subplot\n",
    "specific_columns = [\n",
    "    \"butyrate_mbx\",\n",
    "    \"s__Roseburia_hominis_mgx\",\n",
    "    \"s__Roseburia_hominis_mtx\",\n",
    "    \"s__Roseburia_intestinalis_mgx\",\n",
    "    \"s__Roseburia_intestinalis_mtx\",\n",
    "    \"s__Alistipes_putredinis_mtx\",\n",
    "    \"s__Alistipes_putredinis_mgx\",\n",
    "    \"s__Parabacteroides_merdae_mgx\",\n",
    "    \"s__Parabacteroides_merdae_mtx\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Subplot A: Overall correlation matrix\n",
    "im1 = axes[0].imshow(corr_matrix, cmap=\"seismic\", vmin=-1, vmax=1)\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "\n",
    "# Subplot B: Metabolites only (mbx) - with _mbx suffix\n",
    "mbx_columns = [col for col in columns if \"_mbx\" in col]\n",
    "mbx_corr = corr_matrix.loc[mbx_columns, mbx_columns]\n",
    "im2 = axes[1].imshow(mbx_corr, cmap=\"seismic\", vmin=-1, vmax=1)\n",
    "axes[1].set_xticks([])\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "# Subplot C: Specified columns with labels\n",
    "valid_columns = [col for col in specific_columns if col in columns]\n",
    "if valid_columns:\n",
    "    specific_corr = corr_matrix.loc[valid_columns, valid_columns]\n",
    "    im3 = axes[2].imshow(specific_corr, cmap=\"seismic\", vmin=-1, vmax=1)\n",
    "    axes[2].set_xticks(range(len(valid_columns)))\n",
    "    axes[2].set_yticks(range(len(valid_columns)))\n",
    "    axes[2].set_xticklabels(valid_columns, rotation=90, fontsize=14)\n",
    "    axes[2].set_yticklabels(valid_columns, fontsize=14)\n",
    "else:\n",
    "    axes[2].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        \"No valid\\ncolumns found\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        transform=axes[2].transAxes,\n",
    "    )\n",
    "\n",
    "# Position colorbar to the right of subplot C\n",
    "fig.colorbar(im1, ax=axes[2], shrink=0.8)\n",
    "plt.tight_layout()\n",
    "fig.savefig(\n",
    "    savepath / \"Correlation_matrix_three_panels.png\", dpi=200, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running MOVE:\n",
    "\n",
    "Since this is the main biological dataset, we will explore a bit more in detail what can be done with MOVE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from move.data import io\n",
    "from move.tasks import encode_data\n",
    "# Run move on ibdmdb data:\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = io.read_config(\"ibdmdb\", None)\n",
    "# Print data config file\n",
    "print(OmegaConf.to_yaml(config, resolve=True))\n",
    "\n",
    "# Encode data\n",
    "encode_data(config.data)\n",
    "\n",
    "path = Path(config.data.interim_data_path)\n",
    "cat_datasets, cat_names, con_datasets, con_names = io.load_preprocessed_data(\n",
    "    path, config.data.categorical_names, config.data.continuous_names\n",
    ")\n",
    "dataset_names = config.data.categorical_names + config.data.continuous_names\n",
    "\n",
    "# Print shapes of the different datasets\n",
    "for dataset, dataset_name in zip(cat_datasets + con_datasets, dataset_names):\n",
    "    print(f\"{dataset_name}: {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze the latent space:**\n",
    "\n",
    "This step includes our self-implemented SHAP analysis. \n",
    "\n",
    "We will explore the hyperparameter space aiming to:\n",
    "- Identify changes in the way MOVE distributes samples in latent space.\n",
    "- Identify trends that might be shared regardless of the choice of hyperparameters.\n",
    "\n",
    "We will modify:\n",
    "- Number of latent nodes.\n",
    "- Number of hidden nodes.\n",
    "- Beta (KLD strength).\n",
    "- Weight on the loss from continuous datasets.\n",
    "    - Since equal weights leads to latent space fragmentation (i.e. categorical variables like dysbiosis or ibd status\"win\"), we can increase the importance of properly reconstructing the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze latent\n",
    "#! move-dl task=ibdmdb__latent data=ibdmdb # Command line style\n",
    "\n",
    "# Analyze latent\n",
    "results_path = \"./results_ibdmdb/results_temp/\"\n",
    "Path(results_path).mkdir(exist_ok=True, parents=True)\n",
    "hyper_list = [\n",
    "    (lat, hid, bet, w)\n",
    "    for bet in [0.01, 0.0001]\n",
    "    for hid in [[64], [256], [512]]\n",
    "    for lat in [16, 32]\n",
    "    for w in [1, 2]\n",
    "]\n",
    "\n",
    "# Loop over hyperparameters:\n",
    "for i, (lat, hid, bet, w) in enumerate(hyper_list):\n",
    "    foldername = f\"latent_{lat}_{hid}_{bet}_{w}\"\n",
    "    if foldername not in os.listdir(results_path):\n",
    "        os.mkdir(results_path + foldername)\n",
    "\n",
    "        # Remove model file if it exists\n",
    "        model_path = \"./results_ibdmdb/latent_space/model.pt\"\n",
    "        if os.path.exists(model_path):\n",
    "            os.remove(model_path)\n",
    "\n",
    "        # Train the model\n",
    "        cmd = f'move-dl data=ibdmdb task=ibdmdb__latent task.model.beta={bet} task.model.num_latent={lat} \"task.model.num_hidden={hid}\" data.continuous_inputs.0.weight={w} data.continuous_inputs.1.weight={w} data.continuous_inputs.2.weight={w}'\n",
    "        ! {cmd}\n",
    "\n",
    "        # Copy results\n",
    "        ! cp -r results_ibdmdb/latent_space/ \"{results_path}{foldername}\"\n",
    "        print(f\"Finished {foldername}\")\n",
    "    else:\n",
    "        print(f\"{foldername} was already created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rankings:\n",
    "path = Path(\"./results_ibdmdb/results_temp/\")\n",
    "savepath = Path(\"./results_ibdmdb/\")\n",
    "\n",
    "ls = \"latent_space\"\n",
    "\n",
    "datasets = [\"mgx\", \"mtx\", \"mbx\"]\n",
    "\n",
    "for folder in os.listdir(path):\n",
    "    print(folder)\n",
    "    _, lat, hid, beta, w = folder.split(\"_\")\n",
    "    for dataset in datasets:\n",
    "        df = pd.read_csv(\n",
    "            f\"./results_ibdmdb/results_temp/latent_{lat}_{hid}_{beta}_{w}/feat_importance_aligned_{dataset}.tsv\",\n",
    "            sep=\"\\t\",\n",
    "        ).set_index(\"sample\")\n",
    "        # Order\n",
    "        top10_ids = np.argsort(np.sum(np.abs(df.values), axis=0))[\n",
    "            ::-1\n",
    "        ]  # [:10] to trully get the first 10 in each hyperparam set\n",
    "        order = np.take(df.columns, top10_ids)\n",
    "        with open(savepath / f\"SHAP_{dataset}.txt\", \"a\") as f:\n",
    "            for o, feature in enumerate(order):\n",
    "                f.write(f\"{o}\\t{feature}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rankings:\n",
    "for dataset in datasets:\n",
    "    colnames = [\"Rank\", \"feature\"]\n",
    "\n",
    "    df = pd.read_csv(savepath / f\"SHAP_{dataset}.txt\", sep=\"\\t\", names=colnames)\n",
    "    df_violin = pd.DataFrame()\n",
    "    df_violin[\"Lists\"] = df.groupby(\"feature\")[\"Rank\"].apply(list)\n",
    "    df_violin[\"median\"] = [np.median(l) for l in df_violin[\"Lists\"]]\n",
    "    df_violin = df_violin.sort_values(by=\"median\")\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "    parts = plt.violinplot(df_violin[\"Lists\"], showmedians=True, showextrema=False)\n",
    "\n",
    "    for pc in parts[\"bodies\"]:\n",
    "        pc.set_facecolor(\"#c9dbeb\")\n",
    "        pc.set_alpha(1)\n",
    "\n",
    "    # Customize median points\n",
    "    median_line = parts[\"cmedians\"]\n",
    "    median_line.set_linewidth(0)  # Hide the line\n",
    "    scatter_x = range(1, len(df_violin[\"Lists\"]) + 1)\n",
    "    scatter_y = [np.median(subset) for subset in df_violin[\"Lists\"]]\n",
    "\n",
    "    plt.scatter(scatter_x, scatter_y, color=\"black\", marker=\"D\", s=5, zorder=3)\n",
    "    plt.xticks(\n",
    "        np.arange(1, len(df_violin[\"Lists\"]) + 1), labels=df_violin.index, rotation=90\n",
    "    )\n",
    "    plt.ylabel(\"Rank\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(savepath / f\"{dataset}_SHAP.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate mean positions for categorical clusters and mean latent displacement after perturbations for the IBDMDB cohort:**\n",
    "\n",
    "```latent_location.npy```is a 3D numpy array with shape ($N_{samples}$, $N_{lat}$, $N_{pert}+1$), where the last $N_{samples}$ $\\times$ $N_{lat}$ array corresponds to the baseline representations of the samples in latent space.\n",
    "\n",
    "Please note that we used a VAE with 32 latent nodes, i.e. a latent space with 32 dimensions.\n",
    "\n",
    "Here we will:\n",
    "1. Obtain the mean position / centroid of Dysbiotic samples.\n",
    "2. Obtain the mean position / centroid of non-Dysbiotic samples.\n",
    "3. Calculate the Euclidean distance (in 32D) between them.\n",
    "\n",
    "We will also:\n",
    "\n",
    "4. Calculate the mean displacement of the samples when perturbing each metagenomics feature (plus std.). \n",
    "\n",
    "All of these distances will be given in the arbitrary latent space units (l.u.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "DATASET = \"Aligned_mgx\"  # \"Continuous_A\"\n",
    "feature_list = pd.read_csv(\n",
    "    f\"./interim_data_ibdmdb/{DATASET}.txt\", header=None\n",
    ")\n",
    "feature_list = feature_list.values.flatten()\n",
    "latent_space = np.load(\"./results_ibdmdb/results_temp_id_assoc/aligned_mgx_ks_w1/id_assoc_32_256_0.0001_aligned_mgx_ks_0_w1/latent_location.npy\")\n",
    "print(latent_space.shape)\n",
    "dysbiosis_status = pd.read_csv(\"./ibdmdb_data/aligned_dysbiosis.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Extract baseline (last perturbation)\n",
    "latent_baseline = latent_space[:, :, -1]  # shape (293, 32)\n",
    "\n",
    "# Get boolean mask for dysbiosis status\n",
    "dysbiosis_mask = dysbiosis_status['dysbiosis'].values  # assuming column name is 'dysbiosis'\n",
    "\n",
    "# Calculate baseline centroids for each class\n",
    "centroid_true_baseline = latent_baseline[dysbiosis_mask].mean(axis=0)\n",
    "centroid_false_baseline = latent_baseline[~dysbiosis_mask].mean(axis=0)\n",
    "dysbiosis_centroid_distance = euclidean(centroid_true_baseline, centroid_false_baseline)\n",
    "\n",
    "print(\"True centroid\", centroid_true_baseline)\n",
    "print(\"False centroid\", centroid_false_baseline)\n",
    "\n",
    "print(\"Dysbiosis_centroid_distance:\", np.sqrt(np.sum((centroid_true_baseline-centroid_false_baseline)**2)))\n",
    "print(\"Dysbiosis_centroid_distance:\", dysbiosis_centroid_distance)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(latent_space.shape[2] - 1):  # exclude baseline (-1)\n",
    "\n",
    "    latent_perturbed = latent_space[:, :, i]\n",
    "    # Individual sample movement distances\n",
    "    sample_distances = np.array([euclidean(latent_baseline[j], latent_perturbed[j]) \n",
    "                                for j in range(latent_space.shape[0])])\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'perturbation': i,\n",
    "        'feature': feature_list[i],\n",
    "        'sample_distances_mean': sample_distances.mean(),\n",
    "        'sample_distances_std': sample_distances.std()})\n",
    "\n",
    "# Convert to DataFrame for easy analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df_sorted = results_df.sort_values('sample_distances_mean', ascending=False)\n",
    "\n",
    "results_df_sorted.to_csv('Average_latent_sample_movement_perturbing_mgx.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same model architecture was trained from scratch to predict the movement of samples after categorical perturbations. We perturbed the dysbiosis flag of the samples and measured:\n",
    "\n",
    "1. Distance between class centroids\n",
    "2. Distance between perturbed and baseline sample representations, sample by sample, when perturbing dysbiosis.\n",
    "3. Mean distance travelled by a sample when perturbing dysbiosis.\n",
    "\n",
    "The main conclusion is that $d_{centroids} \\gt \\gt \\frac{1}{N_{samples}}\\sum_{i}^{N_{samples}} \\big | \\vec{r_{pert}}_{i} - \\vec{r_{baseline}}_{i} \\big |$, and hence the samples do not move enough to bridge the gap between clusters. The model therefore does not perceive the representatons of perturbed samples as truly dysbiotic, which we see in the reconstructions as a default back to the original, unperturbed dysbiotic status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"data_path\": \"ibdmdb_data\",\n",
    "    \"interim_path\": \"interim_data_ibdmdb_latent_movements\",\n",
    "    \"results_path\": \"results_ibdmdb_latent_movements\",\n",
    "}\n",
    "\n",
    "! move-dl task=encode_data data=ibdmdb data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} \n",
    "# Copy model trained on analyze latent to id_assoc directory\n",
    "! mkdir -p {hyperparams[\"interim_path\"]}/models\n",
    "! move-dl task=ibdmdb__id_assoc_bayes data=ibdmdb data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} task.target_dataset=\"aligned_dysbiosis\" task.target_value=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "DATASET = \"aligned_dysbiosis\"  # \"Continuous_A\"\n",
    "feature_list = pd.read_csv(\n",
    "    f\"./interim_data_ibdmdb_latent_movements/{DATASET}.txt\", header=None\n",
    ")\n",
    "feature_list = feature_list.values.flatten()\n",
    "latent_space = np.load(\"./latent_location_Bayes.npy\")\n",
    "print(latent_space.shape)\n",
    "dysbiosis_status = pd.read_csv(\"./ibdmdb_data/aligned_dysbiosis.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Extract baseline (last perturbation)\n",
    "latent_baseline = latent_space[:, :, -1]  # shape (293, 32)\n",
    "\n",
    "# Get boolean mask for dysbiosis status\n",
    "dysbiosis_mask = dysbiosis_status['dysbiosis'].values  # assuming column name is 'dysbiosis'\n",
    "\n",
    "# Calculate baseline centroids for each class\n",
    "centroid_true_baseline = latent_baseline[dysbiosis_mask].mean(axis=0)\n",
    "centroid_false_baseline = latent_baseline[~dysbiosis_mask].mean(axis=0)\n",
    "\n",
    "dysbiosis_centroid_distance = euclidean(centroid_true_baseline, centroid_false_baseline)\n",
    "print(\"True centroid\", centroid_true_baseline)\n",
    "print(\"False centroid\", centroid_false_baseline)\n",
    "\n",
    "print(\"Dysbiosis_centroid_distance:\", np.sqrt(np.sum((centroid_true_baseline-centroid_false_baseline)**2)))\n",
    "print(\"Dysbiosis_centroid_distance:\", dysbiosis_centroid_distance)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(latent_space.shape[2] - 1):  # exclude baseline (-1)\n",
    "\n",
    "    latent_perturbed = latent_space[:, :, i]\n",
    "    # Individual sample movement distances\n",
    "    sample_distances = np.array([euclidean(latent_baseline[j], latent_perturbed[j]) \n",
    "                                for j in range(latent_space.shape[0])])\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'perturbation': i,\n",
    "        'feature': feature_list[i],\n",
    "        'sample_distances_mean': sample_distances.mean(),\n",
    "        'sample_distances_std': sample_distances.std()})\n",
    "\n",
    "# Convert to DataFrame for easy analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df_sorted = results_df.sort_values('sample_distances_mean', ascending=False)\n",
    "results_df_sorted\n",
    "results_df_sorted.to_csv('Average_latent_sample_movement_perturbing_dysbiosis.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tunning:**\n",
    "\n",
    "Here we will assess the impact of different hyperparameter combinations on the reconstructed profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune reconstructions\n",
    "! move-dl experiment=ibdmdb__tune_reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"./results_ibdmdb/tune_model/reconstruction_stats.tsv\", sep=\"\\t\")\n",
    "results.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = Path(\"./results_ibdmdb/tune_model/\")\n",
    "\n",
    "fig = plt.figure(layout=\"constrained\", figsize=(20, 8))\n",
    "subfigs = fig.subfigures(2, 1)\n",
    "\n",
    "batch_sizes = [16, 32]\n",
    "num_hidden = [[256], [512]]\n",
    "num_latent = [8, 32, 64]\n",
    "\n",
    "# Colors:\n",
    "batch_colors = {\"train\": [\"#7570b3\", \"#4c4982\"], \"test\": [\"#1b9e77\", \"#0f6b4f\"]}\n",
    "\n",
    "for i in range(len(num_hidden)):\n",
    "    subfigs[i].suptitle(f\"Hidden: {num_hidden[i]}\")\n",
    "\n",
    "    subset_results = results.query(f\"`task.model.num_hidden` == '{num_hidden[i]}'\")\n",
    "\n",
    "    datasets = [\n",
    "        \"aligned_mbx\",\n",
    "        \"aligned_mgx\",\n",
    "        \"aligned_mtx\",\n",
    "        \"aligned_ibd\",\n",
    "        \"aligned_dysbiosis\",\n",
    "    ]\n",
    "    titles = [\n",
    "        \"Metabolomics\",\n",
    "        \"Metagenomics\",\n",
    "        \"Metatranscriptomics\",\n",
    "        \"IBD status\",\n",
    "        \"Dysbiosis\",\n",
    "    ]\n",
    "\n",
    "    axes = subfigs[i].subplots(1, 5)\n",
    "\n",
    "    legend_handles_all = []  # Store handles for legend\n",
    "\n",
    "    for j, (dataset, title) in enumerate(zip(datasets, titles)):\n",
    "        dataset_results = subset_results.query(f\"dataset == '{dataset}'\")\n",
    "\n",
    "        for batch_idx, batch_size in enumerate(batch_sizes):\n",
    "            batch_results = dataset_results.query(f\"`task.batch_size` == {batch_size}\")\n",
    "\n",
    "            train_stats = []\n",
    "            test_stats = []\n",
    "\n",
    "            for latent in num_latent:\n",
    "                train_data = batch_results.query(\n",
    "                    f\"split == 'train' and `task.model.num_latent` == {latent}\"\n",
    "                )\n",
    "                test_data = batch_results.query(\n",
    "                    f\"split == 'test' and `task.model.num_latent` == {latent}\"\n",
    "                )\n",
    "\n",
    "                if len(train_data) > 0:\n",
    "                    train_stats.append(train_data.to_dict(orient=\"records\")[0])\n",
    "                    train_stats[-1][\"fliers\"] = []\n",
    "\n",
    "                if len(test_data) > 0:\n",
    "                    test_stats.append(test_data.to_dict(orient=\"records\")[0])\n",
    "                    test_stats[-1][\"fliers\"] = []\n",
    "\n",
    "            n_fields = len(train_stats)\n",
    "            offset = batch_idx * 0.2 - 0.1\n",
    "            positions_train = [x + offset for x in range(0, n_fields * 2, 2)]\n",
    "            positions_test = [x + offset for x in range(1, n_fields * 2, 2)]\n",
    "\n",
    "            if train_stats:\n",
    "                coll1 = axes[j].bxp(\n",
    "                    train_stats,\n",
    "                    positions=positions_train,\n",
    "                    widths=0.15,\n",
    "                    boxprops=dict(facecolor=batch_colors[\"train\"][batch_idx]),\n",
    "                    patch_artist=True,\n",
    "                )\n",
    "                if j == 0:  # Only collect from first dataset\n",
    "                    legend_handles_all.append(coll1[\"boxes\"][0])\n",
    "\n",
    "            if test_stats:\n",
    "                coll2 = axes[j].bxp(\n",
    "                    test_stats,\n",
    "                    positions=positions_test,\n",
    "                    widths=0.15,\n",
    "                    boxprops=dict(facecolor=batch_colors[\"test\"][batch_idx]),\n",
    "                    patch_artist=True,\n",
    "                )\n",
    "                if j == 0:  # Only collect from first dataset\n",
    "                    legend_handles_all.append(coll2[\"boxes\"][0])\n",
    "\n",
    "        axes[j].set(\n",
    "            xticks=np.arange(0.5, n_fields * 2, 2),\n",
    "            xticklabels=num_latent[:n_fields],\n",
    "            ylim=(0, 1),\n",
    "            xlabel=\"# latent dimensions\",\n",
    "            title=title,\n",
    "        )\n",
    "\n",
    "        if j == 0:\n",
    "            axes[j].set_ylabel(\"Reconstruction accuracy\")\n",
    "        else:\n",
    "            axes[j].set_yticklabels([])\n",
    "\n",
    "        if i == 0:\n",
    "            axes[j].set_xlabel(\"\")\n",
    "\n",
    "    # Add legend to last subplot of bottom row\n",
    "    if i == 1 and legend_handles_all:\n",
    "        legend_labels = [\n",
    "            f\"train (bs={batch_sizes[0]})\",\n",
    "            f\"test (bs={batch_sizes[0]})\",\n",
    "            f\"train (bs={batch_sizes[1]})\",\n",
    "            f\"test (bs={batch_sizes[1]})\",\n",
    "        ]\n",
    "        axes[-1].legend(\n",
    "            legend_handles_all,\n",
    "            legend_labels,\n",
    "            title=\"split & batch size\",\n",
    "            fontsize=\"small\",\n",
    "        )\n",
    "\n",
    "fig.savefig(savepath / \"Tunning_reconstructions.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify associations**\n",
    "\n",
    "Here we will identify associations between Dysbiosis (cat), Metagenomics and Metatranscriptomics (cont) and the other continuous variables. We will use both KS and Bayes methods for the continuous, and the associations will be calculated when setting the weights on all continuous data modalities to 1 or 2. These hyperparams will be overwritten on the fly using hydra's functionalities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified identification associations analysis\n",
    "results_path = \"./results_ibdmdb/results_temp_id_assoc/\"\n",
    "Path(results_path).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Configuration\n",
    "target_ds_list = [\n",
    "    (\"aligned_mgx\", \"plus_std\"),\n",
    "    (\"aligned_mtx\", \"plus_std\"),\n",
    "    (\"aligned_dysbiosis\", 1),\n",
    "]\n",
    "NUM_EPOCHS = 400\n",
    "N_REFITS = 24\n",
    "BAYES_SIG_THR = 0.1\n",
    "hyper_list = [(lat, hid, bet) for bet in [0.0001] for hid in [[256]] for lat in [32]]\n",
    "w_values = [1, 2]\n",
    "\n",
    "for w in w_values:\n",
    "    print(f\"=== Processing weight {w} ===\")\n",
    "\n",
    "    for i, (lat, hid, bet) in enumerate(hyper_list):\n",
    "        hid_flat = hid[0]  # Extract 500 from [500]\n",
    "\n",
    "        # Run Bayes with N_REFITS for all targets\n",
    "        for TARGET_DS, TARGET_VALUE in target_ds_list:\n",
    "            ds_results_path = f\"{results_path}{TARGET_DS}_bayes_w{w}/\"\n",
    "            ! mkdir -p {ds_results_path}\n",
    "\n",
    "            foldername = f\"id_assoc_{lat}_{hid_flat}_{bet}_{TARGET_DS}_bayes_w{w}\"\n",
    "\n",
    "            if foldername not in os.listdir(ds_results_path):\n",
    "                ! mkdir -p {ds_results_path}{foldername}\n",
    "                print(f\"Starting Bayes {foldername}\")\n",
    "\n",
    "                cmd = f'move-dl data=ibdmdb task=ibdmdb__id_assoc_bayes task.model.beta={bet} task.model.num_latent={lat} \"task.model.num_hidden={hid}\" task.target_dataset={TARGET_DS} task.num_refits={N_REFITS} task.target_value={TARGET_VALUE} task.sig_threshold={BAYES_SIG_THR} task.training_loop.num_epochs={NUM_EPOCHS} data.continuous_inputs.0.weight={w} data.continuous_inputs.1.weight={w} data.continuous_inputs.2.weight={w}'\n",
    "                ! {cmd}\n",
    "\n",
    "                ! cp -r results_ibdmdb/identify_associations/ \"{ds_results_path}{foldername}\"\n",
    "                print(f\"Finished Bayes {foldername}\")\n",
    "            else:\n",
    "                print(f\"Bayes {foldername} already exists\")\n",
    "\n",
    "        # Store models after Bayes training\n",
    "        models_backup_path = f\"./all_models/models_{lat}_{hid_flat}_{bet}_w{w}/\"\n",
    "        ! mkdir -p {models_backup_path}\n",
    "        ! cp -r interim_data_ibdmdb/models {models_backup_path}\n",
    "\n",
    "        # Run KS N_REFITS times (only for continuous targets)\n",
    "        for run_idx in range(N_REFITS):\n",
    "            # Reset models directory and load single model\n",
    "            ! rm -rf interim_data_ibdmdb/models\n",
    "            ! mkdir -p interim_data_ibdmdb/models\n",
    "            ! cp \"{models_backup_path}models/model_{lat}_{run_idx}.pt\" interim_data_ibdmdb/models/model_{lat}_0.pt\n",
    "\n",
    "            for TARGET_DS, TARGET_VALUE in target_ds_list:\n",
    "                # Skip categorical targets for KS\n",
    "                if TARGET_VALUE == 1:\n",
    "                    continue\n",
    "\n",
    "                ds_results_path = f\"{results_path}{TARGET_DS}_ks_w{w}/\"\n",
    "                ! mkdir -p {ds_results_path}\n",
    "\n",
    "                foldername = (\n",
    "                    f\"id_assoc_{lat}_{hid_flat}_{bet}_{TARGET_DS}_ks_{run_idx}_w{w}\"\n",
    "                )\n",
    "\n",
    "                if foldername not in os.listdir(ds_results_path):\n",
    "                    ! mkdir -p {ds_results_path}{foldername}\n",
    "                    print(f\"Starting KS {foldername}\")\n",
    "\n",
    "                    cmd = f'move-dl data=ibdmdb task=ibdmdb__id_assoc_ks task.model.beta={bet} task.model.num_latent={lat} \"task.model.num_hidden={hid}\" task.target_dataset={TARGET_DS} task.num_refits=1 task.target_value={TARGET_VALUE} task.sig_threshold=0.999 task.training_loop.num_epochs={NUM_EPOCHS} data.continuous_inputs.0.weight={w} data.continuous_inputs.1.weight={w} data.continuous_inputs.2.weight={w}'\n",
    "                    ! {cmd}\n",
    "\n",
    "                    ! cp -r results_ibdmdb/identify_associations/ \"{ds_results_path}{foldername}\"\n",
    "                    print(f\"Finished KS {foldername}\")\n",
    "                else:\n",
    "                    print(f\"KS {foldername} already exists\")\n",
    "\n",
    "print(\"All analyses completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "cmd = f'move-dl data=ibdmdb task=ibdmdb__latent task.model.beta=0.0001 task.model.num_latent=16 \"task.model.num_hidden=[64]\" data.continuous_inputs.0.weight=1 data.continuous_inputs.1.weight=1 data.continuous_inputs.2.weight=1'\n",
    "! {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "target_ds_list = [\n",
    "    (\"aligned_mgx\", \"plus_std\"),\n",
    "    (\"aligned_mtx\", \"plus_std\"),\n",
    "    (\"aligned_dysbiosis\", 1),\n",
    "]\n",
    "NUM_EPOCHS = 400\n",
    "N_REFITS = 24\n",
    "BAYES_SIG_THR = 0.1\n",
    "hyper_list = [(lat, hid, bet) for bet in [0.0001] for hid in [[256]] for lat in [32]]\n",
    "w_values = [1, 2]\n",
    "\n",
    "cmd = f'move-dl data=ibdmdb task=ibdmdb__id_assoc_bayes task.model.beta=0.0001 task.model.num_latent=32 \"task.model.num_hidden=[256]\" task.target_dataset=aligned_dysbiosis task.num_refits=24 task.target_value=1 task.sig_threshold={BAYES_SIG_THR} task.training_loop.num_epochs={NUM_EPOCHS} data.continuous_inputs.0.weight=2 data.continuous_inputs.1.weight=2 data.continuous_inputs.2.weight=2'\n",
    "! {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_threshold(N, alpha):\n",
    "    return np.sqrt(-(1 / N) * np.log(alpha / 2))\n",
    "\n",
    "\n",
    "N_SAMPLES = 293\n",
    "ALPHA = 0.05\n",
    "KS_THR = ks_threshold(N_SAMPLES, ALPHA)\n",
    "KS_THR = 0.075\n",
    "\n",
    "\n",
    "def extract_folder_info(folder_name):\n",
    "    \"\"\"Extract dataset, method, weight, and run from folder names\"\"\"\n",
    "    parts = folder_name.split(\"_\")\n",
    "\n",
    "    # Find weight (always last part starting with 'w')\n",
    "    weight = parts[-1][1:]  # Remove 'w' prefix\n",
    "\n",
    "    # Find method (bayes or ks)\n",
    "    method_idx = None\n",
    "    for i, part in enumerate(parts):\n",
    "        if part in [\"bayes\", \"ks\"]:\n",
    "            method_idx = i\n",
    "            method = part\n",
    "            break\n",
    "\n",
    "    if method_idx is None:\n",
    "        return None\n",
    "\n",
    "    # Dataset is the part before method\n",
    "    dataset = parts[method_idx - 1]\n",
    "\n",
    "    # For ks, run is the part after method\n",
    "    run = None\n",
    "    if method == \"ks\" and method_idx + 1 < len(parts) - 1:\n",
    "        run = parts[method_idx + 1]\n",
    "\n",
    "    return {\"dataset\": dataset, \"method\": method, \"weight\": weight, \"run\": run}\n",
    "\n",
    "\n",
    "def process_bayes_folders(results_dir):\n",
    "    \"\"\"Process all bayes folders and combine TSVs\"\"\"\n",
    "    bayes_dfs = []\n",
    "\n",
    "    for method_folder in os.listdir(results_dir):\n",
    "        if \"bayes\" not in method_folder:\n",
    "            continue\n",
    "\n",
    "        method_path = os.path.join(results_dir, method_folder)\n",
    "        if not os.path.isdir(method_path):\n",
    "            continue\n",
    "\n",
    "        for subfolder in os.listdir(method_path):\n",
    "            subfolder_path = os.path.join(method_path, subfolder)\n",
    "            if not os.path.isdir(subfolder_path):\n",
    "                continue\n",
    "\n",
    "            info = extract_folder_info(subfolder)\n",
    "            if not info or info[\"method\"] != \"bayes\" or info[\"dataset\"] == \"dysbiosis\":\n",
    "                continue\n",
    "\n",
    "            tsv_file = os.path.join(subfolder_path, f\"results_sig_assoc_bayes.tsv\")\n",
    "            if os.path.exists(tsv_file):\n",
    "                df = pd.read_csv(tsv_file, sep=\"\\t\")\n",
    "                df[\"Perturbing_dataset\"] = info[\"dataset\"]\n",
    "                df[\"Continuous_weight\"] = info[\"weight\"]\n",
    "                bayes_dfs.append(df)\n",
    "\n",
    "    if bayes_dfs:\n",
    "        return pd.concat(bayes_dfs, ignore_index=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_ks_folders(results_dir):\n",
    "    \"\"\"Process all ks folders and combine TSVs into single table\"\"\"\n",
    "    ks_dfs = []\n",
    "\n",
    "    for method_folder in os.listdir(results_dir):\n",
    "        if \"ks\" not in method_folder:\n",
    "            continue\n",
    "\n",
    "        method_path = os.path.join(results_dir, method_folder)\n",
    "        if not os.path.isdir(method_path):\n",
    "            continue\n",
    "\n",
    "        for subfolder in os.listdir(method_path):\n",
    "            subfolder_path = os.path.join(method_path, subfolder)\n",
    "            if not os.path.isdir(subfolder_path):\n",
    "                continue\n",
    "\n",
    "            info = extract_folder_info(subfolder)\n",
    "            if not info or info[\"method\"] != \"ks\" or info[\"dataset\"] == \"dysbiosis\":\n",
    "                continue\n",
    "\n",
    "            tsv_file = os.path.join(subfolder_path, f\"results_sig_assoc_ks.tsv\")\n",
    "            if os.path.exists(tsv_file):\n",
    "                df = pd.read_csv(tsv_file, sep=\"\\t\")\n",
    "                df[\"Perturbing_dataset\"] = info[\"dataset\"]\n",
    "                df[\"Continuous_weight\"] = info[\"weight\"]\n",
    "                df[\"Run\"] = info[\"run\"]\n",
    "                ks_dfs.append(df)\n",
    "\n",
    "    if ks_dfs:\n",
    "        return pd.concat(ks_dfs, ignore_index=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Main execution\n",
    "results_dir = \"./results_ibdmdb/results_temp_id_assoc\"\n",
    "\n",
    "# Process bayes folders\n",
    "bayes_combined = process_bayes_folders(results_dir)\n",
    "if bayes_combined is not None:\n",
    "    bayes_combined.to_csv(\"combined_bayes_results.tsv\", sep=\"\\t\", index=False)\n",
    "    print(f\"Bayes results saved: {len(bayes_combined)} rows\")\n",
    "\n",
    "# Process ks folders\n",
    "ks_combined = process_ks_folders(results_dir)\n",
    "if ks_combined is not None:\n",
    "    # At the top with other variables\n",
    "    ks_combined = ks_combined[ks_combined[\"ks_distance\"] >= KS_THR].sort_values(\n",
    "        \"ks_distance\", ascending=False\n",
    "    )\n",
    "    ks_combined.to_csv(\"combined_ks_results.tsv\", sep=\"\\t\", index=False)\n",
    "    print(f\"KS results saved: {len(ks_combined)} rows\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_sorted = ks_combined.sort_values(\"ks_distance\", ascending=True)\n",
    "ks_sorted[ks_sorted[\"feature_b_dataset\"] == \"aligned_mbx\"].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous: \n",
    "\n",
    "**KS threshold as a function of the number of samples and the alpha / p-value threshold we'd like to set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute KS threshold\n",
    "\n",
    "\n",
    "def ks_threshold(N, alpha):\n",
    "    return np.sqrt(-(1 / N) * np.log(alpha / 2))\n",
    "\n",
    "\n",
    "# Define the range for N and alpha\n",
    "N_values = np.linspace(100, 500, 10000)  # Avoid zero to prevent division by zero\n",
    "alpha_values = np.linspace(0.001, 1, 1000)  # Avoid zero for alpha to prevent log(0)\n",
    "\n",
    "# Create a meshgrid for N and alpha\n",
    "N_grid, alpha_grid = np.meshgrid(N_values, alpha_values)\n",
    "ks_thrs = ks_threshold(N_grid, alpha_grid)\n",
    "\n",
    "# Create a 2D heatmap\n",
    "fig, ax = plt.subplots(figsize=(7, 3.5))\n",
    "c = ax.pcolormesh(N_grid, alpha_grid, ks_thrs, shading=\"auto\", cmap=\"RdYlBu\")\n",
    "\n",
    "# Add a color bar\n",
    "fig.colorbar(c, ax=ax, label=\"KS Threshold\")\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel(\"Number of Samples N\")\n",
    "ax.set_ylabel(\"Significance Threshold\")\n",
    "ax.set_title(\"Heatmap of KS Threshold\")\n",
    "# Set x-axis to logarithmic scale\n",
    "# ax.set_xscale('log')\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"KS_thr_heatmap.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes score trends:**\n",
    "\n",
    "The Bayes score measured, at a cohort level, whether perturbing a feature A will shift the values of any other feature B up or down. The score is derived from the fraction of individuals or samples that recapitulate such behavior ($prob$). Here we plot the different scores as a function of the number of samples going up or down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = np.arange(0, 1.01, 0.01)\n",
    "bayes_k = np.log(prob + 1e-8) - np.log(1 - prob + 1e-8)\n",
    "bayes_abs = np.abs(bayes_k)\n",
    "bayes_p = np.exp(bayes_abs) / (1 + np.exp(bayes_abs))\n",
    "\n",
    "fig, axs = plt.subplots(3)\n",
    "axs[0].plot(prob, prob)\n",
    "axs[0].set_xlabel(\"prob\")\n",
    "axs[0].set_ylabel(\"prob\")\n",
    "axs[1].plot(prob, bayes_k)\n",
    "axs[1].set_xlabel(\"prob\")\n",
    "axs[1].set_ylabel(\"bayes_k\")\n",
    "axs[2].plot(prob, bayes_p)\n",
    "axs[2].set_xlabel(\"prob\")\n",
    "axs[2].set_ylabel(\"bayes_p\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residualization:**\n",
    "\n",
    "This function can be used if we want to remove the linear contributions of a number of covariates to the data before feeding the data to MOVE. Please be careful not to log2 transform residualized data, which will contain negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residualize(targets, covariates):\n",
    "    \"\"\"\n",
    "    This function trains a linear model to take into account the contributions\n",
    "    of age, sex and batch in the gene expression values (RNA counts).\n",
    "\n",
    "    Args:\n",
    "        targets: Pandas dataframe of shape (N_samples x N_target_features)\n",
    "                 containing the target features to correct.\n",
    "        covariates: Pandas dataframe of shape (N_samples x N_covariates)\n",
    "                    containing the independent variables that we want to\n",
    "                    correct for.\n",
    "    Returns:\n",
    "        corrected_targets: Array with the linear contributions of the covariates\n",
    "                           removed.\n",
    "\n",
    "    \"\"\"\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Add a constant term to the independent variables\n",
    "    covariates = sm.add_constant(covariates)\n",
    "\n",
    "    # Fit a multivariate linear regression model\n",
    "    model = sm.OLS(targets, covariates).fit()\n",
    "    predictions = pd.DataFrame(data=model.predict(covariates))\n",
    "    predictions.columns = targets.columns\n",
    "\n",
    "    # Get the residuals (corrected values) for each target variable\n",
    "    residuals = targets - predictions\n",
    "\n",
    "    # Return the residuals (corrected targets)\n",
    "    return residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the reconstructed class after perturbing a categorical variable:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.load(\"cat_class_baseline.npy\"), np.load(\"cat_recon_baseline.npy\")\n",
    "c, d = np.load(\"cat_class_perturb_0.npy\"), np.load(\"cat_recon_perturb_0.npy\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"class_baseline\": a[:, 1],\n",
    "        \"recon_baseline\": b[:, 1],\n",
    "        \"class_perturb\": c[:, 1],\n",
    "        \"recon_perturb\": d[:, 1],\n",
    "    }\n",
    ")\n",
    "df.to_csv(\"Baseline_vs_reconstructed_Bayes_dysbiosis.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Posterior collapse:**\n",
    "\n",
    "*Is the KLD term zero because we multiply it by zero ($\\beta \\sim 0$) or do we actually have a posterior collapse?*\n",
    "> **A:** $\\beta \\sim 0$\n",
    ">\n",
    "> In move's source code we find ```loss_function()``` in the file VAE.py. Comment the KLD product in the returned variables as follows:\n",
    "> ```python\n",
    ">        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / (batch_size)\n",
    ">        KLD_weight = self.beta * kld_w\n",
    ">        loss = CE + MSE + KLD * KLD_weight\n",
    ">\n",
    ">        return loss, CE, MSE, KLD #* KLD_weight\n",
    "> ```\n",
    "> - KLD is the pure, unweighted KLD.\n",
    "\n",
    "The results show that KLD term (not including warm-up weight nor beta) rises unconstrained because KLD does not contribute to the loss.\n",
    "\n",
    "On the contrary:\n",
    "> **B:** $\\beta = 1$ always, with no warm-up.\n",
    "> ```python\n",
    ">        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / (batch_size)\n",
    ">        KLD_weight = self.beta * kld_w\n",
    ">        loss = CE + MSE + KLD #* KLD_weight\n",
    ">\n",
    ">        return loss, CE, MSE, KLD #* KLD_weight\n",
    "> ```\n",
    "\n",
    "Using directly KLD with weight 1 from epoch zero generally leads to posterior collapse, with a latent space that is not gaussian, zero KLD in the end and poor reconstruction accuracies.\n",
    "\n",
    "> **C:** $\\beta = 2,10,...$ with KLD warmup.\n",
    "\n",
    "When we increase $\\beta$, we increase the push of the samples towards the center of the latent space. However, the KLD is still competing with the categorical loss. We still observed the creation of separate clusters, more condensed and closer to the center of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"N_SAMPLES\": 1000,\n",
    "    \"HIGH_CORR\": True,\n",
    "    \"data_path\": \"synthetic_data_II\",\n",
    "    \"interim_path\": \"interim_data_cont_paper_III\",\n",
    "    \"results_path\": \"results_cont_paper_III\",\n",
    "    \"w\": 1,\n",
    "}\n",
    "\n",
    "! move-dl task=encode_data data=random_continuous_paper_II data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} +data.continuous_inputs.0.weight={hyperparams[\"w\"]} +data.continuous_inputs.1.weight={hyperparams[\"w\"]}\n",
    "! move-dl task=random_continuous_paper_II__latent data=random_continuous_paper_II data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} +data.continuous_inputs.0.weight={hyperparams[\"w\"]} +data.continuous_inputs.1.weight={hyperparams[\"w\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happens under high regularization, i.e. $\\beta$ = 1 ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"N_SAMPLES\": 1000,\n",
    "    \"HIGH_CORR\": True,\n",
    "    \"data_path\": \"synthetic_data_II\",\n",
    "    \"interim_path\": \"interim_data_cont_paper_beta_1\",\n",
    "    \"results_path\": \"results_cont_paper_beta_1\",\n",
    "    \"beta\":1\n",
    "}\n",
    "\n",
    "! move-dl task=encode_data data=random_continuous_paper_II data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} \n",
    "! move-dl task=random_continuous_paper_II__latent data=random_continuous_paper_II data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} task.model.beta={hyperparams[\"beta\"]} \n",
    "# Copy model trained on analyze latent to id_assoc directory\n",
    "! mkdir -p {hyperparams[\"interim_path\"]}/models\n",
    "! mv {hyperparams[\"results_path\"]}/latent_space/model.pt {hyperparams[\"interim_path\"]}/models/model_3_0.pt\n",
    "! move-dl task=random_continuous_paper_II__id_assoc_ks data=random_continuous_paper_II data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} task.model.beta={hyperparams[\"beta\"]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"classic\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"font.family\": \"Times New Roman\",\n",
    "        \"font.size\": 14,\n",
    "        \"xtick.labelsize\": 14,\n",
    "        \"ytick.labelsize\": 14,\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"legend.fontsize\": 14,\n",
    "    }\n",
    ")\n",
    "\n",
    "DATASET = \"Continuous_B\"  # \"Continuous_A\"\n",
    "feature_list = pd.read_csv(\n",
    "    f\"./interim_data_cont_paper_beta_1_no_warmup/random.random_all_sim.{DATASET}.txt\", header=None\n",
    ")\n",
    "feature_list = feature_list.values.flatten()\n",
    "\n",
    "figure_path = Path(\"./synthetic_data_II/figures_synthetic_small_beta_1_no_warmup/\")\n",
    "! mkdir -p {figure_path}\n",
    "\n",
    "latent_space = np.load(\n",
    "    \"./results_cont_paper_beta_1_no_warmup/identify_associations/latent_location.npy\"\n",
    ")\n",
    "\n",
    "# Main code:\n",
    "for i, feature in enumerate(feature_list):\n",
    "    print(i)\n",
    "    feature_values = np.load(\n",
    "        f\"./interim_data_cont_paper_beta_1_no_warmup/random.random_all_sim.{DATASET}.npy\"\n",
    "    )\n",
    "    CONDITION = \"Categorical\" in DATASET\n",
    "    if CONDITION:\n",
    "        feature_values = np.argmax(feature_values, axis=2)[:, i]\n",
    "    else:\n",
    "        feature_values = feature_values[:, i]\n",
    "\n",
    "    print(feature_values)\n",
    "    latent_space_baseline = latent_space[:, :, -1]\n",
    "    latent_space_perturbed = latent_space[:, :, i]\n",
    "\n",
    "    # # Plot latent space:\n",
    "    n_pictures = 10\n",
    "    PLOT = [0] if CONDITION else [0, 2, 3, 9]  # Plot Categorical A1\n",
    "    if i in PLOT:\n",
    "        print(i)\n",
    "        # for pic_num, (azimuth, altitude) in enumerate(zip(\n",
    "        #    np.linspace(90, 180, n_pictures), np.linspace(35, 55, n_pictures)\n",
    "        # )):\n",
    "\n",
    "        for pic_num, (azimuth, altitude) in enumerate([[120, 44]]):\n",
    "            print(pic_num, azimuth, altitude)\n",
    "            title = feature\n",
    "            fig = plot_3D_latent_and_displacement(\n",
    "                latent_space_baseline,\n",
    "                latent_space_perturbed,\n",
    "                feature_values=feature_values,\n",
    "                feature_name=f\"{title}\",\n",
    "                show_baseline=True,\n",
    "                show_perturbed=False,\n",
    "                show_arrows=False,\n",
    "                altitude=altitude,\n",
    "                azimuth=azimuth,\n",
    "            )\n",
    "            fig.savefig(figure_path / f\"3D_latent_{pic_num}_perturbed_{feature}.svg\")\n",
    "            plt.close(fig)\n",
    "\n",
    "            fig = plot_3D_latent_and_displacement(\n",
    "                latent_space_baseline,\n",
    "                latent_space_perturbed,\n",
    "                feature_values=feature_values,\n",
    "                feature_name=f\"{title}\",\n",
    "                show_baseline=False,\n",
    "                show_perturbed=False,\n",
    "                show_arrows=True,\n",
    "                altitude=altitude,\n",
    "                azimuth=azimuth,\n",
    "            )\n",
    "            fig.savefig(\n",
    "                figure_path / f\"3D_latent_{pic_num}_perturbed_{feature}_arrows_max.svg\"\n",
    "            )\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No correlations:**\n",
    "\n",
    "*What happens when there are no correlations to be found? Do the models still find associations?*\n",
    "\n",
    "**Create a dataset with no correlations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Hyperparameters ####################################\n",
    "PROJECT_NAME = \"random_all_sim\"\n",
    "MODE = \"linear\"  # \"non-linear\"\n",
    "SEED_1 = 123  # 1234\n",
    "np.random.seed(SEED_1)\n",
    "rnd.seed(SEED_1)\n",
    "\n",
    "COV_ALPHA = 1\n",
    "\n",
    "data_regimes_dict = {\n",
    "    \"synthetic_data_nocorr\": {\n",
    "        \"N_SAMPLES\": 1000,\n",
    "        \"HIGH_CORR\": False,\n",
    "        \"interim_path\": \"interim_data_nocorr\",\n",
    "        \"results_path\": \"results_nocorr\",\n",
    "    },  # high N, added explicit corr\n",
    "}\n",
    "\n",
    "SETTINGS = {\n",
    "    \"Continuous_A\": {\n",
    "        \"features\": 10,\n",
    "        \"frequencies\": [0.002, 0.01, 0.02],\n",
    "        \"coefficients\": [500, 100, 50],\n",
    "        \"phase\": 0,\n",
    "        \"offset\": 500,\n",
    "    },\n",
    "    \"Continuous_B\": {\n",
    "        \"features\": 10,\n",
    "        \"frequencies\": [0.001, 0.05, 0.08],\n",
    "        \"coefficients\": [80, 20, 10],\n",
    "        \"phase\": np.pi / 2,\n",
    "        \"offset\": 400,\n",
    "    },\n",
    "    \"Categorical_A\": {\n",
    "        \"features\": 1,\n",
    "        \"frequencies\": [0.1, 0.5, 0.8],\n",
    "        \"coefficients\": [0.2, 0.1, 0.05],\n",
    "        \"phase\": np.pi / 2,\n",
    "        \"offset\": 10,\n",
    "    },\n",
    "    \"Categorical_B\": {\n",
    "        \"features\": 1,\n",
    "        \"frequencies\": [0.01, 0.5, 0.08],\n",
    "        \"coefficients\": [10, 0.1, 0.05],\n",
    "        \"phase\": np.pi,\n",
    "        \"offset\": 1,\n",
    "    },\n",
    "}\n",
    "\n",
    "for path, hyperparams in data_regimes_dict.items():\n",
    "\n",
    "    COR_THRES = 0.02\n",
    "    PAIRS_OF_INTEREST = [(1, 2), (3, 4)]\n",
    "    HIGH_CORR = hyperparams[\"HIGH_CORR\"]\n",
    "    N_SAMPLES = hyperparams[\"N_SAMPLES\"]\n",
    "\n",
    "    # Path to store output files\n",
    "    outpath = Path(f\"{path}\")\n",
    "    outpath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # %%\n",
    "    # Add all datasets in a single matrix:\n",
    "    all_feature_names = get_feature_names(SETTINGS)\n",
    "    feat_means = create_mean_profiles(SETTINGS)\n",
    "\n",
    "    # %%\n",
    "    ###### Covariance matrix definition ######\n",
    "    if MODE == \"linear\":\n",
    "        covariance_matrix = make_sparse_spd_matrix(\n",
    "            n_dim=len(all_feature_names),\n",
    "            alpha=COV_ALPHA,\n",
    "            smallest_coef=0,\n",
    "            largest_coef=1,\n",
    "            norm_diag=True,\n",
    "            random_state=SEED_1,\n",
    "        )\n",
    "    elif MODE == \"non-linear\":\n",
    "        covariance_matrix = np.identity(len(all_feature_names))\n",
    "\n",
    "    ABS_MAX = np.max(abs(covariance_matrix))\n",
    "    fig = plot_score_matrix(\n",
    "        covariance_matrix, all_feature_names, vmin=-ABS_MAX, vmax=ABS_MAX\n",
    "    )\n",
    "    fig.savefig(outpath / f\"Covariance_matrix_{PROJECT_NAME}.svg\")\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = np.random.multivariate_normal(feat_means, covariance_matrix, N_SAMPLES)\n",
    "\n",
    "    # Add non-linearities\n",
    "    if MODE == \"non-linear\":\n",
    "        for i, j in PAIRS_OF_INTEREST:\n",
    "            freq = np.random.choice([4, 5, 6])\n",
    "            dataset[:, i] += np.sin(freq * dataset[:, j])\n",
    "\n",
    "    # scaled_dataset, _ = scale(dataset)\n",
    "    # No scaling in the dataset creation! It will be handled in preprocessing.\n",
    "    scaled_dataset = dataset\n",
    "\n",
    "    if HIGH_CORR:  # The last half of the features are combinations of the first half:\n",
    "        for i in range(scaled_dataset.shape[1] // 2):\n",
    "            col_1 = np.random.choice(range(scaled_dataset.shape[1] // 2))\n",
    "            col_2 = np.random.choice(range(scaled_dataset.shape[1] // 2))\n",
    "            scaled_dataset[:, i + scaled_dataset.shape[1] // 2] = (\n",
    "                scaled_dataset[:, col_1] + scaled_dataset[:, col_2]\n",
    "            ) / 2 + np.random.normal()\n",
    "\n",
    "    # Binarize the categorical dataset\n",
    "    NUM_CAT = (\n",
    "        SETTINGS[\"Categorical_A\"][\"features\"] + SETTINGS[\"Categorical_B\"][\"features\"]\n",
    "    )\n",
    "    columns_to_binarize = scaled_dataset[:, -NUM_CAT:]\n",
    "\n",
    "    # Compute the mean of each of the categorical columns\n",
    "    means = columns_to_binarize.mean(axis=0)\n",
    "\n",
    "    # Apply the binarization\n",
    "    scaled_dataset[:, -NUM_CAT:] = (columns_to_binarize > means).astype(int)\n",
    "\n",
    "    # Correlations\n",
    "    correlations = np.corrcoef(scaled_dataset, rowvar=False)\n",
    "    fig = plot_score_matrix(\n",
    "        correlations, all_feature_names, vmin=-1, vmax=1, label_step=5\n",
    "    )\n",
    "    fig.savefig(outpath / f\"Correlations_{PROJECT_NAME}.svg\", dpi=200)\n",
    "\n",
    "    # Sort correlations by absolute value\n",
    "    associations = create_ground_truth_correlations_file(correlations)\n",
    "    associations.to_csv(outpath / f\"changes.{PROJECT_NAME}.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "    # Plot feature profiles per sample\n",
    "    fig = plot_feature_profiles(scaled_dataset, feat_means)\n",
    "    fig.savefig(outpath / \"Multi-omic_profiles.svg\")\n",
    "\n",
    "    ## Plot correlations\n",
    "    fig = plot_feature_correlations(dataset, PAIRS_OF_INTEREST)\n",
    "    fig.savefig(outpath / \"Feature_correlations.svg\")\n",
    "\n",
    "    fig = plot_feature_correlations(scaled_dataset, PAIRS_OF_INTEREST)\n",
    "    fig.savefig(outpath / \"Feature_correlations_scaled.svg\")\n",
    "\n",
    "    # Write tsv files with feature values for all samples in both datasets:\n",
    "    save_splitted_datasets(\n",
    "        SETTINGS, PROJECT_NAME, scaled_dataset, all_feature_names, N_SAMPLES, outpath\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"N_SAMPLES\": 1000,\n",
    "    \"HIGH_CORR\": False,\n",
    "    \"data_path\": \"synthetic_data_nocorr\",\n",
    "    \"interim_path\": \"interim_data_nocorr\",\n",
    "    \"results_path\": \"results_nocorr\",\n",
    "    \"w\": 1,\n",
    "}\n",
    "\n",
    "! move-dl task=encode_data data=random_continuous_paper_II data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} +data.continuous_inputs.0.weight={hyperparams[\"w\"]} +data.continuous_inputs.1.weight={hyperparams[\"w\"]}\n",
    "! move-dl task=random_continuous_paper_II__latent data=random_continuous_paper_II data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} +data.continuous_inputs.0.weight={hyperparams[\"w\"]} +data.continuous_inputs.1.weight={hyperparams[\"w\"]}\n",
    "# Copy model trained on analyze latent to id_assoc directory\n",
    "! mkdir -p {hyperparams[\"interim_path\"]}/models\n",
    "! mv {hyperparams[\"results_path\"]}/latent_space/model.pt {hyperparams[\"interim_path\"]}/models/model_3_0.pt\n",
    "! move-dl task=random_continuous_paper_II__id_assoc_ks data=random_continuous_paper_II data.raw_data_path={hyperparams[\"data_path\"]} data.interim_data_path={hyperparams[\"interim_path\"]}  data.results_path={hyperparams[\"results_path\"]} +data.continuous_inputs.0.weight={hyperparams[\"w\"]} +data.continuous_inputs.1.weight={hyperparams[\"w\"]} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(\"./interim_data_nocorr/random.random_all_sim.Continuous_A.npy\")\n",
    "b = np.load(\"./interim_data_nocorr/random.random_all_sim.Continuous_B.npy\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(a[:,2],b[:,0])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel(\"Cont_A_3\")\n",
    "ax.set_ylabel(\"Cont_B_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"classic\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"font.family\": \"Times New Roman\",\n",
    "        \"font.size\": 14,\n",
    "        \"xtick.labelsize\": 14,\n",
    "        \"ytick.labelsize\": 14,\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"legend.fontsize\": 14,\n",
    "    }\n",
    ")\n",
    "\n",
    "DATASET = \"Continuous_B\"  # \"Continuous_A\"\n",
    "feature_list = pd.read_csv(\n",
    "    f\"./interim_data_cont_paper_II/random.random_all_sim.{DATASET}.txt\", header=None\n",
    ")\n",
    "feature_list = feature_list.values.flatten()\n",
    "\n",
    "figure_path = Path(\"./synthetic_data_nocorr/figures_synthetic_small/\")\n",
    "! mkdir -p {figure_path}\n",
    "\n",
    "latent_space = np.load(\n",
    "    \"./results_nocorr/identify_associations/latent_location.npy\"\n",
    ")\n",
    "\n",
    "# Main code:\n",
    "for i, feature in enumerate(feature_list):\n",
    "    print(i)\n",
    "    feature_values = np.load(\n",
    "        f\"./interim_data_nocorr/random.random_all_sim.{DATASET}.npy\"\n",
    "    )\n",
    "    CONDITION = \"Categorical\" in DATASET\n",
    "    if CONDITION:\n",
    "        feature_values = np.argmax(feature_values, axis=2)[:, i]\n",
    "    else:\n",
    "        feature_values = feature_values[:, i]\n",
    "\n",
    "    print(feature_values)\n",
    "    latent_space_baseline = latent_space[:, :, -1]\n",
    "    latent_space_perturbed = latent_space[:, :, i]\n",
    "\n",
    "    # # Plot latent space:\n",
    "    n_pictures = 10\n",
    "    PLOT = [0] if CONDITION else [0, 2, 3, 9]  # Plot Categorical A1\n",
    "    if i in PLOT:\n",
    "        print(i)\n",
    "        # for pic_num, (azimuth, altitude) in enumerate(zip(\n",
    "        #    np.linspace(90, 180, n_pictures), np.linspace(35, 55, n_pictures)\n",
    "        # )):\n",
    "\n",
    "        for pic_num, (azimuth, altitude) in enumerate([[120, 44]]):\n",
    "            print(pic_num, azimuth, altitude)\n",
    "            title = feature\n",
    "            fig = plot_3D_latent_and_displacement(\n",
    "                latent_space_baseline,\n",
    "                latent_space_perturbed,\n",
    "                feature_values=feature_values,\n",
    "                feature_name=f\"{title}\",\n",
    "                show_baseline=True,\n",
    "                show_perturbed=False,\n",
    "                show_arrows=False,\n",
    "                altitude=altitude,\n",
    "                azimuth=azimuth,\n",
    "            )\n",
    "            fig.savefig(figure_path / f\"3D_latent_{pic_num}_perturbed_{feature}.svg\")\n",
    "            plt.close(fig)\n",
    "\n",
    "            fig = plot_3D_latent_and_displacement(\n",
    "                latent_space_baseline,\n",
    "                latent_space_perturbed,\n",
    "                feature_values=feature_values,\n",
    "                feature_name=f\"{title}\",\n",
    "                show_baseline=False,\n",
    "                show_perturbed=False,\n",
    "                show_arrows=True,\n",
    "                altitude=altitude,\n",
    "                azimuth=azimuth,\n",
    "            )\n",
    "            fig.savefig(\n",
    "                figure_path / f\"3D_latent_{pic_num}_perturbed_{feature}_arrows_max.svg\"\n",
    "            )\n",
    "            plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "move_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "596px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
