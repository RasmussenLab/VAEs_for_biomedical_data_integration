{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MOVE EDITS <center>\n",
    "\n",
    "This document keeps track of the edits made to MOVE's main code for this specific project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Removing feature_mask\n",
    "\n",
    "When identifying associations (```src/move/tasks/identify_associations.py```), feature_masks were removed and we only kept nan_masks. \n",
    "\n",
    "> _NOTE: When extending MOVE to perform continuous perturbations, we observed that (as expected) the most significant associations between continuous variables would be each feature with itself (both continuous). Feature_masks were originally intended to mask the perturbed feature, but the desired behavior was in the end obtained by pushing self associations to the bottom of the lists, setting their scores to the minimum found in the cohort._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile identify_associations.py\n",
    "\n",
    "__all__ = [\"identify_associations\"]\n",
    "\n",
    "from functools import reduce\n",
    "from os.path import exists\n",
    "from pathlib import Path\n",
    "from typing import Literal, Sized, Union, cast\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.stats import ks_2samp, pearsonr  # type: ignore\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from move.analysis.metrics import get_2nd_order_polynomial\n",
    "from move.conf.schema import (\n",
    "    IdentifyAssociationsBayesConfig,\n",
    "    IdentifyAssociationsConfig,\n",
    "    IdentifyAssociationsKSConfig,\n",
    "    IdentifyAssociationsTTestConfig,\n",
    "    MOVEConfig,\n",
    ")\n",
    "from move.core.logging import get_logger\n",
    "from move.core.typing import BoolArray, FloatArray, IntArray\n",
    "from move.data import io\n",
    "from move.data.dataloaders import MOVEDataset, make_dataloader\n",
    "from move.data.perturbations import (\n",
    "    ContinuousPerturbationType,\n",
    "    perturb_categorical_data,\n",
    "    perturb_continuous_data_extended,\n",
    ")\n",
    "from move.data.preprocessing import one_hot_encode_single\n",
    "from move.models.vae import VAE\n",
    "from move.visualization.dataset_distributions import (\n",
    "    plot_correlations,\n",
    "    plot_cumulative_distributions,\n",
    "    plot_feature_association_graph,\n",
    "    plot_reconstruction_movement,\n",
    ")\n",
    "\n",
    "TaskType = Literal[\"bayes\", \"ttest\", \"ks\"]\n",
    "CONTINUOUS_TARGET_VALUE = [\"minimum\", \"maximum\", \"plus_std\", \"minus_std\"]\n",
    "\n",
    "\n",
    "def _get_task_type(\n",
    "    task_config: IdentifyAssociationsConfig,\n",
    ") -> TaskType:\n",
    "    task_type = OmegaConf.get_type(task_config)\n",
    "    if task_type is IdentifyAssociationsBayesConfig:\n",
    "        return \"bayes\"\n",
    "    if task_type is IdentifyAssociationsTTestConfig:\n",
    "        return \"ttest\"\n",
    "    if task_type is IdentifyAssociationsKSConfig:\n",
    "        return \"ks\"\n",
    "    raise ValueError(\"Unsupported type of task!\")\n",
    "\n",
    "\n",
    "def _validate_task_config(\n",
    "    task_config: IdentifyAssociationsConfig, task_type: TaskType\n",
    ") -> None:\n",
    "    if not (0.0 <= task_config.sig_threshold <= 1.0):\n",
    "        raise ValueError(\"Significance threshold must be within [0, 1].\")\n",
    "    if task_type == \"ttest\":\n",
    "        task_config = cast(IdentifyAssociationsTTestConfig, task_config)\n",
    "        if len(task_config.num_latent) != 4:\n",
    "            raise ValueError(\"4 latent space dimensions required.\")\n",
    "\n",
    "\n",
    "def prepare_for_categorical_perturbation(\n",
    "    config: MOVEConfig,\n",
    "    interim_path: Path,\n",
    "    baseline_dataloader: DataLoader,\n",
    "    cat_list: list[FloatArray],\n",
    ") -> tuple[\n",
    "    list[DataLoader],\n",
    "    BoolArray,\n",
    "    BoolArray,\n",
    "]:\n",
    "    \"\"\"\n",
    "    This function creates the required dataloaders and masks\n",
    "    for further categorical association analysis.\n",
    "\n",
    "    Args:\n",
    "        config: main configuration file\n",
    "        interim_path: path where the intermediate outputs are saved\n",
    "        baseline_dataloader: reference dataloader that will be perturbed\n",
    "        cat_list: list of arrays with categorical data\n",
    "\n",
    "    Returns:\n",
    "        dataloaders: all dataloaders, including baseline appended last.\n",
    "        nan_mask: mask for Nans\n",
    "        feature_mask: masks the column for the perturbed feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read original data and create perturbed datasets\n",
    "    task_config = cast(IdentifyAssociationsConfig, config.task)\n",
    "    logger = get_logger(__name__)\n",
    "\n",
    "    # Loading mappings:\n",
    "    mappings = io.load_mappings(interim_path / \"mappings.json\")\n",
    "    target_mapping = mappings[task_config.target_dataset]\n",
    "    target_value = one_hot_encode_single(target_mapping, task_config.target_value)\n",
    "    logger.debug(\n",
    "        f\"Target value: {task_config.target_value} => {target_value.astype(int)[0]}\"\n",
    "    )\n",
    "\n",
    "    dataloaders = perturb_categorical_data(\n",
    "        baseline_dataloader,\n",
    "        config.data.categorical_names,\n",
    "        task_config.target_dataset,\n",
    "        target_value,\n",
    "    )\n",
    "    dataloaders.append(baseline_dataloader)\n",
    "\n",
    "    baseline_dataset = cast(MOVEDataset, baseline_dataloader.dataset)\n",
    "\n",
    "    assert baseline_dataset.con_all is not None\n",
    "    orig_con = baseline_dataset.con_all\n",
    "    nan_mask = (orig_con == 0).numpy()  # NaN values encoded as 0s\n",
    "    logger.debug(f\"# NaN values: {np.sum(nan_mask)}/{orig_con.numel()}\")\n",
    "\n",
    "    target_dataset_idx = config.data.categorical_names.index(task_config.target_dataset)\n",
    "    target_dataset = cat_list[target_dataset_idx]\n",
    "    feature_mask = np.all(target_dataset == target_value, axis=2)  # 2D: N x P\n",
    "    feature_mask |= np.sum(target_dataset, axis=2) == 0\n",
    "\n",
    "    return (\n",
    "        dataloaders,\n",
    "        nan_mask,\n",
    "        feature_mask,\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_for_continuous_perturbation(\n",
    "    config: MOVEConfig,\n",
    "    output_subpath: Path,\n",
    "    baseline_dataloader: DataLoader,\n",
    ") -> tuple[\n",
    "    list[DataLoader],\n",
    "    BoolArray,\n",
    "    BoolArray,\n",
    "]:\n",
    "    \"\"\"\n",
    "    This function creates the required dataloaders and masks\n",
    "    for further continuous association analysis.\n",
    "\n",
    "    Args:\n",
    "        config:\n",
    "            main configuration file.\n",
    "        output_subpath:\n",
    "            path where the output plots for continuous analysis are saved.\n",
    "        baseline_dataloader:\n",
    "            reference dataloader that will be perturbed.\n",
    "\n",
    "    Returns:\n",
    "        dataloaders:\n",
    "            list with all dataloaders, including baseline appended last.\n",
    "        nan_mask:\n",
    "            mask for NaNs\n",
    "        feature_mask:\n",
    "            same as `nan_mask`, in this case.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read original data and create perturbed datasets\n",
    "    logger = get_logger(__name__)\n",
    "    task_config = cast(IdentifyAssociationsConfig, config.task)\n",
    "\n",
    "    dataloaders = perturb_continuous_data_extended(\n",
    "        baseline_dataloader,\n",
    "        config.data.continuous_names,\n",
    "        task_config.target_dataset,\n",
    "        cast(ContinuousPerturbationType, task_config.target_value),\n",
    "        output_subpath,\n",
    "    )\n",
    "    dataloaders.append(baseline_dataloader)\n",
    "\n",
    "    baseline_dataset = cast(MOVEDataset, baseline_dataloader.dataset)\n",
    "\n",
    "    assert baseline_dataset.con_all is not None\n",
    "    orig_con = baseline_dataset.con_all\n",
    "    nan_mask = (orig_con == 0).numpy()  # NaN values encoded as 0s\n",
    "    logger.debug(f\"# NaN values: {np.sum(nan_mask)}/{orig_con.numel()}\")\n",
    "    feature_mask = nan_mask\n",
    "\n",
    "    return (dataloaders, nan_mask, feature_mask)\n",
    "\n",
    "\n",
    "def _bayes_approach(\n",
    "    config: MOVEConfig,\n",
    "    task_config: IdentifyAssociationsBayesConfig,\n",
    "    train_dataloader: DataLoader,\n",
    "    baseline_dataloader: DataLoader,\n",
    "    dataloaders: list[DataLoader],\n",
    "    models_path: Path,\n",
    "    num_perturbed: int,\n",
    "    num_samples: int,\n",
    "    num_continuous: int,\n",
    "    nan_mask: BoolArray,\n",
    "    feature_mask: BoolArray,\n",
    ") -> tuple[Union[IntArray, FloatArray], ...]:\n",
    "\n",
    "    assert task_config.model is not None\n",
    "    device = torch.device(\"cuda\" if task_config.model.cuda else \"cpu\")\n",
    "\n",
    "    # Train models\n",
    "    logger = get_logger(__name__)\n",
    "    logger.info(\"Training models\")\n",
    "    mean_diff = np.zeros((num_perturbed, num_samples, num_continuous))\n",
    "    normalizer = 1 / task_config.num_refits\n",
    "\n",
    "    # Last appended dataloader is the baseline\n",
    "    baseline_dataset = cast(MOVEDataset, baseline_dataloader.dataset)\n",
    "\n",
    "    for j in range(task_config.num_refits):\n",
    "        # Initialize model\n",
    "        model: VAE = hydra.utils.instantiate(\n",
    "            task_config.model,\n",
    "            continuous_shapes=baseline_dataset.con_shapes,\n",
    "            categorical_shapes=baseline_dataset.cat_shapes,\n",
    "        )\n",
    "        if j == 0:\n",
    "            logger.debug(f\"Model: {model}\")\n",
    "\n",
    "        # Train/reload model\n",
    "        model_path = models_path / f\"model_{task_config.model.num_latent}_{j}.pt\"\n",
    "        if model_path.exists():\n",
    "            logger.debug(f\"Re-loading refit {j + 1}/{task_config.num_refits}\")\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model.to(device)\n",
    "        else:\n",
    "            logger.debug(f\"Training refit {j + 1}/{task_config.num_refits}\")\n",
    "            model.to(device)\n",
    "            hydra.utils.call(\n",
    "                task_config.training_loop,\n",
    "                model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "            )\n",
    "            if task_config.save_refits:\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "        model.eval()\n",
    "\n",
    "        # Calculate baseline reconstruction\n",
    "        _, baseline_recon = model.reconstruct(baseline_dataloader)\n",
    "        min_feat, max_feat = np.zeros((num_perturbed, num_continuous)), np.zeros(\n",
    "            (num_perturbed, num_continuous)\n",
    "        )\n",
    "        min_baseline, max_baseline = np.min(baseline_recon, axis=0), np.max(\n",
    "            baseline_recon, axis=0\n",
    "        )\n",
    "\n",
    "        # Calculate perturb reconstruction => keep track of mean difference\n",
    "        for i in range(num_perturbed):\n",
    "            _, perturb_recon = model.reconstruct(dataloaders[i])\n",
    "            diff = perturb_recon - baseline_recon  # 2D: N x C\n",
    "            mean_diff[i, :, :] += diff * normalizer\n",
    "\n",
    "            min_perturb, max_perturb = np.min(perturb_recon, axis=0), np.max(\n",
    "                perturb_recon, axis=0\n",
    "            )\n",
    "            min_feat[i, :], max_feat[i, :] = np.min(\n",
    "                [min_baseline, min_perturb], axis=0\n",
    "            ), np.max([max_baseline, max_perturb], axis=0)\n",
    "\n",
    "    # Calculate Bayes factors\n",
    "    logger.info(\"Identifying significant features\")\n",
    "    bayes_k = np.empty((num_perturbed, num_continuous))\n",
    "    bayes_mask = np.zeros(np.shape(bayes_k))\n",
    "    for i in range(num_perturbed):\n",
    "        mask = nan_mask  # 2D: N x C\n",
    "        diff = np.ma.masked_array(mean_diff[i, :, :], mask=mask)  # 2D: N x C\n",
    "        prob = np.ma.compressed(np.mean(diff > 1e-8, axis=0))  # 1D: C\n",
    "        bayes_k[i, :] = np.log(prob + 1e-8) - np.log(1 - prob + 1e-8)\n",
    "        if task_config.target_value in CONTINUOUS_TARGET_VALUE:\n",
    "            bayes_mask[i, :] = (\n",
    "                baseline_dataloader.dataset.con_all[0, :]\n",
    "                - dataloaders[i].dataset.con_all[0, :]\n",
    "            )\n",
    "\n",
    "    bayes_mask[bayes_mask != 0] = 1\n",
    "    bayes_mask = np.array(bayes_mask, dtype=bool)\n",
    "\n",
    "    # Calculate Bayes probabilities \n",
    "    bayes_abs = np.abs(bayes_k)\n",
    "    bayes_p = np.exp(bayes_abs) / (1 + np.exp(bayes_abs))  # 2D: N x C\n",
    "    bayes_abs[bayes_mask] = np.min(\n",
    "        bayes_abs\n",
    "    )  # Bring feature_i feature_i associations to minimum\n",
    "    sort_ids = np.argsort(bayes_abs, axis=None)[::-1]  # 1D: N x C\n",
    "    prob = np.take(bayes_p, sort_ids)  # 1D: N x C\n",
    "    logger.debug(f\"Bayes proba range: [{prob[-1]:.3f} {prob[0]:.3f}]\")\n",
    "\n",
    "    # Sort Bayes\n",
    "    bayes_k = np.take(bayes_k, sort_ids)  # 1D: N x C\n",
    "\n",
    "    # Calculate FDR\n",
    "    fdr = np.cumsum(1 - prob) / np.arange(1, prob.size + 1)  # 1D\n",
    "    idx = np.argmin(np.abs(fdr - task_config.sig_threshold))\n",
    "    logger.debug(f\"FDR range: [{fdr[0]:.3f} {fdr[-1]:.3f}]\")\n",
    "\n",
    "    return sort_ids[:idx], prob[:idx], fdr[:idx], bayes_k[:idx]\n",
    "\n",
    "\n",
    "def _ttest_approach(\n",
    "    task_config: IdentifyAssociationsTTestConfig,\n",
    "    train_dataloader: DataLoader,\n",
    "    baseline_dataloader: DataLoader,\n",
    "    dataloaders: list[DataLoader],\n",
    "    models_path: Path,\n",
    "    interim_path: Path,\n",
    "    num_perturbed: int,\n",
    "    num_samples: int,\n",
    "    num_continuous: int,\n",
    "    nan_mask: BoolArray,\n",
    "    feature_mask: BoolArray,\n",
    ") -> tuple[Union[IntArray, FloatArray], ...]:\n",
    "\n",
    "    from scipy.stats import ttest_rel\n",
    "\n",
    "    assert task_config.model is not None\n",
    "    device = torch.device(\"cuda\" if task_config.model.cuda else \"cpu\")\n",
    "\n",
    "    # Train models\n",
    "    logger = get_logger(__name__)\n",
    "    logger.info(\"Training models\")\n",
    "    pvalues = np.empty(\n",
    "        (\n",
    "            len(task_config.num_latent),\n",
    "            task_config.num_refits,\n",
    "            num_perturbed,\n",
    "            num_continuous,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Last appended dataloader is the baseline\n",
    "    baseline_dataset = cast(MOVEDataset, baseline_dataloader.dataset)\n",
    "\n",
    "    for k, num_latent in enumerate(task_config.num_latent):\n",
    "        for j in range(task_config.num_refits):\n",
    "\n",
    "            # Initialize model\n",
    "            model: VAE = hydra.utils.instantiate(\n",
    "                task_config.model,\n",
    "                continuous_shapes=baseline_dataset.con_shapes,\n",
    "                categorical_shapes=baseline_dataset.cat_shapes,\n",
    "                num_latent=num_latent,\n",
    "            )\n",
    "            if j == 0:\n",
    "                logger.debug(f\"Model: {model}\")\n",
    "\n",
    "            # Train model\n",
    "            model_path = models_path / f\"model_{num_latent}_{j}.pt\"\n",
    "            if model_path.exists():\n",
    "                logger.debug(f\"Re-loading refit {j + 1}/{task_config.num_refits}\")\n",
    "                model.load_state_dict(torch.load(model_path))\n",
    "                model.to(device)\n",
    "            else:\n",
    "                logger.debug(f\"Training refit {j + 1}/{task_config.num_refits}\")\n",
    "                model.to(device)\n",
    "                hydra.utils.call(\n",
    "                    task_config.training_loop,\n",
    "                    model=model,\n",
    "                    train_dataloader=train_dataloader,\n",
    "                )\n",
    "                if task_config.save_refits:\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "            model.eval()\n",
    "\n",
    "            # Get baseline reconstruction and baseline difference\n",
    "            _, baseline_recon = model.reconstruct(baseline_dataloader)\n",
    "            baseline_diff = np.empty((10, num_samples, num_continuous))\n",
    "            for i in range(10):\n",
    "                _, recon = model.reconstruct(baseline_dataloader)\n",
    "                baseline_diff[i, :, :] = recon - baseline_recon\n",
    "            baseline_diff = np.mean(baseline_diff, axis=0)  # 2D: N x C\n",
    "            baseline_diff = np.where(nan_mask, np.nan, baseline_diff)\n",
    "\n",
    "            # T-test between baseline and perturb difference\n",
    "            for i in range(num_perturbed):\n",
    "                _, perturb_recon = model.reconstruct(dataloaders[i])\n",
    "                perturb_diff = perturb_recon - baseline_recon\n",
    "                mask = feature_mask[:, [i]] | nan_mask  # 2D: N x C\n",
    "                _, pvalues[k, j, i, :] = ttest_rel(\n",
    "                    a=np.where(mask, np.nan, perturb_diff),\n",
    "                    b=np.where(mask, np.nan, baseline_diff),\n",
    "                    axis=0,\n",
    "                    nan_policy=\"omit\",\n",
    "                )\n",
    "\n",
    "    # Correct p-values (Bonferroni)\n",
    "    pvalues = np.minimum(pvalues * num_continuous, 1.0)\n",
    "    np.save(interim_path / \"pvals.npy\", pvalues)\n",
    "\n",
    "    # Find significant hits\n",
    "    overlap_thres = task_config.num_refits // 2\n",
    "    reject = pvalues <= task_config.sig_threshold  # 4D: L x R x P x C\n",
    "    overlap = reject.sum(axis=1) >= overlap_thres  # 3D: L x P x C\n",
    "    sig_ids = overlap.sum(axis=0) >= 3  # 2D: P x C\n",
    "    sig_ids = np.flatnonzero(sig_ids)  # 1D\n",
    "\n",
    "    # Report median p-value\n",
    "    masked_pvalues = np.ma.masked_array(pvalues, mask=~reject)  # 4D\n",
    "    masked_pvalues = np.ma.median(masked_pvalues, axis=1)  # 3D\n",
    "    masked_pvalues = np.ma.median(masked_pvalues, axis=0)  # 2D\n",
    "    sig_pvalues = np.ma.compressed(np.take(masked_pvalues, sig_ids))  # 1D\n",
    "\n",
    "    return sig_ids, sig_pvalues\n",
    "\n",
    "\n",
    "def _ks_approach(\n",
    "    config: MOVEConfig,\n",
    "    task_config: IdentifyAssociationsKSConfig,\n",
    "    train_dataloader: DataLoader,\n",
    "    baseline_dataloader: DataLoader,\n",
    "    dataloaders: list[DataLoader],\n",
    "    models_path: Path,\n",
    "    num_perturbed: int,\n",
    "    num_samples: int,\n",
    "    num_continuous: int,\n",
    "    con_names: list[list[str]],\n",
    "    output_path: Path,\n",
    ") -> tuple[Union[IntArray, FloatArray], ...]:\n",
    "    \"\"\"\n",
    "    Find associations between continuous features using Kolmogorov-Smirnov distances.\n",
    "    When perturbing feature A, this function measures the shift of the reconstructed\n",
    "    distribution for feature B (over samples) from 1) the baseline reconstruction to 2)\n",
    "    the reconstruction when perturbing A.\n",
    "\n",
    "    If A and B are related the perturbation of A in the input will lead to a change in\n",
    "    feature B's reconstruction, that will be measured by KS distance.\n",
    "\n",
    "    Associations are then ranked according to KS distance (absolute value).\n",
    "\n",
    "\n",
    "    Args:\n",
    "        config: MOVE main configuration.\n",
    "        task_config: IdentifyAssociationsKSConfig configuration.\n",
    "        train_dataloader: training DataLoader.\n",
    "        baseline_dataloader: unperturbed DataLoader.\n",
    "        dataloaders: list of DataLoaders where DataLoader[i] is obtained by perturbing\n",
    "                     feature i in the target dataset.\n",
    "        models_path: path to the models.\n",
    "        num_perturbed: number of perturbed features.\n",
    "        num_samples: total number of samples\n",
    "        num_continuous: number of continuous features\n",
    "                        (all continuous datasets concatenated).\n",
    "        con_names: list of lists where eah inner list\n",
    "                   contains the feature names of a specific continuous dataset\n",
    "        output_path: path where QC summary metrics will be saved.\n",
    "\n",
    "    Returns:\n",
    "        sort_ids: list with flattened IDs of the associations\n",
    "                  above the significance threshold.\n",
    "        ks_distance: Ordered list with signed KS scores. KS scores quantify the\n",
    "                    direction and magnitude of the shift in feature B's reconstruction\n",
    "                    when perturbing feature A.\n",
    "\n",
    "\n",
    "    !!! Note !!!:\n",
    "\n",
    "    The sign of the KS score can be misleading: negative sign means positive shift.\n",
    "    since the cumulative distribution starts growing later and is found below\n",
    "    the reference (baseline). Hence:\n",
    "    a) with plus_std, negative sign means a positive correlation.\n",
    "    b) with minus_std, negative sign means a negative correlation.\n",
    "    \"\"\"\n",
    "\n",
    "    assert task_config.model is not None\n",
    "    device = torch.device(\"cuda\" if task_config.model.cuda else \"cpu\")\n",
    "    figure_path = output_path / \"figures\"\n",
    "    figure_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Data containers\n",
    "    stats = np.empty((task_config.num_refits, num_perturbed, num_continuous))\n",
    "    stat_signs = np.empty_like(stats)\n",
    "    rec_corr, slope = np.empty((task_config.num_refits, num_continuous)), np.empty(\n",
    "        (task_config.num_refits, num_continuous)\n",
    "    )\n",
    "    ks_mask = np.zeros((num_perturbed, num_continuous))\n",
    "    latent_matrix = np.empty(\n",
    "        (num_samples, task_config.model.num_latent, len(dataloaders))\n",
    "    )\n",
    "\n",
    "    # Last appended dataloader is the baseline\n",
    "    baseline_dataset = cast(MOVEDataset, baseline_dataloader.dataset)\n",
    "\n",
    "    # Train models\n",
    "    logger = get_logger(__name__)\n",
    "    logger.info(\"Training models\")\n",
    "\n",
    "    target_dataset_idx = config.data.continuous_names.index(task_config.target_dataset)\n",
    "    perturbed_names = con_names[target_dataset_idx]\n",
    "\n",
    "    for j in range(task_config.num_refits):  # Train num_refits models\n",
    "\n",
    "        # Initialize model\n",
    "        model: VAE = hydra.utils.instantiate(\n",
    "            task_config.model,\n",
    "            continuous_shapes=baseline_dataset.con_shapes,\n",
    "            categorical_shapes=baseline_dataset.cat_shapes,\n",
    "        )\n",
    "        if j == 0:\n",
    "            logger.debug(f\"Model: {model}\")\n",
    "\n",
    "        # Train/reload model\n",
    "        model_path = models_path / f\"model_{task_config.model.num_latent}_{j}.pt\"\n",
    "        if model_path.exists():\n",
    "            logger.debug(f\"Re-loading refit {j + 1}/{task_config.num_refits}\")\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model.to(device)\n",
    "        else:\n",
    "            logger.debug(f\"Training refit {j + 1}/{task_config.num_refits}\")\n",
    "            model.to(device)\n",
    "            hydra.utils.call(\n",
    "                task_config.training_loop,\n",
    "                model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "            )\n",
    "            if task_config.save_refits:\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "        model.eval()\n",
    "\n",
    "        # Calculate baseline reconstruction\n",
    "        _, baseline_recon = model.reconstruct(baseline_dataloader)\n",
    "        min_feat = np.zeros((num_perturbed, num_continuous))\n",
    "        max_feat = np.zeros((num_perturbed, num_continuous))\n",
    "        min_baseline = np.min(baseline_recon, axis=0)\n",
    "        max_baseline = np.max(baseline_recon, axis=0)\n",
    "\n",
    "        # QC of feature's reconstruction ##############################\n",
    "        logger.debug(\"Calculating quality control of the feature reconstructions\")\n",
    "        # Correlation and slope for each feature's reconstruction\n",
    "        feature_names = reduce(list.__add__, con_names)\n",
    "\n",
    "        for k in range(num_continuous):\n",
    "            x = baseline_dataloader.dataset.con_all.numpy()[:, k]  # baseline_recon[:,i]\n",
    "            y = baseline_recon[:, k]\n",
    "            x_pol, y_pol, (a2, a1, a) = get_2nd_order_polynomial(x, y)\n",
    "            slope[j, k] = a1\n",
    "            rec_corr[j, k] = pearsonr(x, y).statistic\n",
    "\n",
    "            if (\n",
    "                feature_names[k] in task_config.perturbed_feature_names\n",
    "                or feature_names[k] in task_config.target_feature_names\n",
    "            ):\n",
    "\n",
    "                # Plot correlations\n",
    "                fig = plot_correlations(x, y, x_pol, y_pol, a2, a1, a, k)\n",
    "                fig.savefig(\n",
    "                    figure_path\n",
    "                    / f\"Input_vs_reconstruction_correlation_feature_{k}_refit_{j}.png\",\n",
    "                    dpi=200,\n",
    "                )\n",
    "\n",
    "        # Calculate perturbed reconstruction and shifts #############################\n",
    "        logger.debug(\"Computing KS scores\")\n",
    "\n",
    "        # Save original latent space for first refit:\n",
    "        if j == 0:\n",
    "            latent = model.project(baseline_dataloader)\n",
    "            latent_matrix[:, :, -1] = latent\n",
    "\n",
    "        for i, pert_feat in enumerate(perturbed_names):\n",
    "            _, perturb_recon = model.reconstruct(dataloaders[i])\n",
    "            min_perturb = np.min(perturb_recon, axis=0)\n",
    "            max_perturb = np.max(perturb_recon, axis=0)\n",
    "            min_feat[i, :] = np.min([min_baseline, min_perturb], axis=0)\n",
    "            max_feat[i, :] = np.max([max_baseline, max_perturb], axis=0)\n",
    "\n",
    "            # Save latent representation for perturbed samples\n",
    "            if j == 0:\n",
    "                latent_pert = model.project(dataloaders[i])\n",
    "                latent_matrix[:, :, i] = latent_pert\n",
    "\n",
    "            for k, targ_feat in enumerate(feature_names):\n",
    "                # Calculate ks factors: measure distance between baseline and perturbed\n",
    "                # reconstruction distributions per feature (k)\n",
    "                res = ks_2samp(perturb_recon[:, k], baseline_recon[:, k])\n",
    "                stats[j, i, k] = res.statistic\n",
    "                stat_signs[j, i, k] = res.statistic_sign\n",
    "\n",
    "                if (\n",
    "                    pert_feat in task_config.perturbed_feature_names\n",
    "                    and targ_feat in task_config.target_feature_names\n",
    "                ):\n",
    "\n",
    "                    # Plotting preliminary results:\n",
    "                    n_bins = 50\n",
    "                    hist_base, edges = np.histogram(\n",
    "                        baseline_recon[:, k],\n",
    "                        bins=np.linspace(min_feat[i, k], max_feat[i, k], n_bins),\n",
    "                        density=True,\n",
    "                    )\n",
    "                    hist_pert, edges = np.histogram(\n",
    "                        perturb_recon[:, k],\n",
    "                        bins=np.linspace(min_feat[i, k], max_feat[i, k], n_bins),\n",
    "                        density=True,\n",
    "                    )\n",
    "\n",
    "                    # Cumulative distribution:\n",
    "                    fig = plot_cumulative_distributions(\n",
    "                        edges,\n",
    "                        hist_base,\n",
    "                        hist_pert,\n",
    "                        title=f\"Cumulative_perturbed_{i}_measuring_\"\n",
    "                        f\"{k}_stats_{stats[j, i, k]}\",\n",
    "                    )\n",
    "                    fig.savefig(\n",
    "                        figure_path\n",
    "                        / (\n",
    "                            f\"Cumulative_refit_{j}_perturbed_{i}_\"\n",
    "                            f\"measuring_{k}_stats_{stats[j, i, k]}.png\"\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # Feature changes:\n",
    "                    fig = plot_reconstruction_movement(baseline_recon, perturb_recon, k)\n",
    "                    fig.savefig(\n",
    "                        figure_path / f\"Changes_pert_{i}_on_feat_{k}_refit_{j}.png\"\n",
    "                    )\n",
    "\n",
    "    # Save latent space matrix:\n",
    "    np.save(output_path / \"latent_location.npy\", latent_matrix)\n",
    "    np.save(output_path / \"perturbed_features_list.npy\", np.array(perturbed_names))\n",
    "\n",
    "    # Creating a mask for self associations\n",
    "    logger.debug(\"Creating self-association mask\")\n",
    "    for i in range(num_perturbed):\n",
    "        if task_config.target_value in CONTINUOUS_TARGET_VALUE:\n",
    "            ks_mask[i, :] = (\n",
    "                baseline_dataloader.dataset.con_all[0, :]\n",
    "                - dataloaders[i].dataset.con_all[0, :]\n",
    "            )\n",
    "    ks_mask[ks_mask != 0] = 1\n",
    "    ks_mask = np.array(ks_mask, dtype=bool)\n",
    "\n",
    "    # Take the median of KS values (with sign) over refits.\n",
    "    final_stats = np.nanmedian(stats * stat_signs, axis=0)\n",
    "    final_stats[ks_mask] = (\n",
    "        0.0  # Zero all masked values, placing them at end of the ranking\n",
    "    )\n",
    "    # Write max value to check scale of variation induced by the perturbations.\n",
    "    max_stats, max_abs_filtered  = (np.max(stats), np.max(abs(final_stats)))\n",
    "    logger.debug(f\"Max KS value:{max_stats}. Max after filtering:{max_abs_filtered}\")\n",
    "\n",
    "    # KS-threshold:\n",
    "    ks_thr = np.sqrt(-np.log(task_config.sig_threshold / 2) * 1 / (num_samples))\n",
    "    logger.info(f\"Suggested absolute KS threshold is: {ks_thr}\")\n",
    "\n",
    "    # Sort associations by absolute KS value\n",
    "    sort_ids = np.argsort(abs(final_stats), axis=None)[::-1]  # 1D: N x C\n",
    "    ks_distance = np.take(final_stats, sort_ids)  # 1D: N x C\n",
    "\n",
    "    # Writing Quality control csv file.\n",
    "    # Mean slope and correlation over refits as qc metrics.\n",
    "    logger.info(\"Writing QC file\")\n",
    "    qc_df = pd.DataFrame({\"Feature names\": feature_names})\n",
    "    qc_df[\"slope\"] = np.nanmean(slope, axis=0)\n",
    "    qc_df[\"reconstruction_correlation\"] = np.nanmean(rec_corr, axis=0)\n",
    "    qc_df.to_csv(output_path / \"QC_summary_KS.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "    # Return first idx associations: redefined for reasonable threshold\n",
    "\n",
    "    return sort_ids[abs(ks_distance) >= ks_thr], ks_distance[abs(ks_distance) >= ks_thr]\n",
    "\n",
    "\n",
    "def save_results(\n",
    "    config: MOVEConfig,\n",
    "    con_shapes: list[int],\n",
    "    cat_names: list[list[str]],\n",
    "    con_names: list[list[str]],\n",
    "    output_path: Path,\n",
    "    sig_ids,\n",
    "    extra_cols,\n",
    "    extra_colnames,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    This function saves the obtained associations in a TSV file containing\n",
    "    the following columns:\n",
    "        feature_a_id\n",
    "        feature_b_id\n",
    "        feature_a_name\n",
    "        feature_b_name\n",
    "        feature_b_dataset\n",
    "        proba/p_value: number quantifying the significance of the association\n",
    "\n",
    "    Args:\n",
    "        config: main config\n",
    "        con_shapes: tuple with the number of features per continuous dataset\n",
    "        cat_names: list of lists of names for the categorical features.\n",
    "                   Each inner list corresponds to a separate dataset.\n",
    "        con_names: list of lists of names for the continuous features.\n",
    "                   Each inner list corresponds to a separate dataset.\n",
    "        output_path: path where the results will be saved\n",
    "        sig_ids: ids for the significat features\n",
    "        extra_cols: extra data when calling the approach function\n",
    "        extra_colnames: names for the extra data columns\n",
    "    \"\"\"\n",
    "    logger = get_logger(__name__)\n",
    "    logger.info(f\"Significant hits found: {sig_ids.size}\")\n",
    "    task_config = cast(IdentifyAssociationsConfig, config.task)\n",
    "    task_type = _get_task_type(task_config)\n",
    "\n",
    "    num_continuous = sum(con_shapes)  # C\n",
    "\n",
    "    if sig_ids.size > 0:\n",
    "        sig_ids = np.vstack((sig_ids // num_continuous, sig_ids % num_continuous)).T\n",
    "        logger.info(\"Writing results\")\n",
    "        results = pd.DataFrame(sig_ids, columns=[\"feature_a_id\", \"feature_b_id\"])\n",
    "\n",
    "        # Check if the task is for continuous or categorical data\n",
    "        if task_config.target_value in CONTINUOUS_TARGET_VALUE:\n",
    "            target_dataset_idx = config.data.continuous_names.index(\n",
    "                task_config.target_dataset\n",
    "            )\n",
    "            a_df = pd.DataFrame(dict(feature_a_name=con_names[target_dataset_idx]))\n",
    "        else:\n",
    "            target_dataset_idx = config.data.categorical_names.index(\n",
    "                task_config.target_dataset\n",
    "            )\n",
    "            a_df = pd.DataFrame(dict(feature_a_name=cat_names[target_dataset_idx]))\n",
    "        a_df.index.name = \"feature_a_id\"\n",
    "        a_df.reset_index(inplace=True)\n",
    "        feature_names = reduce(list.__add__, con_names)\n",
    "        b_df = pd.DataFrame(dict(feature_b_name=feature_names))\n",
    "        b_df.index.name = \"feature_b_id\"\n",
    "        b_df.reset_index(inplace=True)\n",
    "        results = results.merge(a_df, on=\"feature_a_id\", how=\"left\").merge(\n",
    "            b_df, on=\"feature_b_id\", how=\"left\"\n",
    "        )\n",
    "        results[\"feature_b_dataset\"] = pd.cut(\n",
    "            results[\"feature_b_id\"],\n",
    "            bins=cast(list[int], np.cumsum([0] + con_shapes)),\n",
    "            right=False,\n",
    "            labels=config.data.continuous_names,\n",
    "        )\n",
    "        for col, colname in zip(extra_cols, extra_colnames):\n",
    "            results[colname] = col\n",
    "        results.to_csv(\n",
    "            output_path / f\"results_sig_assoc_{task_type}.tsv\", sep=\"\\t\", index=False\n",
    "        )\n",
    "\n",
    "\n",
    "def identify_associations(config: MOVEConfig) -> None:\n",
    "    \"\"\"\n",
    "    Leads to the execution of the appropriate association\n",
    "    identification tasks. The function is organized in three\n",
    "    blocks:\n",
    "        1) Prepare the data and create the dataloaders with their masks.\n",
    "        2) Evaluate associations using bayes or ttest approach.\n",
    "        3) Save results.\n",
    "    \"\"\"\n",
    "    # DATA PREPARATION ######################\n",
    "    # Read original data and create perturbed datasets####\n",
    "\n",
    "    logger = get_logger(__name__)\n",
    "    task_config = cast(IdentifyAssociationsConfig, config.task)\n",
    "    task_type = _get_task_type(task_config)\n",
    "    _validate_task_config(task_config, task_type)\n",
    "\n",
    "    interim_path = Path(config.data.interim_data_path)\n",
    "\n",
    "    models_path = interim_path / \"models\"\n",
    "    if task_config.save_refits:\n",
    "        models_path.mkdir(exist_ok=True)\n",
    "\n",
    "    output_path = Path(config.data.results_path) / \"identify_associations\"\n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Load datasets:\n",
    "    cat_list, cat_names, con_list, con_names = io.load_preprocessed_data(\n",
    "        interim_path,\n",
    "        config.data.categorical_names,\n",
    "        config.data.continuous_names,\n",
    "    )\n",
    "\n",
    "    train_dataloader = make_dataloader(\n",
    "        cat_list,\n",
    "        con_list,\n",
    "        shuffle=True,\n",
    "        batch_size=task_config.batch_size,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    con_shapes = [con.shape[1] for con in con_list]\n",
    "\n",
    "    num_samples = len(cast(Sized, train_dataloader.sampler))  # N\n",
    "    num_continuous = sum(con_shapes)  # C\n",
    "    logger.debug(f\"# continuous features: {num_continuous}\")\n",
    "\n",
    "    # Creating the baseline dataloader:\n",
    "    baseline_dataloader = make_dataloader(\n",
    "        cat_list, con_list, shuffle=False, batch_size=task_config.batch_size\n",
    "    )\n",
    "\n",
    "    # Indentify associations between continuous features:\n",
    "    logger.info(f\"Perturbing dataset: '{task_config.target_dataset}'\")\n",
    "    if task_config.target_value in CONTINUOUS_TARGET_VALUE:\n",
    "        logger.info(f\"Beginning task: identify associations continuous ({task_type})\")\n",
    "        logger.info(f\"Perturbation type: {task_config.target_value}\")\n",
    "        output_subpath = Path(output_path) / \"perturbation_visualization\"\n",
    "        output_subpath.mkdir(exist_ok=True, parents=True)\n",
    "        (\n",
    "            dataloaders,\n",
    "            nan_mask,\n",
    "            feature_mask,\n",
    "        ) = prepare_for_continuous_perturbation(\n",
    "            config, output_subpath, baseline_dataloader\n",
    "        )\n",
    "\n",
    "    # Identify associations between categorical and continuous features:\n",
    "    else:\n",
    "        logger.info(\"Beginning task: identify associations categorical\")\n",
    "        (\n",
    "            dataloaders,\n",
    "            nan_mask,\n",
    "            feature_mask,\n",
    "        ) = prepare_for_categorical_perturbation(\n",
    "            config, interim_path, baseline_dataloader, cat_list\n",
    "        )\n",
    "\n",
    "    num_perturbed = len(dataloaders) - 1  # P\n",
    "    logger.debug(f\"# perturbed features: {num_perturbed}\")\n",
    "\n",
    "    # APPROACH EVALUATION ##########################\n",
    "\n",
    "    if task_type == \"bayes\":\n",
    "        task_config = cast(IdentifyAssociationsBayesConfig, task_config)\n",
    "        sig_ids, *extra_cols = _bayes_approach(\n",
    "            config,\n",
    "            task_config,\n",
    "            train_dataloader,\n",
    "            baseline_dataloader,\n",
    "            dataloaders,\n",
    "            models_path,\n",
    "            num_perturbed,\n",
    "            num_samples,\n",
    "            num_continuous,\n",
    "            nan_mask,\n",
    "            feature_mask,\n",
    "        )\n",
    "\n",
    "        extra_colnames = [\"proba\", \"fdr\", \"bayes_k\"]\n",
    "\n",
    "    elif task_type == \"ttest\":\n",
    "        task_config = cast(IdentifyAssociationsTTestConfig, task_config)\n",
    "        sig_ids, *extra_cols = _ttest_approach(\n",
    "            task_config,\n",
    "            train_dataloader,\n",
    "            baseline_dataloader,\n",
    "            dataloaders,\n",
    "            models_path,\n",
    "            interim_path,\n",
    "            num_perturbed,\n",
    "            num_samples,\n",
    "            num_continuous,\n",
    "            nan_mask,\n",
    "            feature_mask,\n",
    "        )\n",
    "\n",
    "        extra_colnames = [\"p_value\"]\n",
    "\n",
    "    elif task_type == \"ks\":\n",
    "        task_config = cast(IdentifyAssociationsKSConfig, task_config)\n",
    "        sig_ids, *extra_cols = _ks_approach(\n",
    "            config,\n",
    "            task_config,\n",
    "            train_dataloader,\n",
    "            baseline_dataloader,\n",
    "            dataloaders,\n",
    "            models_path,\n",
    "            num_perturbed,\n",
    "            num_samples,\n",
    "            num_continuous,\n",
    "            con_names,\n",
    "            output_path,\n",
    "        )\n",
    "\n",
    "        extra_colnames = [\"ks_distance\"]\n",
    "\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "    # RESULTS ################################\n",
    "    save_results(\n",
    "        config,\n",
    "        con_shapes,\n",
    "        cat_names,\n",
    "        con_names,\n",
    "        output_path,\n",
    "        sig_ids,\n",
    "        extra_cols,\n",
    "        extra_colnames,\n",
    "    )\n",
    "\n",
    "    if exists(output_path / f\"results_sig_assoc_{task_type}.tsv\"):\n",
    "        association_df = pd.read_csv(\n",
    "            output_path / f\"results_sig_assoc_{task_type}.tsv\", sep=\"\\t\"\n",
    "        )\n",
    "        _ = plot_feature_association_graph(association_df, output_path)\n",
    "        _ = plot_feature_association_graph(association_df, output_path, layout=\"spring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Corrected Cumulative distributions\n",
    "\n",
    "Cumulative distributions obtained when using the KS method were not properly normalyzed. We now multiply by bin_width to obtain a distribution between 0-1. The edited function,  ```plot_cumulative```, is to be found in ```src/move/visualization/dataset_distributions.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset_distributions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset_distributions.py\n",
    "\n",
    "__all__ = [\"plot_value_distributions\"]\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.figure\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from move.core.typing import FloatArray\n",
    "from move.visualization.style import (\n",
    "    DEFAULT_DIVERGING_PALETTE,\n",
    "    DEFAULT_PLOT_STYLE,\n",
    "    style_settings,\n",
    ")\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'Latin Modern Roman'\n",
    "\n",
    "def plot_value_distributions(\n",
    "    feature_values: FloatArray,\n",
    "    style: str = \"fast\",\n",
    "    nbins: int = 100,\n",
    "    colormap: str = DEFAULT_DIVERGING_PALETTE,\n",
    ") -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Given a certain dataset, plot its distribution of values.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        feature_values:\n",
    "            Values of the features, a 2D array (`num_samples` x `num_features`).\n",
    "        style:\n",
    "            Name of style to apply to the plot.\n",
    "        colormap:\n",
    "            Name of colormap to apply to the colorbar.\n",
    "\n",
    "    Returns:\n",
    "        Figure\n",
    "    \"\"\"\n",
    "    vmin, vmax = np.nanmin(feature_values), np.nanmax(feature_values)\n",
    "    with style_settings(style):\n",
    "        fig = plt.figure(layout=\"constrained\", figsize=(7, 7))\n",
    "        ax = fig.add_subplot(projection=\"3d\")\n",
    "        x_val = np.linspace(vmin, vmax, nbins)\n",
    "        y_val = np.arange(np.shape(feature_values)[1])\n",
    "        x_val, y_val = np.meshgrid(x_val, y_val)\n",
    "\n",
    "        histogram = []\n",
    "        for i in range(np.shape(feature_values)[1]):\n",
    "            feat_i_list = feature_values[:, i]\n",
    "            feat_hist, feat_bin_edges = np.histogram(\n",
    "                feat_i_list, bins=nbins, range=(vmin, vmax)\n",
    "            )\n",
    "            histogram.append(feat_hist)\n",
    "\n",
    "        ax.plot_surface(x_val, y_val, np.array(histogram), cmap=colormap)\n",
    "        ax.set_xlabel(\"Feature value\")\n",
    "        ax.set_ylabel(\"Feature ID number\")\n",
    "        ax.set_zlabel(\"Frequency\")\n",
    "        # ax.legend()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_reconstruction_diff(\n",
    "    diff_array: FloatArray,\n",
    "    vmin=None,\n",
    "    vmax=None,\n",
    "    style: str = DEFAULT_PLOT_STYLE,\n",
    "    colormap: str = DEFAULT_DIVERGING_PALETTE,\n",
    ") -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Plot the reconstruction differences as a heatmap.\n",
    "    \"\"\"\n",
    "    with style_settings(style):\n",
    "        if vmin is None:\n",
    "            vmin = np.min(diff_array)\n",
    "        elif vmax is None:\n",
    "            vmax = np.max(diff_array)\n",
    "        fig = plt.figure(layout=\"constrained\", figsize=(7, 7))\n",
    "        plt.imshow(diff_array, cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "        plt.xlabel(\"Feature\")\n",
    "        plt.ylabel(\"Sample\")\n",
    "        plt.colorbar()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_feature_association_graph(\n",
    "    association_df, output_path, layout=\"circular\", style: str = DEFAULT_PLOT_STYLE\n",
    ") -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    This function plots a graph where each node corresponds to a feature and the edges\n",
    "    represent the associations between features. Edge width represents the probability\n",
    "    of said association, not the association's effect size.\n",
    "\n",
    "    Input:\n",
    "        association_df: pandas dataframe containing the following columns:\n",
    "                            - feature_a: source node\n",
    "                            - feature_b: target node\n",
    "                            - p_value/bayes_score: edge weight\n",
    "        output_path: Path object where the picture will be stored.\n",
    "\n",
    "    Output:\n",
    "        Feature_association_graph.png: picture of the graph\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if \"p_value\" in association_df.columns:\n",
    "        association_df[\"weight\"] = 1 - association_df[\"p_value\"]\n",
    "\n",
    "    elif \"proba\" in association_df.columns:\n",
    "        association_df[\"weight\"] = association_df[\"proba\"]\n",
    "\n",
    "    elif \"ks_distance\" in association_df.columns:\n",
    "        association_df[\"weight\"] = association_df[\"ks_distance\"]\n",
    "\n",
    "    with style_settings(style):\n",
    "        fig = plt.figure(figsize=(45, 45))\n",
    "        G = nx.from_pandas_edgelist(\n",
    "            association_df,\n",
    "            source=\"feature_a_name\",\n",
    "            target=\"feature_b_name\",\n",
    "            edge_attr=\"weight\",\n",
    "        )\n",
    "\n",
    "        nodes = list(G.nodes)\n",
    "\n",
    "        datasets = association_df[\"feature_b_dataset\"].unique()\n",
    "        color_map = {\n",
    "            dataset: (np.random.uniform(), np.random.uniform(), np.random.uniform())\n",
    "            for dataset in datasets\n",
    "        }\n",
    "        node_dataset_map = {\n",
    "            target_feature: dataset\n",
    "            for (target_feature, dataset) in zip(\n",
    "                association_df[\"feature_b_name\"], association_df[\"feature_b_dataset\"]\n",
    "            )\n",
    "        }\n",
    "\n",
    "        if layout == \"spring\":\n",
    "            pos = nx.spring_layout(G)\n",
    "            with_labels = True\n",
    "        elif layout == \"circular\":\n",
    "            pos = nx.circular_layout(G)\n",
    "            _ = [\n",
    "                plt.text(\n",
    "                    pos[node][0],\n",
    "                    pos[node][1],\n",
    "                    nodes[i],\n",
    "                    rotation=(i / float(len(nodes))) * 360,\n",
    "                    fontsize=10,\n",
    "                    horizontalalignment=\"center\",\n",
    "                    verticalalignment=\"center\",\n",
    "                )\n",
    "                for i, node in enumerate(nodes)\n",
    "            ]\n",
    "            with_labels = False\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Graph layout (layout argument) must be either 'circular' or 'spring'.\"\n",
    "            )\n",
    "\n",
    "        nx.draw(\n",
    "            G,\n",
    "            pos=pos,\n",
    "            with_labels=with_labels,\n",
    "            node_size=2000,\n",
    "            node_color=[\n",
    "                (\n",
    "                    color_map[node_dataset_map[feature]]\n",
    "                    if feature in node_dataset_map.keys()\n",
    "                    else \"white\"\n",
    "                )\n",
    "                for feature in G.nodes\n",
    "            ],\n",
    "            edge_color=list(nx.get_edge_attributes(G, \"weight\").values()),\n",
    "            font_color=\"black\",\n",
    "            font_size=10,\n",
    "            edge_cmap=matplotlib.colormaps[\"Purples\"],\n",
    "            connectionstyle=\"arc3, rad=1\",\n",
    "        )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(\n",
    "            output_path / f\"Feature_association_graph_{layout}.png\", format=\"png\"\n",
    "        )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_feature_mean_median(\n",
    "    array: FloatArray, axis=0, style: str = DEFAULT_PLOT_STYLE\n",
    ") -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Plot feature values together with the mean, median, min and max values\n",
    "    at each array position.\n",
    "    \"\"\"\n",
    "    with style_settings(style):\n",
    "        fig = plt.figure(figsize=(15, 3))\n",
    "        y = np.mean(array, axis=axis)\n",
    "        y_2 = np.median(array, axis=axis)\n",
    "        y_3 = np.max(array, axis=axis)\n",
    "        y_4 = np.min(array, axis=axis)\n",
    "        plt.plot(np.arange(len(y)), y, \"bo\", label=\"mean\")\n",
    "        plt.plot(np.arange(len(y_2)), y_2, \"ro\", label=\"median\")\n",
    "        plt.plot(np.arange(len(y_3)), y_3, \"go\", label=\"max\")\n",
    "        plt.plot(np.arange(len(y_4)), y_4, \"yo\", label=\"min\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"feature\")\n",
    "        plt.ylabel(\"mean/median/min/max\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_reconstruction_movement(\n",
    "    baseline_recon: FloatArray,\n",
    "    perturb_recon: FloatArray,\n",
    "    k: int,\n",
    "    style: str = DEFAULT_PLOT_STYLE,\n",
    ") -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Plot, for each sample, the change in value from the unperturbed reconstruction to\n",
    "    the perturbed reconstruction. Blue lines are left/negative shifts,\n",
    "    red lines are right/positive shifts.\n",
    "\n",
    "    Args:\n",
    "        baseline_recon: baseline reconstruction array with s samples\n",
    "                        and k features (s,k).\n",
    "        perturb_recon:  perturbed\n",
    "        k: feature index. The shift (movement) of this feature's reconstruction\n",
    "                          will be plotted for all samples s.\n",
    "    \"\"\"\n",
    "    with style_settings(style):\n",
    "        # Feature changes\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        for s in range(np.shape(baseline_recon)[0]):\n",
    "            plt.arrow(\n",
    "                baseline_recon[s, k],\n",
    "                s / np.shape(baseline_recon)[0] ,\n",
    "                perturb_recon[s, k],\n",
    "                0,\n",
    "                length_includes_head=True,\n",
    "                color=[\"r\" if baseline_recon[s, k] < perturb_recon[s, k] else \"b\"][0],\n",
    "            )\n",
    "        plt.ylabel(\"Sample / Total Samples\", size=14)\n",
    "        plt.xlabel(\"Feature_value\", size=14)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_cumulative_distributions(\n",
    "    edges: FloatArray,\n",
    "    hist_base: FloatArray,\n",
    "    hist_pert: FloatArray,\n",
    "    title: str,\n",
    "    style: str = DEFAULT_PLOT_STYLE,\n",
    ") -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Plot the cumulative distribution of the histograms for the baseline\n",
    "    and perturbed reconstructions. This is useful to visually assess the\n",
    "    magnitude of the shift corresponding to the KS score.\n",
    "\n",
    "    \"\"\"\n",
    "    # Calculate bin widths\n",
    "    bin_widths = np.diff(edges)\n",
    "    \n",
    "    # Calculate cumulative distributions\n",
    "    cum_base = np.cumsum(hist_base * bin_widths)\n",
    "    cum_pert = np.cumsum(hist_pert * bin_widths)\n",
    "\n",
    "    with style_settings(style):\n",
    "        # Cumulative distribution:\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        plt.plot(\n",
    "            (edges[:-1] + edges[1:]) / 2,\n",
    "            cum_base,\n",
    "            color=\"blue\",\n",
    "            label=\"baseline\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        plt.plot(\n",
    "            (edges[:-1] + edges[1:]) / 2,\n",
    "            cum_pert,\n",
    "            color=\"red\",\n",
    "            label=\"Perturbed\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{title}.png\")\n",
    "        plt.xlabel(\"Feature value\")\n",
    "        plt.ylabel(\"Cumulative distribution\")\n",
    "        plt.legend()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_correlations(\n",
    "    x: FloatArray,\n",
    "    y: FloatArray,\n",
    "    x_pol: FloatArray,\n",
    "    y_pol: FloatArray,\n",
    "    a2: float,\n",
    "    a1: float,\n",
    "    a: float,\n",
    "    k: int,\n",
    "    style: str = DEFAULT_PLOT_STYLE,\n",
    ") -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Plot y vs x and the corresponding polynomial fit.\n",
    "    \"\"\"\n",
    "    with style_settings(style):\n",
    "        # Plot correlations\n",
    "        fig = plt.figure(figsize=(7, 7))\n",
    "        plt.plot(x, y, marker=\".\", lw=0, markersize=5, color=\"red\")\n",
    "        plt.plot(\n",
    "            x_pol,\n",
    "            y_pol,\n",
    "            color=\"blue\",\n",
    "            label=\"{0:.2f}x^2 {1:.2f}x {2:.2f}\".format(a2, a1, a),\n",
    "            lw=1,\n",
    "        )\n",
    "        plt.plot(x_pol, x_pol, lw=1, color=\"k\")\n",
    "        plt.xlabel(f\"Feature {k} baseline values \")\n",
    "        plt.ylabel(f\"Feature {k} baseline  value reconstruction\")\n",
    "        plt.legend()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def get_2nd_order_polynomial(x_array, y_array, n_points=100):\n",
    "    \"\"\"\n",
    "    Given a set of x an y values, find the 2nd oder polynomial fitting best the data.\n",
    "    Returns:\n",
    "        x_pol: x coordinates for the polynomial function evaluation.\n",
    "        y_pol: y coordinates for the polynomial function evaluation.\n",
    "    \"\"\"\n",
    "    a2, a1, a = np.polyfit(x_array, y_array, deg=2)\n",
    "\n",
    "    x_pol = np.linspace(np.min(x_array), np.max(x_array), n_points)\n",
    "    y_pol = a2 * x_pol**2 + a1 * x_pol + a\n",
    "\n",
    "    return x_pol, y_pol, (a2, a1, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Checking the reconstructed class after perturbing a categorical variable. \n",
    "\n",
    "We noticed that categorical perturbations in the inputs induced tiny movements of the representations of the samples in latent space. Since samples did not abandon the cluster of origin, we suspected that the reconstructions would not recapitulate the perturbed state, but rather go back to the class of origin. To do that, we added a couple of lines to store the true targets and predicted targets for both baseline and perturbed inputs when identifying associations using the Bayes approach. To do that, we used the ```latent``` method in the VAE class.\n",
    "\n",
    "In ```identify_associations.py```, we added: \n",
    "\n",
    "```python \n",
    "# Calculate baseline reconstruction\n",
    "_, baseline_recon = model.reconstruct(baseline_dataloader) # Was there already\n",
    "_, _, cat_recon_baseline, cat_class_baseline, _, _, _ = model.latent(baseline_dataloader, kld_weight=0.0001)\n",
    "np.save('cat_recon_baseline.npy',cat_recon_baseline)\n",
    "np.save('cat_class_baseline.npy',cat_class_baseline)\n",
    "\n",
    "```\n",
    "\n",
    "Idem for the perturbed dataloaders, using ```dataloaders[i]``` as the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('move')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe194b1d27fbcd7437bf5eb8413313a2683e7f0cd626c1458cb32c6954f64d40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
