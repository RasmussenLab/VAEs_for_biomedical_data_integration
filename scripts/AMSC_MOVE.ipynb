{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Characterizing the evolution of adipose-derived mesenchymal stem cells (AMSCs) after FFA addition with VAEs <center> \n",
    "\n",
    "**Summary**\n",
    "    \n",
    "In this project we will use MOVE (Multi-Omics Variational Autoencoder) to integrate Lipocyte Profiler data, RNA-seq data and Polygenic risk scores (PRSs). We will aim to:\n",
    "\n",
    "- Find the most relevant variables to characterize the system of study.\n",
    "- Find associations between gene expression levels and morphological features.\n",
    "- Analyze the effects of lowering/increasing PRS scores.\n",
    "    \n",
    "**Papers of interest:**\n",
    "    \n",
    "- Allesøe, R.L., Lundgaard, A.T., Hernández Medina, R. *et al*. Discovery of\n",
    "drug–omics associations in type 2 diabetes with generative deep-learning models.*Nat Biotechnol* (2023). https://www.nature.com/articles/s41587-022-01520-x\n",
    "- Samantha Laber, Sophie Strobel, Josep M. Mercader, Hesam Dashti, Felipe R.C. dos Santos, Phil Kubitz, Maya Jackson, Alina Ainbinder, Julius Honecker, Saaket Agrawal, Garrett Garborcauskas, David R. Stirling, Aaron Leong, Katherine Figueroa, Nasa Sinnott-Armstrong, Maria Kost-Alimova, Giacomo Deodato, Alycen Harney, Gregory P. Way, Alham Saadat, Sierra Harken, Saskia Reibe-Pal, Hannah Ebert, Yixin Zhang, Virtu Calabuig-Navarro, Elizabeth McGonagle, Adam Stefek, Josée Dupuis, Beth A. Cimini, Hans Hauner, Miriam S. Udler, Anne E. Carpenter, Jose C. Florez, Cecilia Lindgren, Suzanne B.R. Jacobs, Melina Claussnitzer. Discovering cellular programs of intrinsic and extrinsic drivers of metabolic traits using LipocyteProfiler, Cell Genomics,Volume 3, Issue 7 (2023) https://doi.org/10.1016/j.xgen.2023.100346\n",
    "    \n",
    "    \n",
    "**Theoretical insights**\n",
    "\n",
    "**About Lipocyte Profiler:**\n",
    "\n",
    "Lipocyte Profiler extends Cell-Painting and Cell Profiler. Cell images are obtained in a multi-channel fashion, staining for the nucleus, mitochondria, golgi apparatus, lipid vesicles, etc. Then, morphological features characterizing the cell's shape and texture (like granularity, radial distribution of certain organelles, intensity) are quantified and summarized in feature vectors.\n",
    "\n",
    "**About RNA-seq:**\n",
    "\n",
    "RNA sequencing will provide gene expression levels at different timepoints for:\n",
    "- Control cells\n",
    "- Cells that have been subject to the addition of free fatty acids (FFAs).\n",
    "\n",
    "**About MOVE:**\n",
    "\n",
    "MOVE is a Variational AutoEncoder, i.e., it is a neural network trained to compress the information and reconstruct it as accurately as possible. The variational part of it makes it more robust to noise in the input, allows us to treat it from a bayesian perspective, and enables us to use it as a generative model, i.e. we can generate new samples.\n",
    "\n",
    "MOVE applies *in silico* perturbations to the input in order to find associations between variables.\n",
    "\n",
    "*__The architecture of the model__*\n",
    "\n",
    "MOVE consists in two parts: an encoder and a decoder. \n",
    "\n",
    "The encoder will compress the input information and extract shared information between features. The encoder is composed by the input layer, a vector of feature values for a given sample; hidden layers connecting the different features, and a latent layer.\n",
    "\n",
    "The latent layer will contain a compressed representation of our data (lower dimensionality feature vector). Each sample will lie somewhere in what we call \"the latent space\". The decoder will then reconstruct the input from its latent representation.\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages\n",
    "The first step is to load all third party packages required to perform the different tasks in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "from firecloud import fiss\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import gc\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Latin Modern Roman' #Font for the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a number of global hyperparameters of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Hyperparams #############\n",
    "INSTALL = True # If it is the first run, we want to install MOVE in this environment\n",
    "CHECK_CORR = False # To visualize the correlation matrices of RNA features\n",
    "RESIDUALIZE = True #Do we want to correct the filtered RNA features for sex, batch and age dependencies?\n",
    "HYPER_TUNNING = True # To perform hyper-parameter tuning\n",
    "\n",
    "DATA_FOLDERS = ['data_residualized/','interim_data_rsd/'] if RESIDUALIZE else ['data/','interim_data/'] \n",
    "FIGURE_FOLDER = 'paper_figures/'\n",
    "! mkdir -p {FIGURE_FOLDER} # Make figure folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time we run it we will install a number of packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INSTALL:\n",
    "    # We will clone the version under development of MOVE, which can handle perturbations of continuous variables\n",
    "    #! git clone -b developer-continuous-v3 https://github.com/RasmussenLab/MOVE.git /home/jupyter/.local/bin/MOVE\n",
    "    ! git clone https://github.com/RasmussenLab/MOVE.git /home/jupyter/.local/bin/MOVE\n",
    "    sys.path.append(\"/home/jupyter/.local/bin/MOVE/src\")\n",
    "    # ! pip install -e /home/jupyter/.local/bin/MOVE/ \n",
    "    \n",
    "    #from terra_notebook_utils import table, gs, drs\n",
    "    # We can also enable different sections to fold\n",
    "    ! jupyter nbextension enable codefolding/main\n",
    "    ! jupyter nbextension enable collapsible_headings/main\n",
    "\n",
    "    ! pip install omegaconf upsetplot umap-learn\n",
    "    ! pip install -r /home/jupyter/.local/bin/MOVE/requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables\n",
    "\n",
    "\n",
    "**Navigating the Terra interface and file structure:**\n",
    "\n",
    "For this project we will mainly use two buckets: \n",
    "- 1) The \"local\" project bucket where we will store all predefined configuration files. These files are found in the DATA tab of this workspace: ```other data``` $\\to$ ```files  ```\n",
    "- 2) The \"external\" bucket where we will get the data from, called ```shared_2023```.\n",
    "\n",
    "We can store the names of these buckets in variables for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BILLING_PROJECT_ID = os.environ['WORKSPACE_NAMESPACE']\n",
    "WORKSPACE = os.environ['WORKSPACE_NAME']\n",
    "bucket = os.environ['WORKSPACE_BUCKET']\n",
    "\n",
    "#EXTERNAL_BUCKET = \"gs://amsc_datasets/\"\n",
    "EXTERNAL_BUCKET = \"gs://collaborations_marc/shared_2023/\"\n",
    "CONTENT = !gsutil ls {EXTERNAL_BUCKET}\n",
    "    \n",
    "print(\"Billing project: \" + BILLING_PROJECT_ID)\n",
    "print(\"Workspace: \" + WORKSPACE)\n",
    "print(\"Bucket: \" + bucket)\n",
    "print(\"Files in the external bucket:\", *CONTENT, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split matching RNA-LP file and create IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work with the file _matchedID_normalizedRNA_LP.csv_ . To store the data in a MOVE-friendly manner we have to:\n",
    "\n",
    "1) __Create an ID file:__ We need unique IDs on a sample per sample basis, so we'll keep an ID column with numerical ids.\n",
    "\n",
    "2) __Split the data in smaller datasets that will be perturbed one by one.__ These datasets will be:\n",
    "\n",
    "    - Demographics\n",
    "        - Sex \n",
    "        - Age (will not be provided to the model)\n",
    "        - Batch (will not be provided to the model)\n",
    "    - RNA IDs\n",
    "    - Nuclei\n",
    "    - Cells\n",
    "    - Cytoplasm\n",
    "    \n",
    "3) __Filter out genes that have low expression or low variance.__\n",
    "4) __Filter out LP features that have extreme outliers.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original csv files as a pandas dataframes\n",
    "RNA_LP = EXTERNAL_BUCKET + \"matchedID_normalizedRNA_LP.csv\" \n",
    "df_RNA_LP = pd.read_csv(RNA_LP)\n",
    "\n",
    "covariates = df_RNA_LP[['age','sex','batch']]\n",
    "\n",
    "variants = EXTERNAL_BUCKET + \"variants.csv\"\n",
    "df_variants = pd.read_csv(variants)\n",
    "\n",
    "PRS = EXTERNAL_BUCKET + \"PRS.csv\"\n",
    "df_PRS = pd.read_csv(PRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding patients that were not genotyped\n",
    "patient_set = set(df_RNA_LP['patientID'].values)\n",
    "variant_set = set(df_variants['SubjID'].values)\n",
    "missing =  patient_set - variant_set\n",
    "print( \"These patients in the RNA-LP dataset were not genotyped:\\n\", *missing, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples falling under different subgroups:\n",
    "for cell_type in ['sc', 'vc']:\n",
    "    for con_3 in [0,1]:\n",
    "        for con_2 in [0,3,8,14]:\n",
    "            CON_1 = df_RNA_LP['cellType'] == cell_type\n",
    "            CON_2 = df_RNA_LP['Day'] == con_2\n",
    "            CON_3 = df_RNA_LP['FFA'] == con_3\n",
    "\n",
    "            # Filter rows where column 'A' has value 't'\n",
    "            filtered_df = df_RNA_LP[CON_1 & CON_2 & CON_3]\n",
    "\n",
    "            # Count the rows in the filtered dataframe\n",
    "            count = filtered_df.shape[0]\n",
    "\n",
    "            print(f\"For cell type {cell_type}, Day {con_2}, FFA {con_3} we have {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing sex-dependent gene expression variation\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "df_violin_1 = df_RNA_LP[df_RNA_LP['sex'] == 1].filter(regex=\"^ENSG\").iloc[:,:50]\n",
    "df_violin_2 = df_RNA_LP[df_RNA_LP['sex'] == 2].filter(regex=\"^ENSG\").iloc[:,:50]\n",
    "plt.violinplot(df_violin_1)\n",
    "plt.violinplot(df_violin_2)\n",
    "plt.xticks(np.arange(1,51),df_violin_1.columns, rotation=90)\n",
    "plt.xlabel('Gene')\n",
    "plt.ylabel('Expression (DESeq norm counts)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compensate for linear effects of sex, age and batch on gene expression features.\n",
    "\n",
    "Here we will train a linear model to remove all linear contributions of the covariates to the expression levels.\n",
    "We should decide on which genes to keep before residualizing, since the transformation changes significantly the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residualize(targets, covariates):\n",
    "    \"\"\"\n",
    "    This function trains a linear model to take into account the contributions\n",
    "    of age, sex and batch in the gene expression values (RNA counts).\n",
    "    \n",
    "    Args:\n",
    "        targets: Pandas dataframe of shape (N_samples x N_target_features) \n",
    "                 containing the target features to correct.\n",
    "        covariates: Pandas dataframe of shape (N_samples x N_covariates)\n",
    "                    containing the independent variables that we want to \n",
    "                    correct for.\n",
    "    Returns:\n",
    "        corrected_targets: Array with the linear contributions of the covariates \n",
    "                           removed.\n",
    "    \n",
    "    \"\"\"\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    # Add a constant term to the independent variables\n",
    "    covariates = sm.add_constant(covariates)\n",
    "    \n",
    "    # Fit a multivariate linear regression model\n",
    "    model = sm.OLS(targets, covariates).fit()\n",
    "    predictions = pd.DataFrame(data=model.predict(covariates))\n",
    "    predictions.columns = targets.columns\n",
    "    \n",
    "    # Get the residuals (corrected values) for each target variable\n",
    "    residuals = targets - predictions\n",
    "    \n",
    "    # Return the residuals (corrected targets)\n",
    "    return residuals\n",
    "\n",
    "def plot_expression_distributions(targets, covariates, title, covariate_of_interest = 'sex', n_genes = 50, savepath = Path(FIGURE_FOLDER)):\n",
    "    # Visualizing sex-dependent gene expression variation\n",
    "    fig = plt.figure(figsize=(9,3))\n",
    "    df_violin_1 = targets[covariates[covariate_of_interest] == 1].iloc[:,:50]\n",
    "    df_violin_2 = targets[covariates[covariate_of_interest] == 2].iloc[:,:50]\n",
    "    plt.violinplot(df_violin_1)\n",
    "    plt.violinplot(df_violin_2)\n",
    "    plt.xticks(np.arange(1,n_genes + 1),df_violin_1.columns, rotation=90)\n",
    "    plt.xlabel('Gene')\n",
    "    plt.ylabel('Expression (DESeq norm counts)')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(savepath / f\"{title}.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create splitted datasets\n",
    "\n",
    "This is one of the most important steps in the pipeline.\n",
    "For the RNA processing, we first:\n",
    "- Filter out genes that have very low expression across samples.\n",
    "    - We keep genes for which at least 20% of the samples have an expression above 10 reads.\n",
    "- Keep the genes that present a high variance (after log2 transform).\n",
    "\n",
    "We then residualize, i.e. remove linear contributions of age, batch and sex to the values. We finally Z-score normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Make a directory where we can store the newly splitted files in this session\n",
    "! mkdir -p {DATA_FOLDERS[0]}\n",
    "data_path = Path(DATA_FOLDERS[0])\n",
    "\n",
    "\n",
    "# Filter the original dataset according to column names:\n",
    "new_dataset_criteria_dict = {\"batch\":\"^batch\",\n",
    "                             \"patientID\":\"patientID\",\n",
    "                             \"sex\":\"sex\",\n",
    "                             \"age\":\"^age\",\n",
    "                             \"BMI\":\"BMI\",\n",
    "                             \"T2D\":\"^T2D\",\n",
    "                             \"FFA\":\"^FFA\",\n",
    "                             \"cellType\":\"^cellType\",\n",
    "                             \"Day\":\"^Day\",\n",
    "                             \"RNA\":\"^ENSG\",\n",
    "                             \"Cyto\":\"^Cytoplasm\",\n",
    "                             \"Cells\":\"^Cells\",\n",
    "                             \"Nuc\":\"^Nuclei\"}\n",
    "\n",
    "\n",
    "################ RNA PREPROCESSING HYPERPARAMS ##############\n",
    "min_expression = 10\n",
    "min_samples = 54 # ca. 20% of the dataset, 269 samples \n",
    "var_threshold = 0.75 # Quantile\n",
    "#############################################################\n",
    "\n",
    "# Create unique ID file:\n",
    "ids = pd.DataFrame(df_RNA_LP.index.astype(str).tolist(), columns=[\"ID\"])\n",
    "ids.to_csv(data_path /  \"AMSC_ids.txt\", index=False, header=False, sep=\"\\t\")\n",
    "\n",
    "# Create separate csv files with shared IDS\n",
    "for key in new_dataset_criteria_dict.keys():    \n",
    "    splitted_df = df_RNA_LP.filter(regex=new_dataset_criteria_dict[key])\n",
    "    \n",
    "    if key == \"T2D\":\n",
    "        splitted_df = splitted_df.astype(\"str\") \n",
    "    # Filter out non-informative genes: zero mean or below a given percentile   \n",
    "    \n",
    "    if key == \"RNA\":\n",
    "        # 1. Initial low-expression filtering\n",
    "        genes_to_keep = (splitted_df > min_expression).sum(axis=0) >= min_samples\n",
    "        filtered_df = splitted_df.loc[:, genes_to_keep]\n",
    "        print(f\"Shape after initial filtering: {filtered_df.shape}\")\n",
    "\n",
    "        # 2. Log2 transformation\n",
    "        log2_df = np.log2(filtered_df + 1)  # Add 1 to avoid log(0)\n",
    "\n",
    "        # 3. Variability-based filtering\n",
    "        gene_vars = log2_df.var()\n",
    "        var_threshold = gene_vars.quantile(var_threshold)\n",
    "        highly_variable_genes = gene_vars[gene_vars > var_threshold].index\n",
    "        splitted_df = log2_df[highly_variable_genes]\n",
    "        print(f\"Shape after variability filtering: {splitted_df.shape}\")\n",
    "\n",
    "        if RESIDUALIZE:\n",
    "            # 4. Residualization\n",
    "            residualized_df = residualize(splitted_df, covariates)\n",
    "            print(\"After residualizing\")\n",
    "            plot_expression_distributions(residualized_df, covariates, \"RNA_after_residualizing\", covariate_of_interest = 'sex', n_genes = 50)     \n",
    "            \n",
    "            # 5. Z-score normalization\n",
    "            splitted_df = pd.DataFrame(scale(residualized_df), \n",
    "                                     index=residualized_df.index, \n",
    "                                     columns=residualized_df.columns)\n",
    "            \n",
    "            print(\"After Z-scoring\")\n",
    "            plot_expression_distributions(splitted_df, covariates, \"RNA_after_rz_scoring\", covariate_of_interest = 'sex', n_genes = 50)\n",
    "            print(np.max(splitted_df.values), np.min(splitted_df.values))\n",
    "            \n",
    "    if (key == \"Cells\") or (key == \"Cyto\"):\n",
    "        #Drop features that contain Nans:\n",
    "        splitted_df = splitted_df.dropna(axis=\"columns\")\n",
    "        value_thr = 5\n",
    "        columns_to_drop = splitted_df.columns[(splitted_df > value_thr).any() | (splitted_df < -value_thr).any()]\n",
    "        print(f\"Columns to drop for the {key} dataset\", len(columns_to_drop))\n",
    "        splitted_df = splitted_df.drop(columns=columns_to_drop)\n",
    "    if key == \"Nuc\":\n",
    "        print(\"NUC:\\n\")\n",
    "        \n",
    "    splitted_df = pd.concat([ids,splitted_df], axis = 1)\n",
    "    splitted_df.to_csv(data_path / f\"{key}.tsv\", sep=\"\\t\", header=True, index=False)\n",
    "    print(\"Final shape of the dataset:\", splitted_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize correlations between features (RNA-seq post filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "MODALITY = \"RNA\"\n",
    "data_path = Path(DATA_FOLDERS[0])\n",
    "df = pd.read_csv(data_path / f\"{MODALITY}.tsv\", sep=\"\\t\").set_index(\"ID\")\n",
    "STEP = 200\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "columns = corr_matrix.columns\n",
    "\n",
    "# Compute the linkage matrix\n",
    "linkage_matrix = linkage(corr_matrix, method='average')\n",
    "\n",
    "# Get the order of the variables\n",
    "ordered_indices = leaves_list(linkage_matrix)\n",
    "corr_matrix = corr_matrix.iloc[ordered_indices, ordered_indices]\n",
    "columns = columns[ordered_indices]\n",
    "\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.imshow(corr_matrix, cmap='seismic', vmin=-1, vmax=1)\n",
    "plt.xticks(np.arange(0,len(columns),STEP), columns[::STEP], rotation = 90)\n",
    "plt.yticks(np.arange(0,len(columns),STEP), columns[::STEP])\n",
    "plt.title(f'{MODALITY} Correlation Matrix')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "fig.savefig(Path(FIGURE_FOLDER) / f\"{MODALITY}_Correlation_matrix.png\", dpi=200)\n",
    "\n",
    "# Extract the upper triangle of the correlation matrix without the diagonal\n",
    "upper_triangle_indices = np.triu_indices_from(corr_matrix, k=1)\n",
    "upper_triangle_values = corr_matrix.values[upper_triangle_indices]\n",
    "\n",
    "# Plot the frequency distribution of the correlation coefficients\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "sns.kdeplot(abs(upper_triangle_values), bw_adjust=0.5)\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Density')\n",
    "plt.title(f'Density Plot of Correlation Coefficients in {MODALITY}')\n",
    "plt.tight_layout()\n",
    "fig.savefig(Path(FIGURE_FOLDER) / f\"{MODALITY}_Correlation_distribution.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Mutual information between features. (RNA post processing, optional)\n",
    "\n",
    "> 🕑 : This step takes a long time and it was skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI = False\n",
    "\n",
    "if MI:\n",
    "\n",
    "    from sklearn.feature_selection import mutual_info_regression\n",
    "    from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "    def mutual_info_continuous(x, y):\n",
    "        return mutual_info_regression(x.reshape(-1, 1), y)[0]\n",
    "\n",
    "    def compute_mutual_info_matrix_continuous(df):\n",
    "        n = df.shape[1]\n",
    "        mi_matrix = np.zeros((n, n))\n",
    "\n",
    "        def compute_mi(i, j):\n",
    "            if i == j:\n",
    "                return (i, j, 0)\n",
    "            else:\n",
    "                mi = mutual_info_continuous(df.iloc[:, i].values, df.iloc[:, j].values)\n",
    "                return (i, j, mi)\n",
    "\n",
    "        results = Parallel(n_jobs=-1)(delayed(compute_mi)(i, j) for i in range(n) for j in range(i, n))\n",
    "\n",
    "        for i, j, mi in results:\n",
    "            mi_matrix[i, j] = mi\n",
    "            mi_matrix[j, i] = mi\n",
    "\n",
    "        return pd.DataFrame(mi_matrix, index=df.columns, columns=df.columns)\n",
    "\n",
    "    # Assuming df is your DataFrame\n",
    "    mi_matrix = compute_mutual_info_matrix_continuous(df)\n",
    "\n",
    "    # Perform hierarchical clustering\n",
    "    linkage_matrix = linkage(mi_matrix, method='average')\n",
    "    ordered_indices = leaves_list(linkage_matrix)\n",
    "\n",
    "    # Reorder the mutual information matrix\n",
    "    ordered_mi_matrix = mi_matrix.iloc[ordered_indices, ordered_indices]\n",
    "\n",
    "\n",
    "    # Plot the MI matrix as a heatmap\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(mi_matrix, cmap='inferno') #, vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the datasets for the variants and PRSs mapping subject IDs to the new numerical IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create tsv file for the variants**\n",
    "\n",
    "We can first visualize the default variants file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "# Output some of the first and last lines of the original variants dataframe\n",
    "df_variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to match these patients to the patients we have LP and gene expression data for. In addition, we will unphase the heterozygous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variant file with shared IDS: map original ID in variant file to new unique id\n",
    "df_variants_new = df_variants.rename(columns={'SubjID':'patientID'}).set_index('patientID')\n",
    "#print(df_variants_new.head)\n",
    "df_patients = pd.read_csv(data_path / \"patientID.tsv\", sep=\"\\t\")\n",
    "# Merge variant file (right df) with patient id mapping file (left df) and remove original patientID column\n",
    "df_patients = df_patients.merge(df_variants_new, left_on='patientID', right_index=True).drop(columns=\"patientID\")\n",
    "\n",
    "\n",
    "# Since not all patients are genotyped we need to fill the missing indices with NAs:\n",
    "new_index = range(len(ids))\n",
    "df_patients = df_patients.reindex(new_index)\n",
    "# We can now unphase the genotypes\n",
    "df_patients = df_patients.replace('0|1', '0/1')\n",
    "df_patients = df_patients.replace('1|0', '0/1')\n",
    "df_patients[\"ID\"] = df_patients.index\n",
    "df_patients.to_csv(data_path / \"variants.tsv\", sep=\"\\t\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create tsv file for the PRSs**\n",
    "\n",
    "We will now create a separate tsv file for the PRS scores. \n",
    "\n",
    "Process-specific PRS scores will be Z-score normalized (They were not originally). This can be seen in the plot below, where the first 5 violins contained data extremely close to zero originally (blue) and have been widened after the transformation (orange)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Same protocol as before for PRS file\n",
    "df_PRS_new = df_PRS.rename(columns={'FID':'patientID'}).set_index('patientID')\n",
    "df_patients = pd.read_csv(data_path / \"patientID.tsv\", sep=\"\\t\")\n",
    "df_patients = df_patients.merge(df_PRS_new, left_on='patientID', right_index=True).drop(columns=\"patientID\")\n",
    "\n",
    "\n",
    "df_patients[\"ID\"] = df_patients.index\n",
    "df_patients = df_patients.drop(columns=\"geneticBatch\") # Genetic batch is not a PRS as such\n",
    "\n",
    "plt.violinplot(df_patients.iloc[:,1:])\n",
    "# Z-score process specific PRSs\n",
    "df_patients[[colname for colname in df_patients.columns if \"prs.\" in colname]] = scale(df_patients[[colname for colname in df_patients.columns if \"prs.\" in colname]], axis=0)\n",
    "#Plot violinplots for PRS value distributions\n",
    "plt.violinplot(df_patients.iloc[:,1:])\n",
    "# Save the results\n",
    "df_patients.to_csv(data_path / \"PRS.tsv\", sep=\"\\t\", header=True, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PRS distributions and related covariates\n",
    "for i,col in enumerate(df_PRS.columns):\n",
    "    fig = plt.figure()\n",
    "    plt.hist(df_PRS[col], bins = 100)\n",
    "    plt.title(f\"{col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "\n",
    "\n",
    "### MOVE\n",
    "\n",
    "MOVE can be found online at RasmussenLab's Github repository (https://github.com/RasmussenLab/MOVE). It can be downloaded as a pip package from the command line as follows:\n",
    "```bash\n",
    "pip install move-dl\n",
    "```\n",
    "\n",
    "However, we will proceed differently. At the beginning of the notebook we cloned MOVE to this environment, i.e. we \"copied\" MOVE here. Now we will install it in \"editable\" mode (-e flag), so that we can change the code locally if required. This could be done by editing the local copy of MOVE, found in ```/home/jupyter/.local/bin/MOVE```.\n",
    "\n",
    "⚠️ **Code edits**\n",
    "A few edits to MOVE's code were made for this project.\n",
    "           \n",
    ">1) **Storing multiple runs:**                    \n",
    "> We stored the SHAP results for different hyperparameter choices and obtained the results as ensemble averages. To do that, we modified ```MOVE/src/move/visualization/feature_importance.py``` to store the results in txt files.\n",
    ">\n",
    ">2) **Removing feature_mask:**\n",
    ">   When identifying associations (```identify_associations.py```) , feature_masks were removed and we only kept nan_masks.\n",
    ">\n",
    "> 3) **Corrected Cumulative distributions**\n",
    "> Cumulative distributions obtained when using the KS method were not properly normalyzed. We now multiply by bin_width to obtain a distribution between 0-1. plot_cumulative function found in ```dataset_distributions.py```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INSTALL:\n",
    "    ! pip install -e /home/jupyter/.local/bin/MOVE/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import config files\n",
    "\n",
    "Config files for this project will be stored in the data section of the workspace. New config files can also be created and uploaded there. When we run the following cell we copy everything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy multiple files in workspace data to the cloud environment\n",
    "!gsutil -m cp -r $bucket/* . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in order to run MOVE is to encode the data in a format that the model can understand.\n",
    "\n",
    "To encode the data we store all datasets in a TSV format. Each table needs to have a shape `N` &times; `M`, i.e. `N` rows and `M` columns where `N` is the number of samples/individuals and `M` is the number of features.\n",
    "\n",
    "> 📡 **How is data encoded?**\n",
    ">\n",
    "> **_Categorical data is one-hot encoded._** _For a feature like cellType , which has discrete values/categories (e.g., sc or vc), we encode these categories as_ binary bit flags. _This means each category is assigned a value starting with one, and then represented in binary format (with zeros and ones)._\n",
    ">\n",
    ">_A useful property of flags is that they do not have hierarchy; they are incompatible with \"<\" or \">\" operators. So (in our example), sc would not be considered more or less important than vc._\n",
    ">\n",
    "> **_Continuous data can be z-score normalized_**, _meaning that each feature can be rescaled to have zero mean and unit variance:_\n",
    ">\n",
    ">  $$ Z = \\frac{x-\\mu}{\\sigma} $$\n",
    "> Where $x$ is the vector of feature values for all samples, $\\mu$ and $\\sigma$ its mean and standard deviation, respectively.\n",
    "> In this specific project we will work with either the already normalized LP and DESeq normalized gene expressions (after a log2(x+1) transformation) or the residualized matrices over sex, age and batch. These steps were performed in the prepreocessing of the data before feeding the datasets to MOVE, so we set the flags for log2 transform and scale in the config files to false.\n",
    "\n",
    "\n",
    "The first step is to read the configuration called `AMSC` and specify the pre-defined task called `encode_data`.\n",
    "\n",
    "⚠️ Remember that the notebook takes user-defined configs in a `config/data` directory located in the current working directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from move.data import io\n",
    "\n",
    "\n",
    "config =io.read_config(\"AMSC\", None, \n",
    "                        f\"data.raw_data_path={DATA_FOLDERS[0]}\", \n",
    "                        f\"data.interim_data_path={DATA_FOLDERS[1]}\")\n",
    "\n",
    "# Print data config file\n",
    "print(OmegaConf.to_yaml(config, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! move-dl data=AMSC task=encode_data data.raw_data_path={DATA_FOLDERS[0]} data.interim_data_path=\"./interim_data_rsd_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check how does the folder structure look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will be encoded accordingly and saved to the directory defined as `interim_data_path` in the `data` configuration.\n",
    "\n",
    "\n",
    "\n",
    "We can confirm how the data looks by loading it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(DATA_FOLDERS[1])\n",
    "\n",
    "cat_datasets, cat_names, con_datasets, con_names = io.load_preprocessed_data(path, config.data.categorical_names, config.data.continuous_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the encoded datasets can now be checked.\n",
    "\n",
    "> 🔺🟡🟩🔷 **What is the shape of a dataset?**\n",
    ">\n",
    "> If we visualize each dataset as a matrix of values, the shape of the matrix refers to the number of values or entries that can be found in every dimension of the matrix. As an example, the dataset `PRS` is described by a matrix with 2 dimensions. It contains 269 samples (in the rows, the first dimension) for which 17 PRS scores were quantified (17 features in columns, the second dimension). The shape of the matrix is indicated in this case as (269, 17). For categorical datasets, we have an additional dimension to hold the number of categories/classes. So a dataset like `cellType` has 269 samples, 1 feature (cellType) and 2 categories (sc/vc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = config.data.categorical_names + config.data.continuous_names\n",
    "\n",
    "for dataset, dataset_name in zip(cat_datasets + con_datasets, dataset_names):\n",
    "    print(f\"{dataset_name}: {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**Note:** Cells and Cyto do not show the same shapes as when we created the datasets. This is because there were some columns full of zeros that were not filtered out originally. These are the true shapes we will work with._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Intermission: ML basics__\n",
    "\n",
    "In the following sections, we will begin mentioning machine learning (ML) terms. If this is your first approximation to ML, read the following snippet and peruse this [ML glossary](https://developers.google.com/machine-learning/glossary) if need be.\n",
    "\n",
    "> 🦾 **ML 101: Training the model**\n",
    ">\n",
    ">_The main goal when training a machine learning algorithm is to minimize the difference between the model's output and the ground truth output we would like to achieve. For autoencoders, the output is the same set of values as we had in the input._\n",
    ">\n",
    "> _To do that, we feed the model a set of input samples, add the errors in the outputs of the model in its current state, and update the relative contributions of the nodes in the network (weights) so that we get closer to the desired output. This is how the network is trained and \"learns\" how to perform the task._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section goes through the hyperparameter tuning of MOVE.\n",
    "\n",
    "\n",
    "> 🔧 **What are hyperparameters?**\n",
    ">\n",
    "> Hyperparameters are the variables that either determine the network's structure/architecture (e.g. number of nodes in a hidden layer) or define how the network is trained (e.g. the learning rate).\n",
    "\n",
    "The process of selecting an optimal set of hyperparameters is called tuning. In MOVE, we define optimal as the settings that produce models that generate the most accurate reconstructions and/or the most stable latent representations.\n",
    "\n",
    "We will first focus on the **reconstruction accuracy**. The reconstruction accuracy measures how well the model is able to reconstruct the data after it has been compressed to a latent space. It ranges from zero to one. One represents a lossless decompression (reconstruction).\n",
    "\n",
    "To illustrate how tuning works, we will assess how the number of latent and hidden nodes influence the model's reconstruction accuracy, among other hyperparameters. Running the following cell will start the training of a set of models models, which correspond to the different possible combinations of hyperparameters specified in the config file ```AMSC_tune_reconstruction.yaml```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HYPER_TUNNING:\n",
    "    ! move-dl experiment=AMSC__tune_reconstruction data.raw_data_path={DATA_FOLDERS[0]} data.interim_data_path={DATA_FOLDERS[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the previous command is a TSV table called ```reconstruction_stats.tsv```, recording the metrics of each\n",
    " run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the results in a pandas DataFrame\n",
    "if HYPER_TUNNING:\n",
    "    results = pd.read_csv(Path(\"./results/tune_model/reconstruction_stats.tsv\"), sep=\"\\t\")\n",
    "    results = results.drop_duplicates(subset=['task.batch_size',\n",
    "                                              'task.model.beta',\n",
    "                                              'task.model.num_hidden',\n",
    "                                              'task.model.num_latent',\n",
    "                                              'task.training_loop.num_epochs',\n",
    "                                              'metric',\n",
    "                                              'dataset',\n",
    "                                              'split'],\n",
    "                                      keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⤵️ _The following code will plot a set of containing the reconstruction accuracy of MOVE when trained using different hyperparameter sets. To visualize the reconstruction accuracies when testing other hyperparameters (e.g. number of hidden layers or number of nodes per layer) further changes in the code must be performed._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HYPER_TUNNING:    \n",
    "    \n",
    "    datasets = ['Cyto','Cells','PRS','RNA', 'Day']\n",
    "    # Hyperparameters that we want to compare:\n",
    "    num_hidden = [500, 1000, 2000] # 2000 # 3000\n",
    "    num_latent = [50, 100, 200]\n",
    "    h = 0\n",
    "\n",
    "    #Hyperparameters that we want to keep:\n",
    "    batch_size =  10\n",
    "    n_epochs =  300 #\n",
    "    beta = .0001\n",
    "\n",
    "    fig, axs = plt.subplots(len(num_hidden),len(datasets), layout=\"constrained\", figsize=(12, 9))\n",
    "    fig.suptitle(f\"beta = {beta}, n_epochs = {n_epochs}, batch_size={batch_size}\")\n",
    "\n",
    "    n_fields = len(num_latent) # Number of inner fields inside each subplot\n",
    "\n",
    "    for h,hidden in enumerate(num_hidden): # y axis shows what happens when varying the number of hidden nodes\n",
    "        # Conditions\n",
    "        subset_conditions = f\"(`task.training_loop.num_epochs` == {n_epochs}) \\\n",
    "        & (`task.model.beta` == {beta}) \\\n",
    "        & (`task.model.num_hidden` == '[{num_hidden[h]}]') \\\n",
    "        & (`task.batch_size` == {batch_size})\"\n",
    "        # & (`task.model.num_latent` == {num_latent[]}) \\\n",
    "        \n",
    "        subset_results = results.query(subset_conditions) #f\"`task.training_loop.num_epochs` == {n_epochs[i]}\")\n",
    "        \n",
    "        #print(subset_results)\n",
    "        for d,dataset in enumerate(datasets): # Each subplot in the x axis shows a different dataset\n",
    "            \n",
    "            results_dataset = subset_results.query(f\"dataset == '{dataset}'\")\n",
    "            #print(results_dataset.head)\n",
    "            test = results_dataset.query(\"split == 'test'\").to_dict(orient=\"records\")\n",
    "            train = results_dataset.query(\"split == 'train'\").to_dict(orient=\"records\")\n",
    "            \n",
    "            # matplotlib complains if fliers are unset\n",
    "            for bxp_stats in chain(train, test):\n",
    "                bxp_stats[\"fliers\"] = []\n",
    "                \n",
    "\n",
    "            coll1 = axs[h,d].bxp(train, positions=[*range(0, n_fields * 2, 2)], boxprops=dict(facecolor=\"#7570b3\"), patch_artist=True)\n",
    "            coll2 = axs[h,d].bxp(test, positions=[*range(1, n_fields * 2, 2)], boxprops=dict(facecolor=\"#1b9e77\"), patch_artist=True)\n",
    "\n",
    "\n",
    "            axs[h,d].set(xticks=np.arange(0.5, n_fields * 2, 2),\n",
    "                         xticklabels= num_latent,\n",
    "                         ylim=(0, 1),\n",
    "                         xlabel=\"# latent\",\n",
    "                         ylabel=f\"{hidden} hidden nodes\",\n",
    "                         title=f\"{dataset} features\")\n",
    "            \n",
    "        if h == 1:\n",
    "            axs[0,0].legend([coll1[\"boxes\"][0], coll2[\"boxes\"][0]], [\"train\", \"test\"], title=\"split\")\n",
    "        \n",
    "        elif d != 0:\n",
    "            axs[h,d].set_ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(Path(FIGURE_FOLDER) / f\"beta_{beta}_n_epochs_{n_epochs}_unphased.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a total of fifteen plots. \n",
    "\n",
    "In different columns of subplots we can see the reconstructions of different datasets.\n",
    "In different rows we can see the impact of changing the number of hidden nodes on the reconstructions.\n",
    "\n",
    "During tuning, we split the dataset into a **training** and **test set**. The former refers to the data that the model uses to learn, whereas the latter is a subset of \"new\" data that the model is unaware of. We can see that each plot has six boxes, with purple boxes showing training performances and green boxes showing test performances. The x-axis of each subplot shows the impact of increasing the number of latent nodes.\n",
    "\n",
    "__Hyperparameter tuning insights:__\n",
    "\n",
    "\n",
    "From the plots above, we can conclude that (in terms of reconstruction accuracy) it does not really matter the set of hyperparameters we use at such a low regularization regime.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section trains MOVE to integrate the data into a latent space. We will then plot the results and find the important variables for the integration using SHAP analysis.\n",
    "\n",
    "> ℹ️ About SHAP analysis.\n",
    ">\n",
    "> There are many ways to identify the most important features in the data, or the set of features that the model will use the most when encoding the data into a compressed/latent representation. One of them is SHAP (SHapley Additive exPlanations) analysis.\n",
    ">\n",
    "> This method measures how much do samples move in latent space when removing one variable at a time from the input. If the model gives a lot of importance to an input variable, e.g. the concentration of a metabolite, removing it from the input will lead to a significant movement of the samples in latent space (i.e., wide band in the SHAP plots, impact on latent space). On the other hand, if an input variable is not really needed, the model will \"ignore\" it and hence the latent space representation of the samples will not change much when that feature is not there anymore (impact on latent space close to 0).\n",
    "\n",
    "\n",
    "\n",
    "As in previous examples, first we need to read our configuration files and then we can run the `AMSC__latent` task. You can have a look at the file in ```config/task/AMSC__latent.yaml```.\n",
    "\n",
    "\n",
    ">_📖 For the advanced reader:_ _This config file, in addition, explicitly sets the learning rate to be 1e-4 and introduces the factors beta (controlling how variational do we make the autoencoder) and the Kullback-Leibler divergence warmup steps, which will gradually introduce the loss term that pushes the samples towards the center of the latent space._\n",
    "\n",
    "\n",
    "**Example latent space analysis:**\n",
    "\n",
    "a) *In command-line style:*\n",
    ">```python\n",
    "! move-dl data=AMSC task=AMSC__latent_Simon data.raw_data_path={DATA_FOLDERS[0]} data.interim_data_path={DATA_FOLDERS[1]}\n",
    ">```\n",
    "\n",
    "b) *Importing the functions themselves:*\n",
    ">```python\n",
    "import warnings\n",
    "from move.core import set_global_seed\n",
    "from move.data import io\n",
    "from move.tasks import analyze_latent\n",
    ">\n",
    ">warnings.filterwarnings(\"ignore\") # Ignore plotting warnings\n",
    "config = io.read_config(\"AMSC\", \"AMSC__latent_Simon\", \n",
    "                        f\"data.raw_data_path={DATA_FOLDERS[0]}\", \n",
    "                        f\"data.interim_data_path={DATA_FOLDERS[1]}\",\n",
    "                        f\"task.multiprocess={MULTIPROCESS}\")\n",
    ">\n",
    ">config#, \"data.interim_data_path='interim_data_rsd/'\") #AMSC__latent_B\n",
    ">set_global_seed(151)  # set seed to get same results, change to get slightly different plots\n",
    ">analyze_latent(config)\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over hyperparameters to increase reliability of the results obtaine from SHAP analysis.\n",
    "\n",
    "SHAP analysis is deeply influenced by the nature of the data, but more importantly it tells us what is important for a model with a given set of weights. There are many possible weight sets that can lead the model to similar losses, and also different sets of hyperparameters might lead to different outcomes. Hence, we trained 24 different models, with different sets of hyperparameters, to compensate for this uncertainty. The idea was to identify what variables were deemed important or were ignored regardless of specific training runs or hyperparameter choices. \n",
    "\n",
    "This is the list of hyperparameter combinations that we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_list = [(lat,hid,bet) for bet in [.01,.0001] for hid in [[200], [500], [1000], [2000]] for lat in [10,50,100]]\n",
    "for i,(lat,hid,bet) in enumerate(hyper_list):\n",
    "    print(i,(lat,hid,bet))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: To run the feature importance on this dataset in Terra we'll need a considerable RAM (ca.30GB). I ran it with 8 CPUs and 52 GB of RAM. 200-2000-.01 was run at the end (and skipped originally,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import warnings\n",
    "from move.core import set_global_seed\n",
    "from move.data import io\n",
    "from move.tasks import analyze_latent\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # Ignore plotting warnings\n",
    "\n",
    "# Hyperparameter set to explore:\n",
    "hyper_list = [(lat,hid,bet) for bet in [.01,.0001] for hid in [[200], [500], [1000], [2000]] for lat in [10,50,100]]\n",
    "! mkdir -p results_temp\n",
    "\n",
    "# Loop over hyperparameters:\n",
    "for i,(lat,hid,bet) in enumerate(hyper_list):\n",
    "    foldername = f\"latent_{lat}_{hid}_{bet}\"\n",
    "    if foldername not in os.listdir(\"./results_temp/\"):\n",
    "        !mkdir -p results_temp/{foldername}\n",
    "        !pwd\n",
    "        # Train again the model\n",
    "        ! rm ./results/latent_space/model.pt\n",
    "        ! move-dl data=AMSC task=AMSC__latent_Simon data.raw_data_path={DATA_FOLDERS[0]} data.interim_data_path={DATA_FOLDERS[1]} task.model.beta={bet} task.model.num_latent={lat} task.model.num_hidden={hid}\n",
    "        # Copy the results to the corresponding folder\n",
    "        !cp -r results/latent_space/ results_temp/{foldername} \n",
    "        print(f\"Finished {foldername}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"{foldername} was already created\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is aimed to read a file with feature names and create an alluvial (Sankey) plot. We will use it to show the fraction of features that appear in the SHAP analyses from different channels and compartments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package\n",
    "! pip install pyalluvial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an alluvial plot:**\n",
    "\n",
    "This code is inspired on the scripts Felipe Dos Santos used to create sankey plots for the Lipocyte Profiler paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pyalluvial.alluvial as alluvial\n",
    "from pathlib import Path\n",
    "\n",
    "# Define your file path and load data\n",
    "file = \"SHAP_LP_new.txt\"\n",
    "df = pd.read_csv(Path(\"./\") / file, sep='\\t', header=None) # Get only column 2\n",
    "feature_names = list(df.values[:, 1].flatten())\n",
    "\n",
    "def plot_alluvial(features):\n",
    "    # Create a DataFrame from the list of features\n",
    "    all_data = pd.DataFrame({'features': features})\n",
    "\n",
    "    # Define functions to categorize features\n",
    "    def categorize_compartment(feature):\n",
    "        if re.search(r'^Cells_', feature):\n",
    "            return 'Cells'\n",
    "        elif re.search(r'^Cytoplasm_', feature):\n",
    "            return 'Cytoplasm'\n",
    "        elif re.search(r'Nuclei', feature):\n",
    "            return 'Nuclei'\n",
    "        else:\n",
    "            return 'Other'\n",
    "\n",
    "    def categorize_feature_channel(feature):\n",
    "        if re.search(r'BODIPY', feature) and not re.search(r'AGP|Mito|DNA', feature):\n",
    "            return 'Lipid'\n",
    "        elif re.search(r'BODIPY', feature) and re.search(r'AGP|Mito', feature):\n",
    "            return 'Combination'\n",
    "        elif re.search(r'Mito', feature) and not re.search(r'AGP|BODIPY|DNA', feature):\n",
    "            return 'Mito'\n",
    "        elif re.search(r'Mito', feature) and re.search(r'AGP|DNA', feature):\n",
    "            return 'Combination'\n",
    "        elif re.search(r'AGP', feature) and not re.search(r'BODIPY|Mito|DNA', feature):\n",
    "            return 'AGP'\n",
    "        elif re.search(r'AGP', feature) and re.search(r'DNA', feature):\n",
    "            return 'Combination'\n",
    "        elif re.search(r'DNA', feature) and not re.search(r'AGP|BODIPY|Mito', feature):\n",
    "            return 'DNA'\n",
    "        else:\n",
    "            return 'Compartimental'\n",
    "\n",
    "    def categorize_feature_measure(feature):\n",
    "        if re.search(r'Texture', feature):\n",
    "            return 'Texture'\n",
    "        elif re.search(r'Intensity', feature):\n",
    "            return 'Intensity'\n",
    "        elif re.search(r'Granularity', feature):\n",
    "            return 'Granularity'\n",
    "        elif re.search(r'Correlation|Location|Angle|Distance|Touching|RadialDistribution', feature):\n",
    "            return 'Position'\n",
    "        else:\n",
    "            return 'Shape/Size/Count'\n",
    "\n",
    "    # Apply categorization\n",
    "    all_data['compartment'] = all_data['features'].apply(categorize_compartment)\n",
    "    all_data['feature_channel'] = all_data['features'].apply(categorize_feature_channel)\n",
    "    all_data['feature_measure'] = all_data['features'].apply(categorize_feature_measure)\n",
    "\n",
    "    df = all_data[['compartment', 'feature_channel', 'feature_measure']]\n",
    "\n",
    "    # Group by specific columns and count unique rows\n",
    "    grouped = df.groupby(['compartment', 'feature_channel', 'feature_measure']).size().reset_index(name='Count')\n",
    "\n",
    "    # Create a DataFrame of unique rows\n",
    "    unique_rows = df.drop_duplicates(subset=['compartment', 'feature_channel', 'feature_measure'])\n",
    "\n",
    "    # Merge the unique rows DataFrame with the count DataFrame\n",
    "    df = unique_rows.merge(grouped, on=['compartment', 'feature_channel', 'feature_measure'], how='left')\n",
    "\n",
    "    # Plot the alluvial diagram\n",
    "    fig = alluvial.plot(df=df, xaxis_names=['compartment', 'feature_channel', 'feature_measure'], \n",
    "                        y_name='Count', alluvium='compartment', cmap_name='Pastel1', figsize=(12, 10))\n",
    "\n",
    "    # Calculate the count sums\n",
    "    compartment_sum = df.groupby('compartment')['Count'].sum()\n",
    "    feature_channel_sum = df.groupby('feature_channel')['Count'].sum()\n",
    "    feature_measure_sum = df.groupby('feature_measure')['Count'].sum()\n",
    "\n",
    "    # Display the results in the console (optional)\n",
    "    print(\"Compartment Sums:\\n\", compartment_sum)\n",
    "    print(\"\\nFeature Channel Sums:\\n\", feature_channel_sum)\n",
    "    print(\"\\nFeature Measure Sums:\\n\", feature_measure_sum)\n",
    "\n",
    "    # Add annotations with count sums\n",
    "    def add_annotations(ax, sum_series, position_dict, color_dict):\n",
    "        for label, value in sum_series.items():\n",
    "            pos = position_dict[label]\n",
    "            color = color_dict[label]\n",
    "            ax.text(pos[0], pos[1], f'{value}', ha='center', va='top', fontsize=12, color=color)\n",
    "\n",
    "    # Define the color mapping\n",
    "    color_mapping = {\n",
    "        'AGP': '#f2cf41',\n",
    "        'Lipid': '#4dac26',\n",
    "        'DNA': '#65a6db',\n",
    "        'Mito': '#f56464',\n",
    "        'Combination': '#8d55c6',\n",
    "        'Compartimental': '#959ca3',\n",
    "        'Granularity': '#808080',\n",
    "        'Intensity': '#B8BAC5',\n",
    "        'Texture': '#383F59',\n",
    "        'Correlation': '#656B7D',\n",
    "        'Position': '#9AAFB5',\n",
    "        'Shape/Size/Count': '#708090',\n",
    "        'Cells': 'blue',\n",
    "        'Cytoplasm':'green'\n",
    "    }\n",
    "\n",
    "    # Mapping of positions where the text should be placed\n",
    "    compartment_positions = {\n",
    "        'Cells': [1, 0.2],\n",
    "        'Cytoplasm': [1, 0.1],\n",
    "        'Nuclei': [1, 0],\n",
    "        'Other': [1, -0.1]\n",
    "    }\n",
    "    feature_channel_positions = {\n",
    "        'Lipid': [2, 0.2],\n",
    "        'Mito': [2, 0.1],\n",
    "        'DNA': [2, 0],\n",
    "        'AGP': [2, -0.1],\n",
    "        'Combination': [2, -0.2],\n",
    "        'Compartimental': [2, -0.3]\n",
    "    }\n",
    "    feature_measure_positions = {\n",
    "        'Texture': [3, 0.2],\n",
    "        'Intensity': [3, 0.1],\n",
    "        'Granularity': [3, 0],\n",
    "        'Position': [3, -0.1],\n",
    "        'Shape/Size/Count': [3, -0.2]\n",
    "    }\n",
    "\n",
    "    ax = fig.gca()\n",
    "    plt.axis('off')\n",
    "    # Add the count sums as annotations\n",
    "    #add_annotations(ax, compartment_sum, compartment_positions, color_mapping)\n",
    "    #add_annotations(ax, feature_channel_sum, feature_channel_positions, color_mapping)\n",
    "    #add_annotations(ax, feature_measure_sum, feature_measure_positions, color_mapping)\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(Path(FIGURE_FOLDER) / \"Alluvial.png\", dpi=200)\n",
    "\n",
    "    # Prepare data for the return\n",
    "    node_dict = {name: i for i, name in enumerate(set(df.values[:, :-1].flatten()))}\n",
    "    source, target, value, edge_colors = [], [], [], []\n",
    "\n",
    "    for line in df.values:\n",
    "        for i in range(2):\n",
    "            source.append(node_dict[line[i]])\n",
    "            target.append(node_dict[line[i + 1]])\n",
    "            value.append(line[3])\n",
    "            edge_colors.append(color_mapping[line[0]])\n",
    "\n",
    "    return list(node_dict.keys()), source, target, value, list(color_mapping.values()), edge_colors\n",
    "\n",
    "# Calling the function with your feature names\n",
    "label, source, target, value, color, edge_colors = plot_alluvial(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = label,\n",
    "      color = color\n",
    "    ),\n",
    "    link = dict(\n",
    "      source = source, # indices correspond to labels, eg A1, A2, A1, B1, ...\n",
    "      target = target,\n",
    "      value = value,\n",
    "      color = edge_colors\n",
    "  ))])\n",
    "\n",
    "fig.update_layout(title_text=\"Basic Sankey Diagram\", font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary on PRS ranks\n",
    "PRS_summary = pd.read_csv(\"./SHAP_PRS.txt\", sep=\"\\t\", header=None)\n",
    "PRS_summary.columns = [\"Rank\", \"PRS\"]\n",
    "\n",
    "PRS_summary.groupby(by=[\"PRS\"]).sum().sort_values(by=[\"Rank\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Latent space analysis insights:__\n",
    "\n",
    "- The model organizes samples in different separable regions\n",
    "\n",
    "⚠️  If you get a similar error:\n",
    "\n",
    "```RuntimeError: Error(s) in loading state_dict for VAE:\n",
    "\tsize mismatch for encoderlayers.0.weight: copying a param with shape torch.Size([720, 1342]) from checkpoint, the shape in current model is torch.Size([900, 1342]). ```\n",
    "\n",
    "Erase the model.pt file in ```results/latent_space```. This can be done by setting REMOVE_LATENT_MODEL = True and running the second cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy results to workspace bucket\n",
    "\n",
    "We can now copy the obtained results back to the workspace bucket. The new files will appear in the data tab inside the files folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Save the results in the workspace bucket\n",
    "#! gsutil cp -r ./results/ {bucket}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying associations between features\n",
    "\n",
    "Lastly, we will use what the model has learnt to identify entangled or associated variables.\n",
    "\n",
    "When perturbing the input value of a certain feature for all samples, the latent representation of said samples will change, and so will their reconstructions. We can track the induced shifts in output values to identify the features that were affected the most when perturbing the original feature, at a cohort level.\n",
    "\n",
    "> ℹ️ We will use use the [Bayesian decision theory-based approach](https://www.nature.com/articles/s41587-022-01520-x#Sec15), presented in the Methods section of the original paper. We will also use an approach based on Kolmogorov-Smirnov distances between the feature reconstruction distributions, comparing them before and after perturbing an input feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over hyperparameter combinations to identify associations between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify associations loop for continuous datasets:\n",
    "\n",
    "####################### Hyper parameters #######################\n",
    "target_ds_list = [(str('Cyto'),\"plus_std\"), (str(\"FFA\"), 1), (str(\"cellType\"), \"sc\"), (str('PRS'),\"plus_std\")]\n",
    "NUM_REFITS = 24\n",
    "TASK = \"AMSC__id_assoc_bayes\"\n",
    "SIG_THR = 0.05 #(0.05 for bayes)\n",
    "NUM_EPOCHS = 300\n",
    "hyper_list = [(lat,hid,bet) for bet in [0.0001] for hid in [[500]] for lat in [50]] # Best performing model in validation\n",
    "\n",
    "###########################################################\n",
    "for i,(lat,hid,bet) in enumerate(hyper_list):\n",
    "    ! pwd\n",
    "    for TARGET_DS,TARGET_VALUE in target_ds_list:\n",
    "        ! mkdir -p results_temp_id_assoc_{TARGET_DS}\n",
    "        foldername = f\"id_assoc_{lat}_{hid}_{bet}_{TARGET_DS}_{TASK}\"\n",
    "        \n",
    "        if foldername not in os.listdir(f\"./results_temp_id_assoc_{TARGET_DS}/\"):\n",
    "            !mkdir -p results_temp_id_assoc_{TARGET_DS}/{foldername}\n",
    "\n",
    "            print(f\"Starting {foldername}\")\n",
    "            ! move-dl data=AMSC task={TASK} data.raw_data_path={DATA_FOLDERS[0]} data.interim_data_path={DATA_FOLDERS[1]} task.model.beta={bet} task.model.num_latent={lat} task.model.num_hidden={hid} task.target_dataset={TARGET_DS} task.num_refits={NUM_REFITS} task.target_value={TARGET_VALUE} task.sig_threshold={SIG_THR} task.training_loop.num_epochs={NUM_EPOCHS}\n",
    "\n",
    "            !cp -r results/identify_associations/ results_temp_id_assoc_{TARGET_DS}/{foldername} \n",
    "            print(f\"Finished {foldername}\")\n",
    "             \n",
    "        else:\n",
    "            print(f\"{foldername} was already created\")\n",
    "            \n",
    "    # Save models:\n",
    "    #! mkdir -p all_models/models_{lat}_{hid}_{bet}\n",
    "    #! cp -r {DATA_FOLDERS[1]}models all_models/models_{lat}_{hid}_{bet}\n",
    "    #! rm -r {DATA_FOLDERS[1]}models/*\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ./all_models/models_50_[500]_0.0001/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS here\n",
    "# Identify associations loop for continuous datasets:\n",
    "target_ds_list = [(str('PRS'),\"plus_std\"), (str('Cyto'),\"plus_std\")]\n",
    "NUM_REFITS = 1\n",
    "TASK = \"AMSC__id_assoc_ks\"\n",
    "SIG_THR = 0.999 #(0.05 for bayes)\n",
    "NUM_EPOCHS = 300\n",
    "\n",
    "#hyper_list = [(lat,hid,bet) for bet in [.01, .001] for hid in [[500], [1000]] for lat in [100]]\n",
    "#hyper_list = [(lat,hid,bet) for bet in [0.001] for hid in [[1000]] for lat in [100]]\n",
    "hyper_list = [(lat,hid,bet) for bet in [0.0001 for _ in range(24)] for hid in [[500]] for lat in [50]]\n",
    "\n",
    "for i,(lat,hid,bet) in enumerate(hyper_list):\n",
    "    ! pwd\n",
    "    ! rm -r {DATA_FOLDERS[1]}models\n",
    "    ! mkdir -p {DATA_FOLDERS[1]}models\n",
    "    ! cp ./all_models/models_50_[500]_0.0001/models/model_{lat}_{i}.pt {DATA_FOLDERS[1]}models/model_{lat}_0.pt\n",
    "    \n",
    "    for TARGET_DS,TARGET_VALUE in target_ds_list:\n",
    "        ! mkdir -p results_temp_id_assoc_{TARGET_DS}\n",
    "        foldername = f\"id_assoc_{lat}_{hid}_{bet}_{TARGET_DS}_{TASK}_{i}\"\n",
    "        \n",
    "        if foldername not in os.listdir(f\"./results_temp_id_assoc_{TARGET_DS}/\"):\n",
    "            !mkdir -p results_temp_id_assoc_{TARGET_DS}/{foldername}\n",
    "\n",
    "            print(f\"Starting {foldername}\")\n",
    "            ! move-dl data=AMSC task={TASK} data.raw_data_path={DATA_FOLDERS[0]} data.interim_data_path={DATA_FOLDERS[1]} task.model.beta={bet} task.model.num_latent={lat} task.model.num_hidden={hid} task.target_dataset={TARGET_DS} task.num_refits={NUM_REFITS} task.target_value={TARGET_VALUE} task.sig_threshold={SIG_THR} task.training_loop.num_epochs={NUM_EPOCHS}\n",
    "\n",
    "            !cp -r results/identify_associations/ results_temp_id_assoc_{TARGET_DS}/{foldername} \n",
    "            print(f\"Finished {foldername}\")\n",
    "             \n",
    "        else:\n",
    "            print(f\"{foldername} was already created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to compute KS threshold\n",
    "def ks_threshold(N, alpha):\n",
    "    return np.sqrt(-(1/N) * np.log(alpha/2))\n",
    "\n",
    "# Define the range for N and alpha\n",
    "N_values = np.linspace(100, 500, 10000)  # Avoid zero to prevent division by zero\n",
    "alpha_values = np.linspace(0.001, 1, 1000)  # Avoid zero for alpha to prevent log(0)\n",
    "\n",
    "# Create a meshgrid for N and alpha\n",
    "N_grid, alpha_grid = np.meshgrid(N_values, alpha_values)\n",
    "ks_thrs = ks_threshold(N_grid, alpha_grid)\n",
    "\n",
    "# Create a 2D heatmap\n",
    "fig, ax = plt.subplots(figsize=(7,3.5))\n",
    "c = ax.pcolormesh(N_grid, alpha_grid, ks_thrs, shading='auto', cmap='RdYlBu')\n",
    "\n",
    "# Add a color bar\n",
    "fig.colorbar(c, ax=ax, label='KS Threshold')\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('Number of Samples N')\n",
    "ax.set_ylabel('Significance Threshold')\n",
    "ax.set_title('Heatmap of KS Threshold')\n",
    "# Set x-axis to logarithmic scale\n",
    "#ax.set_xscale('log')\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIGURE_FOLDER + 'KS_thr_heatmap.png', dpi=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the final association lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "DATASET_LIST = [\"PRS\", \"Cyto\"]\n",
    "\n",
    "for DATASET in DATASET_LIST:\n",
    "    id_assoc_multi_run_path = Path(f\"./results_temp_id_assoc_{DATASET}/\")\n",
    "    combined_df = []\n",
    "    #Loop over different runs\n",
    "    for run_folder in os.scandir(id_assoc_multi_run_path):\n",
    "        run_idx = run_folder.name.split('_')[-1]\n",
    "        print(run_idx)\n",
    "        if os.path.isfile(Path(run_folder) / \"identify_associations/results_sig_assoc_ks.tsv\"):\n",
    "            df = pd.read_csv(Path(run_folder) / \"identify_associations/results_sig_assoc_ks.tsv\", header=0, index_col=0, sep=\"\\t\")\n",
    "            df['run_idx'] = run_idx\n",
    "            combined_df.append(df)\n",
    "\n",
    "    # Concatenate the results from different runs and rank them according to abs(KS distance).\n",
    "    combined_df = pd.concat(combined_df, ignore_index=True).sort_values('ks_distance', key=lambda x: abs(x), ascending=False)\n",
    "    combined_df.to_csv(id_assoc_multi_run_path / f\"{DATASET}_combined_ks_associations.csv\", sep=\"\\t\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ./results_temp_id_assoc_Cyto/id_assoc_50_[500]_0.0001_Cyto_AMSC__id_assoc_bayes/identify_associations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARC_BUCKET = \"gs://collaborations_marc/shared_2023/results_marc/results_residualized/associations_final_tables/KS/\"\n",
    "\n",
    "\n",
    "! gsutil cp ./PRS_combined_ks_associations.csv {MARC_BUCKET}\n",
    "#! gsutil -m cp -r ./results_cont_paper_II/ {MARC_BUCKET}\n",
    "#! gsutil cp -r ./results/* {MARC_BUCKET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with absolute KS values\n",
    "combined_df['abs_ks'] = combined_df['ks_distance'].abs()\n",
    "\n",
    "# Group by 'feature_a_name' and 'feature_b_name', then get index of max abs_ks\n",
    "idx = combined_df.groupby(['feature_a_name', 'feature_b_name'])['abs_ks'].idxmax()\n",
    "\n",
    "# Use these indices to select the rows\n",
    "combined_df = combined_df.loc[idx]\n",
    "\n",
    "# Reset the index\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "# If you want to drop the 'abs_ks' column we created\n",
    "combined_df= combined_df.drop('abs_ks', axis=1)\n",
    "\n",
    "thr = ks_threshold(N=269, alpha=.2)\n",
    "print(\"KS threshold\", thr)\n",
    "\n",
    "combined_df_ks = combined_df[combined_df['ks_distance'].abs() >= thr]\n",
    "\n",
    "\n",
    "df_bayes = pd.read_csv(f\"./results_temp_id_assoc_{DATASET}/id_assoc_50_[500]_0.0001_{DATASET}_AMSC__id_assoc_bayes/identify_associations/results_sig_assoc_bayes.tsv\", header=0, index_col=0, sep=\"\\t\")\n",
    "\n",
    "\n",
    "\n",
    "df_bayes\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "# Assuming you have two DataFrames: df1 and df2\n",
    "# Both containing the same two columns, let's call them 'col1' and 'col2'\n",
    "\n",
    "# Convert DataFrames to sets of tuples for comparison\n",
    "set1 = set(combined_df_ks[['feature_a_name', 'feature_b_name']].itertuples(index=False, name=None))\n",
    "set2 = set(df_bayes[['feature_a_name', 'feature_b_name']].itertuples(index=False, name=None))\n",
    "\n",
    "# Create the Venn diagram\n",
    "plt.figure(figsize=(10, 6))\n",
    "venn2([set1, set2], set_labels=('DataFrame 1', 'DataFrame 2'))\n",
    "\n",
    "plt.title(\"Overlap between DataFrame 1 and DataFrame 2\")\n",
    "plt.show()\n",
    "\n",
    "# To get the actual numbers:\n",
    "only_in_df1 = len(set1 - set2)\n",
    "only_in_df2 = len(set2 - set1)\n",
    "in_both = len(set1.intersection(set2))\n",
    "\n",
    "print(f\"Rows only in DataFrame 1: {only_in_df1}\")\n",
    "print(f\"Rows only in DataFrame 2: {only_in_df2}\")\n",
    "print(f\"Rows in both DataFrames: {in_both}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute KS values\n",
    "abs_ks = df['ks_distance'].abs()\n",
    "\n",
    "# Sort the values\n",
    "sorted_data = np.sort(abs_ks)\n",
    "\n",
    "# Calculate the proportional values of y\n",
    "y = np.arange(1, len(sorted_data)+1) / len(sorted_data)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sorted_data, y, lw=1)\n",
    "plt.xlabel('Absolute KS Distance')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Cumulative Distribution of Absolute KS Values')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add a line at y=0.5 for median\n",
    "plt.axhline(y=0.5, color='r', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D plots: latent space of 3 dimensions\n",
    "\n",
    "This cell is a replica of MOVE's code for the identification of associations. I stores the latent representations of the samples, to plot them subsequently. We set the latent nodes to three to be able to directly visualize the latent location of samples and their movements after the perturbations, i.e. without dimensionality reduction transformations like t-SNE or UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"identify_associations\"]\n",
    "\n",
    "from functools import reduce\n",
    "from os.path import exists\n",
    "from pathlib import Path\n",
    "from typing import Literal, Sized, Union, cast\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.stats import ks_2samp, pearsonr  # type: ignore\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from move.analysis.metrics import get_2nd_order_polynomial\n",
    "\n",
    "from move.conf.schema import (\n",
    "    IdentifyAssociationsBayesConfig,\n",
    "    IdentifyAssociationsConfig,\n",
    "    IdentifyAssociationsKSConfig,\n",
    "    IdentifyAssociationsTTestConfig,\n",
    "    MOVEConfig,\n",
    ")\n",
    "from move.core.logging import get_logger\n",
    "from move.core.typing import BoolArray, FloatArray, IntArray\n",
    "from move.data import io\n",
    "from move.data.dataloaders import MOVEDataset, make_dataloader\n",
    "from move.data.perturbations import (\n",
    "    ContinuousPerturbationType,\n",
    "    perturb_categorical_data,\n",
    "    perturb_continuous_data_extended,\n",
    ")\n",
    "from move.data.preprocessing import one_hot_encode_single\n",
    "from move.models.vae import VAE\n",
    "from move.visualization.dataset_distributions import (\n",
    "    plot_correlations,\n",
    "    plot_cumulative_distributions,\n",
    "    plot_feature_association_graph,\n",
    "    plot_reconstruction_movement,\n",
    ")\n",
    "\n",
    "TaskType = Literal[\"bayes\", \"ttest\", \"ks\"]\n",
    "CONTINUOUS_TARGET_VALUE = [\"minimum\", \"maximum\", \"plus_std\", \"minus_std\"]\n",
    "\n",
    "################################### Hyperparameters ##############################\n",
    "lat, hid, bet = (3,[1000],.1)\n",
    "TARGET_DS = \"cellType\"\n",
    "TARGET_VALUE= \"sc\"\n",
    "config = io.read_config(\"AMSC\",\n",
    "                        \"AMSC__id_assoc_bayes\",\n",
    "                        f\"data.raw_data_path={DATA_FOLDERS[0]}\",\n",
    "                        f\"data.interim_data_path={DATA_FOLDERS[1]}\",\n",
    "                        f\"task.model.beta={bet}\",\n",
    "                        f\"task.model.num_latent={lat}\",\n",
    "                        f\"task.model.num_hidden={hid}\",\n",
    "                        f\"task.num_refits=1\",\n",
    "                        f\"task.target_dataset={TARGET_DS}\", \n",
    "                        f\"task.target_value={TARGET_VALUE}\"\n",
    "                       )\n",
    "####################################################################################\n",
    "\n",
    "def _get_task_type(\n",
    "    task_config: IdentifyAssociationsConfig,\n",
    ") -> TaskType:\n",
    "    task_type = OmegaConf.get_type(task_config)\n",
    "    if task_type is IdentifyAssociationsBayesConfig:\n",
    "        return \"bayes\"\n",
    "    if task_type is IdentifyAssociationsTTestConfig:\n",
    "        return \"ttest\"\n",
    "    if task_type is IdentifyAssociationsKSConfig:\n",
    "        return \"ks\"\n",
    "    raise ValueError(\"Unsupported type of task!\")\n",
    "\n",
    "\n",
    "def _validate_task_config(\n",
    "    task_config: IdentifyAssociationsConfig, task_type: TaskType\n",
    ") -> None:\n",
    "    if not (0.0 <= task_config.sig_threshold <= 1.0):\n",
    "        raise ValueError(\"Significance threshold must be within [0, 1].\")\n",
    "    if task_type == \"ttest\":\n",
    "        task_config = cast(IdentifyAssociationsTTestConfig, task_config)\n",
    "        if len(task_config.num_latent) != 4:\n",
    "            raise ValueError(\"4 latent space dimensions required.\")\n",
    "\n",
    "\n",
    "def prepare_for_categorical_perturbation(\n",
    "    config: MOVEConfig,\n",
    "    interim_path: Path,\n",
    "    baseline_dataloader: DataLoader,\n",
    "    cat_list: list[FloatArray],\n",
    ") -> tuple[list[DataLoader], BoolArray, BoolArray,]:\n",
    "    \"\"\"\n",
    "    This function creates the required dataloaders and masks\n",
    "    for further categorical association analysis.\n",
    "\n",
    "    Args:\n",
    "        config: main configuration file\n",
    "        interim_path: path where the intermediate outputs are saved\n",
    "        baseline_dataloader: reference dataloader that will be perturbed\n",
    "        cat_list: list of arrays with categorical data\n",
    "\n",
    "    Returns:\n",
    "        dataloaders: all dataloaders, including baseline appended last.\n",
    "        nan_mask: mask for Nans\n",
    "        feature_mask: masks the column for the perturbed feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read original data and create perturbed datasets\n",
    "    task_config = cast(IdentifyAssociationsConfig, config.task)\n",
    "    logger = get_logger(__name__)\n",
    "\n",
    "    # Loading mappings:\n",
    "    mappings = io.load_mappings(interim_path / \"mappings.json\")\n",
    "    \n",
    "    target_mapping = mappings[task_config.target_dataset]\n",
    "    target_value = one_hot_encode_single(target_mapping, task_config.target_value)\n",
    "    logger.debug(\n",
    "        f\"Target value: {task_config.target_value} => {target_value.astype(int)[0]}\"\n",
    "    )\n",
    "    \n",
    "    dataloaders = perturb_categorical_data(\n",
    "        baseline_dataloader,\n",
    "        config.data.categorical_names,\n",
    "        task_config.target_dataset,\n",
    "        target_value,\n",
    "    )\n",
    "    \n",
    "    dataloaders.append(baseline_dataloader)\n",
    "\n",
    "    baseline_dataset = cast(MOVEDataset, baseline_dataloader.dataset)\n",
    "\n",
    "    assert baseline_dataset.con_all is not None\n",
    "    orig_con = baseline_dataset.con_all\n",
    "    nan_mask = (orig_con == 0).numpy()  # NaN values encoded as 0s\n",
    "    logger.debug(f\"# NaN values: {np.sum(nan_mask)}/{orig_con.numel()}\")\n",
    "\n",
    "    target_dataset_idx = config.data.categorical_names.index(task_config.target_dataset)\n",
    "    target_dataset = cat_list[target_dataset_idx]\n",
    "    feature_mask = np.all(target_dataset == target_value, axis=2)  # 2D: N x P\n",
    "    feature_mask |= np.sum(target_dataset, axis=2) == 0\n",
    "\n",
    "    return (\n",
    "        dataloaders,\n",
    "        nan_mask,\n",
    "        feature_mask,\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_for_continuous_perturbation(\n",
    "    config: MOVEConfig,\n",
    "    output_subpath: Path,\n",
    "    baseline_dataloader: DataLoader,\n",
    ") -> tuple[list[DataLoader], BoolArray, BoolArray,]:\n",
    "    \"\"\"\n",
    "    This function creates the required dataloaders and masks\n",
    "    for further continuous association analysis.\n",
    "\n",
    "    Args:\n",
    "        config:\n",
    "            main configuration file.\n",
    "        output_subpath:\n",
    "            path where the output plots for continuous analysis are saved.\n",
    "        baseline_dataloader:\n",
    "            reference dataloader that will be perturbed.\n",
    "\n",
    "    Returns:\n",
    "        dataloaders:\n",
    "            list with all dataloaders, including baseline appended last.\n",
    "        nan_mask:\n",
    "            mask for NaNs\n",
    "        feature_mask:\n",
    "            same as `nan_mask`, in this case.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read original data and create perturbed datasets\n",
    "    logger = get_logger(__name__)\n",
    "    task_config = cast(IdentifyAssociationsConfig, config.task)\n",
    "\n",
    "    dataloaders = perturb_continuous_data_extended(\n",
    "        baseline_dataloader,\n",
    "        config.data.continuous_names,\n",
    "        task_config.target_dataset,\n",
    "        cast(ContinuousPerturbationType, task_config.target_value),\n",
    "        output_subpath,\n",
    "    )\n",
    "    dataloaders.append(baseline_dataloader)\n",
    "\n",
    "    baseline_dataset = cast(MOVEDataset, baseline_dataloader.dataset)\n",
    "\n",
    "    assert baseline_dataset.con_all is not None\n",
    "    orig_con = baseline_dataset.con_all\n",
    "    nan_mask = (orig_con == 0).numpy()  # NaN values encoded as 0s\n",
    "    logger.debug(f\"# NaN values: {np.sum(nan_mask)}/{orig_con.numel()}\")\n",
    "    feature_mask = nan_mask\n",
    "\n",
    "    return (dataloaders, nan_mask, feature_mask)\n",
    "\n",
    "\n",
    "def _bayes_approach(\n",
    "    config: MOVEConfig,\n",
    "    task_config: IdentifyAssociationsBayesConfig,\n",
    "    train_dataloader: DataLoader,\n",
    "    baseline_dataloader: DataLoader,\n",
    "    dataloaders: list[DataLoader],\n",
    "    models_path: Path,\n",
    "    num_perturbed: int,\n",
    "    num_samples: int,\n",
    "    num_continuous: int,\n",
    "    nan_mask: BoolArray,\n",
    "    feature_mask: BoolArray,\n",
    ") -> tuple[Union[IntArray, FloatArray], ...]:\n",
    "\n",
    "    assert task_config.model is not None\n",
    "    device = torch.device(\"cuda\" if task_config.model.cuda == True else \"cpu\")\n",
    "\n",
    "    # Train models\n",
    "    logger = get_logger(__name__)\n",
    "    logger.info(\"Training models\")\n",
    "    mean_diff = np.zeros((num_perturbed, num_samples, num_continuous))\n",
    "    normalizer = 1 / task_config.num_refits\n",
    "\n",
    "    # Last appended dataloader is the baseline\n",
    "    baseline_dataset = cast(MOVEDataset, baseline_dataloader.dataset)\n",
    "    \n",
    "    for j in range(task_config.num_refits):\n",
    "        # Initialize model\n",
    "        model: VAE = hydra.utils.instantiate(\n",
    "            task_config.model,\n",
    "            continuous_shapes=baseline_dataset.con_shapes,\n",
    "            categorical_shapes=baseline_dataset.cat_shapes,\n",
    "        )\n",
    "        if j == 0:\n",
    "            logger.debug(f\"Model: {model}\")\n",
    "\n",
    "        # Train/reload model\n",
    "        model_path = models_path / f\"model_{task_config.model.num_latent}_{j}.pt\"\n",
    "        if model_path.exists():\n",
    "            logger.debug(f\"Re-loading refit {j + 1}/{task_config.num_refits}\")\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model.to(device)\n",
    "        else:\n",
    "            logger.debug(f\"Training refit {j + 1}/{task_config.num_refits}\")\n",
    "            model.to(device)\n",
    "            hydra.utils.call(\n",
    "                task_config.training_loop,\n",
    "                model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "            )\n",
    "            if task_config.save_refits:\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "        model.eval()\n",
    "\n",
    "        # Calculate baseline reconstruction\n",
    "        _, baseline_recon = model.reconstruct(baseline_dataloader)\n",
    "        latent_space_baseline = model.project(baseline_dataloader)\n",
    "        \n",
    "        min_feat, max_feat = np.zeros((num_perturbed, num_continuous)), np.zeros(\n",
    "            (num_perturbed, num_continuous)\n",
    "        )\n",
    "        min_baseline, max_baseline = np.min(baseline_recon, axis=0), np.max(\n",
    "            baseline_recon, axis=0\n",
    "        )\n",
    "\n",
    "        # Calculate perturb reconstruction => keep track of mean difference\n",
    "        for i in range(num_perturbed):\n",
    "            _, perturb_recon = model.reconstruct(dataloaders[i])\n",
    "            latent_space_perturbed = model.project(dataloaders[i])\n",
    "            \n",
    "            diff = perturb_recon - baseline_recon  # 2D: N x C\n",
    "            mean_diff[i, :, :] += diff * normalizer\n",
    "\n",
    "            min_perturb, max_perturb = np.min(perturb_recon, axis=0), np.max(\n",
    "                perturb_recon, axis=0\n",
    "            )\n",
    "            min_feat[i, :], max_feat[i, :] = np.min(\n",
    "                [min_baseline, min_perturb], axis=0\n",
    "            ), np.max([max_baseline, max_perturb], axis=0)\n",
    "\n",
    "    # Calculate Bayes factors\n",
    "    logger.info(\"Identifying significant features\")\n",
    "    bayes_k = np.empty((num_perturbed, num_continuous))\n",
    "    bayes_mask = np.zeros(np.shape(bayes_k))\n",
    "    for i in range(num_perturbed):\n",
    "        mask = feature_mask[:, [i]] | nan_mask  # 2D: N x C\n",
    "        diff = np.ma.masked_array(mean_diff[i, :, :], mask=mask)  # 2D: N x C\n",
    "        prob = np.ma.compressed(np.mean(diff > 1e-8, axis=0))  # 1D: C\n",
    "        bayes_k[i, :] = np.log(prob + 1e-8) - np.log(1 - prob + 1e-8)\n",
    "        if task_config.target_value in CONTINUOUS_TARGET_VALUE:\n",
    "            bayes_mask[i, :] = (\n",
    "                baseline_dataloader.dataset.con_all[0, :]\n",
    "                - dataloaders[i].dataset.con_all[0, :]\n",
    "            )\n",
    "\n",
    "    bayes_mask[bayes_mask != 0] = 1\n",
    "    bayes_mask = np.array(bayes_mask, dtype=bool)\n",
    "\n",
    "    # Calculate Bayes probabilities\n",
    "    bayes_abs = np.abs(bayes_k)\n",
    "    bayes_p = np.exp(bayes_abs) / (1 + np.exp(bayes_abs))  # 2D: N x C\n",
    "    bayes_abs[bayes_mask] = np.min(\n",
    "        bayes_abs\n",
    "    )  # Bring feature_i feature_i associations to minimum\n",
    "    sort_ids = np.argsort(bayes_abs, axis=None)[::-1]  # 1D: N x C\n",
    "    prob = np.take(bayes_p, sort_ids)  # 1D: N x C\n",
    "    logger.debug(f\"Bayes proba range: [{prob[-1]:.3f} {prob[0]:.3f}]\")\n",
    "\n",
    "    # Sort Bayes\n",
    "    bayes_k = np.take(bayes_k, sort_ids)  # 1D: N x C\n",
    "\n",
    "    # Calculate FDR\n",
    "    fdr = np.cumsum(1 - prob) / np.arange(1, prob.size + 1)  # 1D\n",
    "    idx = np.argmin(np.abs(fdr - task_config.sig_threshold))\n",
    "    logger.debug(f\"FDR range: [{fdr[0]:.3f} {fdr[-1]:.3f}]\")\n",
    "\n",
    "    return sort_ids[:idx], prob[:idx], fdr[:idx], bayes_k[:idx], latent_space_baseline, latent_space_perturbed\n",
    "\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "task_config = cast(IdentifyAssociationsConfig, config.task)\n",
    "task_type = _get_task_type(task_config)\n",
    "_validate_task_config(task_config, task_type)\n",
    "\n",
    "interim_path = Path(config.data.interim_data_path)\n",
    "\n",
    "models_path = interim_path / \"models\"\n",
    "if task_config.save_refits:\n",
    "    models_path.mkdir(exist_ok=True)\n",
    "\n",
    "output_path = Path(config.data.results_path) / \"identify_associations\"\n",
    "output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Load datasets:\n",
    "cat_list, cat_names, con_list, con_names = io.load_preprocessed_data(\n",
    "    interim_path,\n",
    "    config.data.categorical_names,\n",
    "    config.data.continuous_names,\n",
    ")\n",
    "\n",
    "train_dataloader = make_dataloader(\n",
    "    cat_list,\n",
    "    con_list,\n",
    "    shuffle=True,\n",
    "    batch_size=task_config.batch_size,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "con_shapes = [con.shape[1] for con in con_list]\n",
    "\n",
    "num_samples = len(cast(Sized, train_dataloader.sampler))  # N\n",
    "num_continuous = sum(con_shapes)  # C\n",
    "logger.debug(f\"# continuous features: {num_continuous}\")\n",
    "\n",
    "# Creating the baseline dataloader:\n",
    "baseline_dataloader = make_dataloader(\n",
    "    cat_list, con_list, shuffle=False, batch_size=task_config.batch_size\n",
    ")\n",
    "\n",
    "# Indentify associations between continuous features:\n",
    "logger.info(f\"Perturbing dataset: '{task_config.target_dataset}'\")\n",
    "if task_config.target_value in CONTINUOUS_TARGET_VALUE:\n",
    "    logger.info(f\"Beginning task: identify associations continuous ({task_type})\")\n",
    "    logger.info(f\"Perturbation type: {task_config.target_value}\")\n",
    "    output_subpath = Path(output_path) / \"perturbation_visualization\"\n",
    "    output_subpath.mkdir(exist_ok=True, parents=True)\n",
    "    (dataloaders, nan_mask, feature_mask,) = prepare_for_continuous_perturbation(\n",
    "        config, output_subpath, baseline_dataloader\n",
    "    )\n",
    "\n",
    "# Identify associations between categorical and continuous features:\n",
    "else:\n",
    "    logger.info(\"Beginning task: identify associations categorical\")\n",
    "    (dataloaders, nan_mask, feature_mask,) = prepare_for_categorical_perturbation(\n",
    "        config, interim_path, baseline_dataloader, cat_list\n",
    "    )\n",
    "\n",
    "    \n",
    "num_perturbed = len(dataloaders) - 1  # P\n",
    "logger.debug(f\"# perturbed features: {num_perturbed}\")\n",
    "\n",
    "################# APPROACH EVALUATION ##########################\n",
    "\n",
    "if task_type == \"bayes\":\n",
    "    task_config = cast(IdentifyAssociationsBayesConfig, task_config)\n",
    "    *_ , latent_space_baseline, latent_space_perturbed = _bayes_approach(\n",
    "        config,\n",
    "        task_config,\n",
    "        train_dataloader,\n",
    "        baseline_dataloader,\n",
    "        dataloaders,\n",
    "        models_path,\n",
    "        num_perturbed,\n",
    "        num_samples,\n",
    "        num_continuous,\n",
    "        nan_mask,\n",
    "        feature_mask,\n",
    "    )\n",
    "\n",
    "    extra_colnames = [\"proba\", \"fdr\", \"bayes_k\"]\n",
    "\n",
    "else:\n",
    "    raise ValueError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib.pyplot import cm\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "genes_of_interest = {\"ENSG00000099194\":\"SCD\",\n",
    "                     \"ENSG00000147872\":\"PLIN2\",\n",
    "                     \"ENSG00000079435\":\"LIPE\",\n",
    "                      \"ENSG00000181856\":\"GLUT4\",}\n",
    "                      #\"ENSG00000177370\":\"TIMM22\",\n",
    "                      #\"ENSG00000171105\":\"INSR\"}\n",
    "\n",
    "\n",
    "! mkdir -p figures\n",
    "feature_list = [\"Day\"] + list(genes_of_interest.keys())\n",
    "#feature_list = [\"ENSG00000079435\"]\n",
    "PRS_OF_INTEREST = \"gps_vatadjbmi3\"\n",
    "figure_path = Path(\"./figures/\")\n",
    "results_path = Path(\"./results/identify_associations/\")\n",
    "\n",
    "                                     \n",
    "def plot_3D_latent_and_displacement(\n",
    "    mu_baseline,\n",
    "    mu_perturbed,\n",
    "    feature_values,\n",
    "    feature_name,\n",
    "    show_baseline=True,\n",
    "    show_perturbed=True,\n",
    "    show_arrows=True,\n",
    "    step: int=1,\n",
    "    altitude: int=30,\n",
    "    azimuth: int=45,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the movement of the samples in the 3D latent space after perturbing one\n",
    "    input variable.\n",
    "\n",
    "    Args:\n",
    "        mu_baseline:\n",
    "            ND array with dimensions n_samples x n_latent_nodes containing\n",
    "            the latent representation of each sample\n",
    "        mu_perturbed:\n",
    "            ND array with dimensions n_samples x n_latent_nodes containing\n",
    "            the latent representation of each sample after perturbing the input\n",
    "        feature_values:\n",
    "            1D array with feature values to map to a colormap (\"bwr\"). Each sample is\n",
    "            colored according to its value for the feature of interest.\n",
    "        feature_name:\n",
    "            name of the feature mapped to a colormap\n",
    "        show_baseline:\n",
    "            plot orginal location of the samples in the latent space\n",
    "        show_perturbed:\n",
    "            plot final location (after perturbation) of the samples in latent space\n",
    "        show_arrows:\n",
    "            plot arrows from original to final location of each sample\n",
    "        angle:\n",
    "            elevation from dim1-dim2 plane for the visualization of latent space.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If latent space is not 3-dimensional (3 hidden nodes).\n",
    "    Returns:\n",
    "        Figure\n",
    "    \"\"\"\n",
    "    # construct cmap\n",
    "    #hex_colors= ['#d95f01','#1c9e78','#323896','#fec071']\n",
    "    #my_cmap = ListedColormap(hex_colors)\n",
    "    #my_cmap = \"inferno\"\n",
    "    #my_cmap = sns.color_palette(\"RdYlBu\", as_cmap=True)\n",
    "    my_cmap = sns.color_palette(\"seismic\", as_cmap=True)\n",
    "\n",
    "    eps = 1e-16\n",
    "    if [np.shape(mu_baseline)[1], np.shape(mu_perturbed)[1]] != [3, 3]:\n",
    "        raise ValueError(\n",
    "            \" The latent space must be 3-dimensional. Redefine num_latent to 3.\"\n",
    "        )\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(7, 7))\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    ax.view_init(altitude, azimuth)\n",
    "\n",
    "    if show_baseline:\n",
    "        vmin, vmax = np.min(feature_values[::step]), np.max(feature_values[::step])\n",
    "        abs_max = np.max([abs(vmin), abs(vmax)])\n",
    "        ax.scatter(\n",
    "            mu_baseline[::step, 0],\n",
    "            mu_baseline[::step, 1],\n",
    "            mu_baseline[::step, 2],\n",
    "            marker=\"o\",\n",
    "            c=feature_values[::step],\n",
    "            s=8,\n",
    "            lw=0,\n",
    "            cmap=my_cmap,\n",
    "            #vmin=0,\n",
    "            #vmax=1\n",
    "        )\n",
    "        ax.set_title(feature_name)\n",
    "        #plt.colorbar()  # Normalize(min(feature_values[::step]),max(feature_values[::step]))), ax=ax)\n",
    "    if show_perturbed:\n",
    "        ax.scatter(\n",
    "            mu_perturbed[::step, 0],\n",
    "            mu_perturbed[::step, 1],\n",
    "            mu_perturbed[::step, 2],\n",
    "            marker=\"o\",\n",
    "            c=feature_values[::step],\n",
    "            s=8,\n",
    "            label=\"perturbed\",\n",
    "            lw=0,\n",
    "        )\n",
    "    if show_arrows:\n",
    "        u = mu_perturbed[::step, 0] - mu_baseline[::step, 0]\n",
    "        v = mu_perturbed[::step, 1] - mu_baseline[::step, 1]\n",
    "        w = mu_perturbed[::step, 2] - mu_baseline[::step, 2]\n",
    "\n",
    "        module = np.sqrt(u * u + v * v + w * w)\n",
    "\n",
    "        mask = module > eps\n",
    "\n",
    "        max_u, max_v, max_w = np.max(abs(u)), np.max(abs(v)), np.max(abs(w))\n",
    "        # Arrow colors will be weighted contributions of red -> dim1, green -> dim2, and blue-> dim3. I.e. purple arrow means movement in dims 1 and 3\n",
    "        colors = [\n",
    "            (abs(du) / max_u, abs(dv) / max_v, abs(dw) / max_w, 0.7)\n",
    "            for du, dv, dw in zip(u, v, w)\n",
    "        ]\n",
    "        ax.quiver(\n",
    "            mu_baseline[::step, 0][mask],\n",
    "            mu_baseline[::step, 1][mask],\n",
    "            mu_baseline[::step, 2][mask],\n",
    "            u[mask],\n",
    "            v[mask],\n",
    "            w[mask],\n",
    "            color=colors,\n",
    "            lw=.8,\n",
    "            )  # alpha=(1-module/np.max(module))**6, arrow_length_ratio=0)\n",
    "        # help(ax.quiver)\n",
    "    ax.set_xlabel(\"Dim 1\")\n",
    "    ax.set_ylabel(\"Dim 2\")\n",
    "    ax.set_zlabel(\"Dim 3\")\n",
    "    # ax.set_axis_off()\n",
    "\n",
    "    return fig\n",
    "\n",
    "for feature in feature_list:\n",
    "    dataset = feature if \"ENSG\" not in feature else \"RNA\" \n",
    "    feature_values = pd.read_csv(f\"./data_residualized/{dataset}.tsv\", sep=\"\\t\")\n",
    "    if feature == \"cellType\":\n",
    "        feature_values = [0 if celltype == 'sc' else 1 for celltype in feature_values['cellType']]\n",
    "    elif feature == \"Day\":\n",
    "        feature_values = [day for day in feature_values['Day']]\n",
    "        \n",
    "    elif feature == \"PRS\":\n",
    "        feature_values = [feature_values[PRS_OF_INTEREST]]\n",
    "    else: # FFA and GENES do already contain the target values in the column\n",
    "        feature_values = feature_values[feature]\n",
    "        \n",
    "    # # Plot latent space:\n",
    "    pic_num = 0\n",
    "    n_pictures = 10\n",
    "    \n",
    "\n",
    "    for azimuth, altitude in zip(\n",
    "        np.linspace(0, 10, n_pictures), np.linspace(15, 45, n_pictures)\n",
    "    ):\n",
    "                     \n",
    "        title = feature if \"ENSG\" not in feature else genes_of_interest[feature]\n",
    "                     \n",
    "        fig = plot_3D_latent_and_displacement(\n",
    "            latent_space_baseline,\n",
    "            latent_space_perturbed,\n",
    "            feature_values=feature_values,\n",
    "            feature_name=f\"Sample movement\",\n",
    "            show_baseline=True,\n",
    "            show_perturbed=False,\n",
    "            show_arrows=True,\n",
    "            step=1,\n",
    "            altitude=altitude,\n",
    "            azimuth=azimuth,\n",
    "        )\n",
    "\n",
    "        fig.savefig(figure_path / f\"3D_latent_movement_{pic_num}_arrows.png\", dpi=100)\n",
    "        plt.close(fig)\n",
    "\n",
    "    \n",
    "        fig = plot_3D_latent_and_displacement(\n",
    "            latent_space_baseline,\n",
    "            latent_space_perturbed,\n",
    "            feature_values=feature_values,\n",
    "            feature_name=f\"{title}\",\n",
    "            show_baseline=True,\n",
    "            show_perturbed=False,\n",
    "            show_arrows=False,\n",
    "            altitude=altitude,\n",
    "            azimuth=azimuth,\n",
    "        )\n",
    "        fig.savefig(\n",
    "            figure_path / f\"3D_latent_movement_{pic_num}_perturbed_feature.png\", dpi=100\n",
    "        )\n",
    "        plt.close(fig)\n",
    "        pic_num += 1\n",
    "\n",
    "    for plot_type in [\"arrows\", \"perturbed_feature\"]:\n",
    "        frames = [\n",
    "            Image.open(figure_path / f\"3D_latent_movement_{pic_num}_{plot_type}.png\")\n",
    "            for pic_num in range(n_pictures)\n",
    "        ]  # sorted(glob.glob(\"*3D_latent*\"))]\n",
    "        frames[0].save(\n",
    "            figure_path / f\"{plot_type}_{title}.gif\",\n",
    "            format=\"GIF\",\n",
    "            append_images=frames[1:],\n",
    "            save_all=True,\n",
    "            duration=75,\n",
    "            loop=0,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example image of the latent space with three coordinates or nodes.**\n",
    "When perturbing the cell type, i.e. adding the flag \"Subcutaneous\" to the samples that were from the visceral depot, we can see that the samples never abandon the original cluster, but they still move in the direction towards sc samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(n_pictures):\n",
    "    image_path = f\"./figures/3D_latent_movement_{i}_arrows.png\"\n",
    "    image = mpimg.imread(image_path)\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARC_BUCKET = \"gs://collaborations_marc/shared_2023/results_marc/results_residualized/figures_cell_type/\"\n",
    "\n",
    "\n",
    "! gsutil -m cp -r ./figures/* {MARC_BUCKET}\n",
    "#! gsutil -m cp -r ./results_cont_paper_II/ {MARC_BUCKET}\n",
    "#! gsutil cp -r ./results/* {MARC_BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRS ranking visualization\n",
    "\n",
    "We will gather the feature importance tsv files, compute SHAP values, and create violin plots to check the median ranking of a PRS across different models architectures. PRSs that are ranked high or low systematically are the ones that matter the most/less to the model to characterize the studied system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "path = Path(\"./results_temp/\")\n",
    "\n",
    "ls = \"latent_space\"\n",
    "\n",
    "feature_set = [\"Cyto\",\"Cells\",\"RNA\",\"PRS\"]\n",
    "\n",
    "for folder in os.listdir(path):\n",
    "    print(folder)\n",
    "    _, lat, hid, beta = folder.split(\"_\")\n",
    "    for feature in feature_set:\n",
    "        df = pd.read_csv(f\"./results_temp/latent_{lat}_{hid}_{beta}/latent_space/feat_importance_{feature}.tsv\", sep=\"\\t\").set_index(\"sample\")\n",
    "        #print(df)\n",
    "        #df = pd.read_csv(f\"./results_temp_II/{folder}/latent_space/feat_importance_{feature}.tsv\")#path / folder / / f\"feat_importance_{feature}.tsv\")\n",
    "        #print(df)\n",
    "        top10_ids = np.argsort(np.sum(np.abs(df.values), axis=0))[::-1]\n",
    "        if \"PRS\" not in feature:\n",
    "            top10_ids = top10_ids[:10]\n",
    "            \n",
    "        order = np.take(df.columns, top10_ids)\n",
    "        savepath = \"/home/jupyter/Characterizing AMSCs using MOVE/edit/\"\n",
    "        gene_shap = \"SHAP_genes_new.txt\"\n",
    "        LP_shap = \"SHAP_LP_new.txt\"\n",
    "        PRS_shap = \"SHAP_PRS_new.txt\"\n",
    "\n",
    "        with open(savepath + gene_shap, 'a') as g, open(savepath + LP_shap, 'a') as l, open(savepath + PRS_shap, 'a') as p:\n",
    "            for o, feature in enumerate(order):\n",
    "                if \"ENS\" in feature:\n",
    "                    g.write(f\"{o}\\t{feature}\\n\")\n",
    "                elif (\"Cytoplasm\" in feature) or (\"Cells\" in feature):\n",
    "                    l.write(f\"{o}\\t{feature}\\n\")\n",
    "                elif (\"gps_\" in feature) or (\"prs.\" in feature):\n",
    "                    p.write(f\"{o}\\t{feature}\\n\")\n",
    "            #print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = Path(\"./\")\n",
    "filename = \"SHAP_PRS_new.txt\"\n",
    "\n",
    "colnames= [\"Rank\",\"PRS\"]\n",
    "\n",
    "df = pd.read_csv(path / filename, sep=\"\\t\", names=colnames)\n",
    "\n",
    "df_violin = pd.DataFrame()\n",
    "\n",
    "df_violin[\"Lists\"] = df.groupby(\"PRS\")[\"Rank\"].apply(list)\n",
    "\n",
    "df_violin['median'] = [np.median(l) for l in df_violin['Lists']]\n",
    "#df_violin[\"mean\"] = df_violin[\"Lists\"].apply(lambda x: sum(x) / len(x))\n",
    "df_violin = df_violin.sort_values(by=\"median\")\n",
    "#df_violin = df_violin.drop(columns=\"median\")\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "parts = plt.violinplot(df_violin[\"Lists\"], showmedians=True, showextrema=False)\n",
    "\n",
    "for pc in parts['bodies']:\n",
    "    pc.set_facecolor('#c9dbeb')\n",
    "    #pc.set_edgecolor('black')  # Optional: change edge color if desired\n",
    "    pc.set_alpha(1)  # Optional: adjust transparency\n",
    "    \n",
    "# Customize median points\n",
    "median_line = parts['cmedians']\n",
    "median_line.set_linewidth(0)  # Hide the line\n",
    "scatter_x = range(1, len(df_violin[\"Lists\"]) + 1)\n",
    "scatter_y = [np.median(subset) for subset in df_violin[\"Lists\"]]\n",
    "plt.scatter(scatter_x, scatter_y, color='black', marker='D', s=5, zorder=3)\n",
    "\n",
    "plt.xticks(np.arange(1,18), labels=df_violin.index, rotation=90)\n",
    "plt.ylabel(\"Rank\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(Path(FIGURE_FOLDER) / \"PRS_SHAP.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving results: \n",
    "\n",
    "- Locally\n",
    "- In the google cloud external bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARC_BUCKET = \"gs://collaborations_marc/shared_2023/results_marc/results_residualized/\"\n",
    "\n",
    "\n",
    "! gsutil -m cp -r ./paper_figures/ {MARC_BUCKET}\n",
    "#! gsutil -m cp -r ./results_cont_paper_II/ {MARC_BUCKET}\n",
    "#! gsutil cp -r ./results/* {MARC_BUCKET}\n",
    "#! gsutil cp /home/jupyter/Characterizing\\ AMSCs\\ using\\ MOVE/edit/results/identify_associations/results_sig_assoc_bayes.tsv {MARC_BUCKET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls {MARC_BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sythetic datasets benchmark\n",
    "\n",
    "### Synthetic dataset creation\n",
    "\n",
    "A synthetic dataset is created as a multivariate gaussian, where different features are different components of the Gaussian and each sample is a draw from the distribution. Explicit stronger correlations can be added by defining some features to be linear combinations of others. Categorical variables (binary) can be obtained by setting negative values to zero and positive values to one for a given feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_sparse_spd_matrix\n",
    "from move.data.preprocessing import scale\n",
    "import os\n",
    "import io\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "from itertools import chain\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Latin Modern Roman' #Font for the plots\n",
    "\n",
    "################################ Functions ####################################\n",
    "\n",
    "def get_feature_names(settings):\n",
    "    all_feature_names = [\n",
    "        f\"{key}_{i+1}\"\n",
    "        for key in settings.keys()\n",
    "        for i in range(settings[key][\"features\"])\n",
    "    ]\n",
    "    return all_feature_names\n",
    "\n",
    "\n",
    "def create_mean_profiles(settings):\n",
    "    feature_means = []\n",
    "    for key in settings.keys():\n",
    "        mean = settings[key][\"offset\"]\n",
    "        for freq, coef in zip(\n",
    "            settings[key][\"frequencies\"], settings[key][\"coefficients\"]\n",
    "        ):\n",
    "            mean += coef * (\n",
    "                np.sin(\n",
    "                    freq * np.arange(settings[key][\"features\"]) + settings[key][\"phase\"]\n",
    "                )\n",
    "                + 1\n",
    "            )\n",
    "        feature_means.extend(list(mean))\n",
    "    return feature_means\n",
    "\n",
    "\n",
    "def create_ground_truth_correlations_file(correlations):\n",
    "    sort_ids = np.argsort(abs(correlations), axis=None)[::-1]  # 1D: N x C\n",
    "    corr = np.take(correlations, sort_ids)  # 1D: N x C\n",
    "    sig_ids = sort_ids[abs(corr) > COR_THRES]\n",
    "    sig_ids = np.vstack(\n",
    "        (sig_ids // len(all_feature_names), sig_ids % len(all_feature_names))\n",
    "    ).T\n",
    "    associations = pd.DataFrame(sig_ids, columns=[\"feature_a_id\", \"feature_b_id\"])\n",
    "    a_df = pd.DataFrame(dict(feature_a_name=all_feature_names))\n",
    "    a_df.index.name = \"feature_a_id\"\n",
    "    a_df.reset_index(inplace=True)\n",
    "    b_df = pd.DataFrame(dict(feature_b_name=all_feature_names))\n",
    "    b_df.index.name = \"feature_b_id\"\n",
    "    b_df.reset_index(inplace=True)\n",
    "    associations = associations.merge(a_df, on=\"feature_a_id\", how=\"left\").merge(\n",
    "        b_df, on=\"feature_b_id\", how=\"left\"\n",
    "    )\n",
    "    associations[\"Correlation\"] = corr[abs(corr) > COR_THRES]\n",
    "    associations = associations[\n",
    "        associations.feature_a_id > associations.feature_b_id\n",
    "    ]  # Only one half of the matrix\n",
    "    return associations\n",
    "\n",
    "\n",
    "def plot_score_matrix(\n",
    "    array, feature_names, cmap=\"bwr\", vmin=None, vmax=None, label_step=5\n",
    "):\n",
    "    if vmin is None:\n",
    "        vmin = np.min(array)\n",
    "    elif vmax is None:\n",
    "        vmax = np.max(array)\n",
    "    # if ax is None:\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(array, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    plt.xticks(\n",
    "        np.arange(0, len(feature_names), label_step),\n",
    "        feature_names[::label_step],\n",
    "        fontsize=8,\n",
    "        rotation=90,\n",
    "    )\n",
    "    plt.yticks(\n",
    "        np.arange(0, len(feature_names), label_step),\n",
    "        feature_names[::label_step],\n",
    "        fontsize=8,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    # ax\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_feature_profiles(dataset, feature_means):\n",
    "    ## Plot profiles\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    plt.plot(\n",
    "        np.arange(len(feature_means)), feature_means, lw=1, marker=\".\", markersize=0\n",
    "    )\n",
    "    for sample in dataset:\n",
    "        plt.plot(\n",
    "            np.arange(len(feature_means)), sample, lw=0.1, marker=\".\", markersize=0\n",
    "        )\n",
    "    plt.xlabel(\"Feature number\")\n",
    "    plt.ylabel(\"Count number\")\n",
    "    plt.title(\"Patient specific profiles\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_feature_correlations(dataset, pairs_2_plot):\n",
    "    fig = plt.figure()\n",
    "    for f1, f2 in pairs_2_plot:\n",
    "        plt.plot(\n",
    "            dataset[:, f1],\n",
    "            dataset[:, f2],\n",
    "            lw=0,\n",
    "            marker=\".\",\n",
    "            markersize=1,\n",
    "            label=f\"{correlations[f1,f2]:.2f}\",\n",
    "        )\n",
    "    plt.xlabel(\"Feature A\")\n",
    "    plt.ylabel(\"Feature B\")\n",
    "    plt.legend(\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, -0.1),\n",
    "        fancybox=True,\n",
    "        shadow=True,\n",
    "        ncol=5,\n",
    "    )\n",
    "    plt.title(\"Feature correlations\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def save_splitted_datasets(\n",
    "    settings: dict, PROJECT_NAME, dataset, all_feature_names, n_samples, outpath\n",
    "):\n",
    "    # Save index file\n",
    "    index = pd.DataFrame({\"ID\": list(np.arange(1, n_samples + 1))})\n",
    "    index.to_csv(outpath / f\"random.{PROJECT_NAME}.ids.txt\", index=False, header=False)\n",
    "    # Save continuous files\n",
    "    df = pd.DataFrame(\n",
    "        dataset, columns=all_feature_names, index=list(np.arange(1, n_samples + 1))\n",
    "    )\n",
    "    cum_feat = 0\n",
    "    for key in settings.keys():\n",
    "        df_feat = settings[key][\"features\"]\n",
    "        df_cont = df.iloc[:, cum_feat : cum_feat + df_feat]\n",
    "        df_cont.insert(0, \"ID\", np.arange(1, n_samples + 1))\n",
    "        df_cont.to_csv(\n",
    "            outpath / f\"random.{PROJECT_NAME}.{key}.tsv\", sep=\"\\t\", index=False\n",
    "        )\n",
    "        cum_feat += df_feat\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Hyperparameters ####################################\n",
    "\n",
    "PROJECT_NAME = \"random_all_sim\"\n",
    "MODE = \"linear\"  # \"non-linear\"\n",
    "HIGH_CORR = True\n",
    "SEED_1 = 1234\n",
    "np.random.seed(SEED_1)\n",
    "rnd.seed(SEED_1)\n",
    "\n",
    "COV_ALPHA = .99 #0.99 #.01 \n",
    "N_SAMPLES = 5000\n",
    "\n",
    "SETTINGS = {\n",
    "    \"Continuous_A\": {\n",
    "        \"features\": 50,\n",
    "        \"frequencies\": [0.002, 0.01, 0.02],\n",
    "        \"coefficients\": [500, 100, 50],\n",
    "        \"phase\": 0,\n",
    "        \"offset\": 700,\n",
    "    },\n",
    "    \"Continuous_B\": {\n",
    "        \"features\": 100,\n",
    "        \"frequencies\": [0.001, 0.05, 0.08],\n",
    "        \"coefficients\": [80, 20, 10],\n",
    "        \"phase\": np.pi / 2,\n",
    "        \"offset\": 300,\n",
    "    },\n",
    "    \"Categorical_A\": {\n",
    "        \"features\": 1,\n",
    "        \"frequencies\": [0.1, 0.5, 0.8],\n",
    "        \"coefficients\": [.2, .1, .05],\n",
    "        \"phase\": np.pi / 2,\n",
    "        \"offset\": 10,\n",
    "    },\n",
    "        \"Categorical_B\": {\n",
    "        \"features\": 1,\n",
    "        \"frequencies\": [0.01, 0.5, 0.08],\n",
    "        \"coefficients\": [10, .1, .05],\n",
    "        \"phase\": np.pi,\n",
    "        \"offset\": 10,\n",
    "    }\n",
    "}\n",
    "\n",
    "COR_THRES = 0.02\n",
    "PAIRS_OF_INTEREST = [(1,2),(3,4)]  # ,(77,75),(99,70),(38,2),(67,62)]\n",
    "\n",
    "# Path to store output files\n",
    "outpath = Path(\"synthetic_data\")\n",
    "outpath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "\n",
    "################################## Main script ##################################\n",
    "# %%\n",
    "# Add all datasets in a single matrix:\n",
    "all_feature_names = get_feature_names(SETTINGS)\n",
    "feat_means = create_mean_profiles(SETTINGS)\n",
    "\n",
    "# %%\n",
    "###### Covariance matrix definition ######\n",
    "if MODE == \"linear\":\n",
    "    covariance_matrix = make_sparse_spd_matrix(\n",
    "        dim=len(all_feature_names),\n",
    "        alpha=COV_ALPHA,\n",
    "        smallest_coef=0,\n",
    "        largest_coef=1,\n",
    "        norm_diag=False,\n",
    "        random_state=SEED_1,\n",
    "    )\n",
    "elif MODE == \"non-linear\":\n",
    "    covariance_matrix = np.identity(len(all_feature_names))\n",
    "\n",
    "ABS_MAX = np.max(abs(covariance_matrix))\n",
    "fig = plot_score_matrix(\n",
    "    covariance_matrix, all_feature_names, vmin=-ABS_MAX, vmax=ABS_MAX\n",
    ")\n",
    "fig.savefig(outpath / f\"Covariance_matrix_{PROJECT_NAME}.png\")\n",
    "\n",
    "#    dataset = np.array(\n",
    "#        [\n",
    "#            list(np.random.multivariate_normal(feat_means, covariance_matrix))\n",
    "#            for _ in range(N_SAMPLES)\n",
    "#        ]\n",
    "#    )\n",
    "\n",
    "dataset = np.random.multivariate_normal(feat_means, covariance_matrix, N_SAMPLES)\n",
    "\n",
    "\n",
    "\n",
    "# Add non-linearities\n",
    "if MODE == \"non-linear\":\n",
    "    for i, j in PAIRS_OF_INTEREST:\n",
    "        freq = np.random.choice([4, 5, 6])\n",
    "        dataset[:, i] += np.sin(freq * dataset[:, j])\n",
    "\n",
    "#scaled_dataset, _ = scale(dataset)\n",
    "# No scaling in the dataset creation! It will be handled in preprocessing.\n",
    "scaled_dataset = dataset\n",
    "\n",
    "if HIGH_CORR: # The last half of the features are combinations of the first half:\n",
    "    for i in range(scaled_dataset.shape[1]//2):\n",
    "        col_1 = np.random.choice(range(scaled_dataset.shape[1]//2))\n",
    "        col_2 = np.random.choice(range(scaled_dataset.shape[1]//2))\n",
    "        scaled_dataset[:,i+scaled_dataset.shape[1]//2] = (scaled_dataset[:,col_1]+scaled_dataset[:,col_2])/2 + np.random.normal()\n",
    "\n",
    "# Binarize the categorical dataset\n",
    "NUM_CAT = SETTINGS[\"Categorical_A\"][\"features\"] + SETTINGS[\"Categorical_B\"][\"features\"]\n",
    "columns_to_binarize = scaled_dataset[:,-NUM_CAT:]\n",
    "\n",
    "# Compute the mean of each of the categorical columns    \n",
    "means = columns_to_binarize.mean(axis=0)\n",
    "\n",
    "# Apply the binarization\n",
    "scaled_dataset[:,-NUM_CAT:] = (columns_to_binarize > means).astype(int)\n",
    "\n",
    "print(np.min(scaled_dataset),np.max(scaled_dataset))\n",
    "# Actual correlations:\n",
    "# correlations = np.empty(np.shape(covariance_matrix))\n",
    "# for ifeat in range(len(covariance_matrix)):\n",
    "#     for jfeat in range(len(covariance_matrix)):\n",
    "#         correlations[ifeat, jfeat] = pearsonr(dataset[:, ifeat], dataset[:, jfeat])[\n",
    "#             0\n",
    "#         ]\n",
    "\n",
    "correlations = np.corrcoef(scaled_dataset, rowvar=False)\n",
    "fig = plot_score_matrix(correlations, all_feature_names, vmin=-1, vmax=1, label_step=20)\n",
    "fig.savefig(outpath / f\"Correlations_{PROJECT_NAME}.png\", dpi=200)\n",
    "\n",
    "# Sort correlations by absolute value\n",
    "associations = create_ground_truth_correlations_file(correlations)\n",
    "associations.to_csv(outpath / f\"changes.{PROJECT_NAME}.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Plot feature profiles per sample\n",
    "fig = plot_feature_profiles(scaled_dataset, feat_means)\n",
    "fig.savefig(outpath / \"Multi-omic_profiles.png\")\n",
    "\n",
    "## Plot correlations\n",
    "fig = plot_feature_correlations(dataset, PAIRS_OF_INTEREST)\n",
    "fig.savefig(outpath / \"Feature_correlations.png\")\n",
    "\n",
    "fig = plot_feature_correlations(scaled_dataset, PAIRS_OF_INTEREST)\n",
    "fig.savefig(outpath / \"Feature_correlations_scaled.png\")\n",
    "\n",
    "# Write tsv files with feature values for all samples in both datasets:\n",
    "save_splitted_datasets(\n",
    "    SETTINGS, PROJECT_NAME, scaled_dataset, all_feature_names, N_SAMPLES, outpath\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running MOVE on synthetic data\n",
    "\n",
    "# Note that we will change the significance threshld for the KS to get more associations.\n",
    "# Encode data\n",
    "! move-dl task=encode_data data=random_continuous_paper data.raw_data_path='synthetic_data'\n",
    "\n",
    "# Identify assoc bayes\n",
    "! move-dl task=random_continuous_paper__id_assoc_bayes data=random_continuous_paper data.raw_data_path='synthetic_data'\n",
    "\n",
    "# Identify assoc KS\n",
    "! move-dl task=random_continuous_paper__id_assoc_ks data=random_continuous_paper data.raw_data_path='synthetic_data'\n",
    "\n",
    "# Identify assoc t-test\n",
    "! move-dl task=random_continuous_paper__id_assoc_ttest data=random_continuous_paper data.raw_data_path='synthetic_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing methods to identify associations\n",
    "\n",
    "We can run MOVE using the three methods to identify associations (t-test, Bayes and KS). The resulting tsv files can be compared with the ground truths file to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from matplotlib_venn import venn2, venn3\n",
    "from upsetplot import UpSet\n",
    "from matplotlib import cm\n",
    "\n",
    "# Create a colormap from seaborn\n",
    "cmap = sns.color_palette(\"Dark2\", 3,  as_cmap=False)  # False for a list of colors\n",
    "\n",
    "# Set the color cycle for matplotlib using the colormap\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=cmap)\n",
    "##################################### Functions #############################################\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          cmap=None,\n",
    "                          normalize=False):\n",
    "    \n",
    "    \"\"\" Function that plots the confusion matrix given cm. Mattias Ohlsson's code extended.\"\"\"\n",
    "\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    fig = plt.figure(figsize=(4,3))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=0, fontsize=12)\n",
    "        plt.yticks(tick_marks, target_names,fontsize=12)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=14)\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=14)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=14)\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass), fontsize=14)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def classify_associations(target_file, assoc_tuples):\n",
    "    self_assoc = 0 # Self associations\n",
    "    found_assoc_dict = {}\n",
    "    false_assoc_dict = {}\n",
    "    tp_fp = np.array([[0,0]])\n",
    "    with open (target_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line[0] != \"f\":\n",
    "                splitline = line.strip().split(\"\\t\") \n",
    "                feat_a = splitline[2]\n",
    "                feat_b = splitline[3]\n",
    "                score = abs(float(splitline[5]))\n",
    "                if feat_a == feat_b: # Self associations will not be counted\n",
    "                    self_assoc += 1\n",
    "                else:\n",
    "                    if (feat_a,feat_b) in assoc_tuples:\n",
    "                        found_assoc_dict[(feat_a,feat_b)] = score\n",
    "                        if (feat_b,feat_a) not in found_assoc_dict.keys(): #If we had not found it yet\n",
    "                            tp_fp = np.vstack((tp_fp,tp_fp[-1]+[0,1]))\n",
    "                    elif (feat_a,feat_b) not in assoc_tuples:\n",
    "                        false_assoc_dict[(feat_a,feat_b)] = score\n",
    "                        if (feat_b,feat_a) not in false_assoc_dict.keys():\n",
    "                            tp_fp = np.vstack((tp_fp,tp_fp[-1]+[1,0]))\n",
    "\n",
    "    # Remove duplicated associations:\n",
    "    for (i,j) in list(found_assoc_dict.keys()):\n",
    "        if (j,i) in found_assoc_dict.keys():\n",
    "            del found_assoc_dict[(j,i)] # remove the weakest direction for the association\n",
    "\n",
    "\n",
    "    for (i,j) in list(false_assoc_dict.keys()):\n",
    "        if (j,i) in false_assoc_dict.keys():\n",
    "            del false_assoc_dict[(i,j)]\n",
    "\n",
    "    return self_assoc, found_assoc_dict, false_assoc_dict, tp_fp\n",
    "\n",
    "\n",
    "def create_confusion_matrix(n_feat,associations,real_assoc,false_assoc):\n",
    "    cm = np.empty((2,2))\n",
    "    # TN: only counting the upper half matrix (non doubled associations)\n",
    "    cm[0,0] = (n_feat*n_feat-n_feat)/2 - (associations+false_assoc) # Diagonal is discarded\n",
    "    cm[0,1] = false_assoc\n",
    "    cm[1,0] = associations- real_assoc\n",
    "    cm[1,1] = real_assoc\n",
    "\n",
    "    return cm\n",
    "\n",
    "def get_precision_recall(found_assoc_dict,false_assoc_dict,associations):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # True Positives \n",
    "    for score in found_assoc_dict.values():\n",
    "        y_true.append(1)\n",
    "        y_pred.append(score)\n",
    "    # False Positives\n",
    "    for score in false_assoc_dict.values():\n",
    "        y_true.append(0)\n",
    "        y_pred.append(score)\n",
    "    # False negatives\n",
    "    for _ in range(associations-len(found_assoc_dict)):\n",
    "        y_true.append(1)\n",
    "        y_pred.append(0)\n",
    "\n",
    "    precision, recall, thr = precision_recall_curve(y_true,y_pred) #thr will tell us score values\n",
    "    avg_prec = average_precision_score(y_true,y_pred)\n",
    "\n",
    "    return precision, recall, thr, avg_prec\n",
    "\n",
    "def plot_precision_recall(precision,recall,avg_prec,label, ax):\n",
    "    ax.scatter(recall,precision, lw=0, marker=\".\", s=5, edgecolors='none', label = f\"{label} - APS:{avg_prec:.2f}\")\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_thr_recall(thr, recall,label,  ax):\n",
    "    ax.scatter(recall[:-1],thr, lw=0, marker=\".\", s=5, edgecolors='none', label=label)\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "def plot_TP_vs_FP(tp_fp, label, ax):\n",
    "    ax.scatter(tp_fp[:,0],tp_fp[:,1],s=2, label=label, edgecolors='none')\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "def plot_filling_order(order_list, last_rank=None):\n",
    "    \n",
    "    if last_rank is None:\n",
    "        last_rank = len(order_list)\n",
    "    fig = plt.figure()\n",
    "    order_img = np.zeros((np.max(order_list),len(order_list)))\n",
    "    for i, element in enumerate(order_list):\n",
    "        order_img[element-1,i:] = 1\n",
    "\n",
    "    plt.imshow(order_img[:last_rank,:], cmap=\"binary\")\n",
    "    plt.xlabel(\"Correct prediction number\")\n",
    "    plt.ylabel(\"Association ranking\")\n",
    "    plt.plot(np.arange(last_rank),np.arange(last_rank))\n",
    "    return fig\n",
    "\n",
    "def plot_effect_size_matching(assoc_tuples_dict,found_assoc_dict,label, ALGORITHM, ax):\n",
    "\n",
    "\n",
    "    ground_truth_effects = [assoc_tuples_dict[key] for key in list(found_assoc_dict.keys())]\n",
    "    predicted_effects = np.array(list(found_assoc_dict.values()))\n",
    "\n",
    "    if ALGORITHM == 'ttest':\n",
    "        #Eq 15 on https://doi.org/10.1146/annurev-statistics-031017-100307\n",
    "        predicted_effects = [-np.log10(p) if p !=0 else -1 for p in predicted_effects]\n",
    "        predicted_effects[predicted_effects == -1] = np.max(predicted_effects) # Change zeros for max likelihood, -1 as dummy value\n",
    "        predicted_effects = np.array(predicted_effects)\n",
    "\n",
    "    max, min  = np.max(predicted_effects), np.min(predicted_effects)\n",
    "    standarized_pred_effects = (predicted_effects-min)/(max-min)\n",
    "    ax.scatter(ground_truth_effects,standarized_pred_effects,s=12, edgecolors='none', label=label)\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "def plot_venn_diagram(venn, ax, mode = 'all', scale='log'):\n",
    "    sets = [set(venn[key][mode]) for key in list(venn.keys())]\n",
    "    labels = (key for key in  list(venn.keys()))\n",
    "\n",
    "    if len(venn) == 2:\n",
    "        venn2(sets, labels, ax=ax)\n",
    "    elif len(venn) == 3:\n",
    "        venn3(sets, labels, ax=ax)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported number of input files.\")\n",
    "\n",
    "\n",
    "def plot_upsetplot(venn,assoc_tuples):\n",
    "    \n",
    "    all_assoc = set([association for ALGORITHM in venn.keys() for association in venn[ALGORITHM]['all']])\n",
    "    columns = ['ground truth']\n",
    "    columns.extend([ALGORITHM for ALGORITHM in list(venn.keys())])\n",
    "\n",
    "    df = {}\n",
    "    for association in all_assoc:\n",
    "        df[association] = []\n",
    "\n",
    "        if association in assoc_tuples:\n",
    "            df[association].append('TP')\n",
    "        else:\n",
    "            df[association].append('FP')\n",
    "        \n",
    "        for ALGORITHM in list(venn.keys()):\n",
    "            if association in venn[ALGORITHM]['all']:\n",
    "                df[association].append(1)\n",
    "            else:\n",
    "                df[association].append(0)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(df, orient='index', columns = columns)\n",
    "    df = df.set_index([pd.Index(df[ALGORITHM] == 1) for ALGORITHM in list(venn.keys())])\n",
    "    upset = UpSet(df, intersection_plot_elements=0, show_counts=True)\n",
    "\n",
    "    upset.add_stacked_bars(by=\"ground truth\", colors=cm.Pastel1,\n",
    "                       title=\"Count by ground truth value\", elements=10)\n",
    "\n",
    "    return upset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################### Main code ################################################\n",
    "\n",
    "def main(args_list):\n",
    "    parser = argparse.ArgumentParser(description='Read two files with ground truth associations and predicted associations.')\n",
    "    parser.add_argument('-p', '--perturbed', metavar='pert', type=str, required=True, help='perturbed feature names')\n",
    "    parser.add_argument('-n', '--features', metavar='feat', type=int, required=True, help='total number of features')\n",
    "    parser.add_argument('-r', '--reference', metavar='ref', type=str, required=True, help='path to the ground truth associations file')\n",
    "    parser.add_argument('-o', '--outpath', metavar='outpath', type=str, required=True, help='path where figures will be saved')\n",
    "    parser.add_argument('-t', '--targets', metavar='tar', type=str, required=True, nargs='+', help='path to the predicted associations files')\n",
    "    \n",
    "    args = parser.parse_args(args_list)\n",
    "\n",
    "\n",
    "    # Defining main performance evaluation figures:\n",
    "    fig_0, ax_0 = plt.subplots(figsize=(5,5))\n",
    "    fig_1, ax_1 = plt.subplots(figsize=(5,5))\n",
    "    fig_2, ax_2 = plt.subplots(figsize=(5,5))\n",
    "    fig_3, ax_3 = plt.subplots(figsize=(5,5))\n",
    "\n",
    "    assoc_tuples_dict = {}\n",
    "\n",
    "    # Reading the file with the ground truth changes:\n",
    "    with open (args.reference, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line[0] != \"f\" and line[0] != \"n\":\n",
    "                splitline = line.strip().split(\"\\t\") \n",
    "                feat_a = splitline[2]\n",
    "                feat_b = splitline[3]\n",
    "                assoc_strength = abs(float(splitline[4]))\n",
    "                # Only can detect associations with perturbed features\n",
    "                if args.perturbed in feat_a or args.perturbed in feat_b: \n",
    "                    assoc_tuples_dict[(feat_a,feat_b)] = assoc_strength\n",
    "                    assoc_tuples_dict[(feat_b,feat_a)] = assoc_strength\n",
    "\n",
    "    associations = int(len(assoc_tuples_dict)/2) \n",
    "    venn = {}\n",
    "    \n",
    "    # Count and save found associations\n",
    "    for target_file in args.targets:      \n",
    "        ALGORITHM = target_file.split('/')[-1].split('_')[3][:-4] \n",
    "        self_assoc, found_assoc_dict, false_assoc_dict, tp_fp = classify_associations(target_file,list(assoc_tuples_dict.keys()))\n",
    "        real_assoc = len(found_assoc_dict) # True predicted associations\n",
    "        false_assoc = len(false_assoc_dict) # False predicted associations\n",
    "        total_assoc = real_assoc + false_assoc\n",
    "\n",
    "        venn[ALGORITHM] = {}\n",
    "        venn[ALGORITHM]['correct'] = list(found_assoc_dict.keys()) \n",
    "        venn[ALGORITHM]['all'] = list(found_assoc_dict.keys()) +  list(false_assoc_dict.keys())\n",
    "\n",
    "        # Assess ranking of associations (they are doubled in assoc_tuples):\n",
    "        order_list = [list(assoc_tuples_dict.keys()).index((feat_a,feat_b))//2 for (feat_a,feat_b) in list(found_assoc_dict.keys())]\n",
    "        fig = plot_filling_order(order_list)\n",
    "        fig.savefig(f\"Order_image_{ALGORITHM}.png\", dpi=200)\n",
    "\n",
    "        ax_0 = plot_effect_size_matching(assoc_tuples_dict, found_assoc_dict, ALGORITHM, ALGORITHM, ax_0)\n",
    "\n",
    "        # Plot confusion matrix:\n",
    "        cm = create_confusion_matrix(args.features,associations,real_assoc,false_assoc)\n",
    "        fig = plot_confusion_matrix(cm,\n",
    "                              [\"No assoc\",\"Association\"],\n",
    "                              cmap=None,\n",
    "                              normalize=False)\n",
    "\n",
    "        fig.savefig(f'Confusion_matrix_{ALGORITHM}.png', dpi=100, bbox_inches='tight')\n",
    "\n",
    "        # Plot precision-recall and TP-FP curves\n",
    "        precision, recall, thr, avg_prec = get_precision_recall(found_assoc_dict,false_assoc_dict,associations)\n",
    "\n",
    "        ax_1 = plot_precision_recall(precision,recall, avg_prec,ALGORITHM, ax_1)\n",
    "        ax_2 = plot_TP_vs_FP(tp_fp, ALGORITHM, ax_2)\n",
    "        ax_3 = plot_thr_recall(thr, recall, ALGORITHM, ax_3)\n",
    "\n",
    "\n",
    "        # Write results:\n",
    "        with open('Performance_evaluation_summary_results.txt','a') as f:\n",
    "            f.write(f\" File:  {target_file}\\n\")\n",
    "            f.write(f\"Ground truth detectable associations (i.e. involving perturbed feature,{args.perturbed}):{associations}\\n\")\n",
    "            f.write(f\"{total_assoc} unique associations found\\n{self_assoc} self-associations were found before filtering\\n{real_assoc} were real associations\\n{false_assoc} were either false or below the significance threshold\\n\")\n",
    "            #print(\"Correct associations:\\n\", found_assoc_tuples, \"\\n\")\n",
    "            f.write(f\"Sensitivity:{real_assoc}/{associations} = {real_assoc/associations}\\n\")\n",
    "            f.write(f\"Precision:{real_assoc}/{total_assoc} = {(real_assoc)/total_assoc}\\n\")\n",
    "            f.write(f\"Order list:{order_list}\\n\\n\")\n",
    "            f.write(\"______________________________________________________\\n\")\n",
    "\n",
    "\n",
    "    # Edit figures: layout\n",
    "    ax_0.set_xlabel(\"Real effect\")\n",
    "    ax_0.set_ylabel(\"Predicted effect\")\n",
    "    ax_0.set_ylim((-0.02,1.02))\n",
    "    ax_0.set_xlim((0,1.02))\n",
    "    ax_0.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
    "              ncol=3, fancybox=True, shadow=True)\n",
    "\n",
    "    ax_1.set_xlabel(\"Recall\")\n",
    "    ax_1.set_ylabel(\"Precision\")\n",
    "    ax_1.legend()\n",
    "    ax_1.set_ylim((0,1.05))\n",
    "    ax_1.set_xlim((0,1.05))\n",
    "\n",
    "    ax_2.set_xlabel(\"False Positives\")\n",
    "    ax_2.set_ylabel(\"True Positives\")\n",
    "    ax_2.set_aspect(\"auto\")\n",
    "\n",
    "    ax_3.set_ylabel(\"Threshold\")\n",
    "    ax_3.set_xlabel(\"Recall\")\n",
    "\n",
    "\n",
    "    # Save main figures:\n",
    "    fig_0.savefig(args.outpath + \"Effect_size_matchin.png\", dpi=200)\n",
    "    fig_1.savefig(args.outpath + \"Precision_recall.png\", dpi=200)\n",
    "    fig_2.savefig(args.outpath + \"TP_vs_FP.png\", dpi=200)\n",
    "    fig_3.savefig(args.outpath + \"thr_vs_recall.png\", dpi=200)\n",
    "\n",
    "    # Plotting venn diagram:\n",
    "    if len(venn) == 2 or len(venn) == 3:\n",
    "        fig_v, ax_v = plt.subplots(figsize=(4,4))\n",
    "        ax_v = plot_venn_diagram(venn, ax_v, mode = 'correct')\n",
    "        fig_v.savefig(args.outpath + 'Venn_diagram.png', dpi=200)\n",
    "\n",
    "    # Plotting UpSet plot\n",
    "    upset = plot_upsetplot(venn, list(assoc_tuples_dict.keys()))\n",
    "    upset.plot()\n",
    "    plt.savefig(args.outpath + 'UpSet.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_list = [\n",
    "    '-p', 'Continuous_A_2',\n",
    "    '-n', '36',\n",
    "    '-r', './synthetic_data/changes.random_all_sim.txt',\n",
    "    '-o', './synthetic_data/',\n",
    "    '-t', \n",
    "    './results_cont_paper/identify_associations/results_sig_assoc_ttest.tsv',\n",
    "    './results_cont_paper/identify_associations/results_sig_assoc_bayes.tsv',\n",
    "     './results_cont_paper/identify_associations/results_sig_assoc_ks.tsv'\n",
    "]\n",
    "\n",
    "main(args_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synthetic data 2: Simplified version:**\n",
    "\n",
    "Low N: 50\n",
    "High N: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r ./interim_data_cont_paper_II/models/\n",
    "! rm results_cont_paper_II/latent_space/model.pt\n",
    "! rm -r results_cont_paper_II/identify_associations/\n",
    "! rm -r ./synthetic_data_II/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Hyperparameters ####################################\n",
    "import random as rnd\n",
    "\n",
    "PROJECT_NAME = \"random_all_sim\"\n",
    "MODE = \"linear\"  # \"non-linear\"\n",
    "HIGH_CORR = True\n",
    "SEED_1 = 1234\n",
    "np.random.seed(SEED_1)\n",
    "rnd.seed(SEED_1)\n",
    "\n",
    "COV_ALPHA = 0.97 #.01 \n",
    "N_SAMPLES = 1000 # LOW N: 50, High N: 1000\n",
    "\n",
    "\n",
    "SETTINGS = {\n",
    "    \"Continuous_A\": {\n",
    "        \"features\": 10,\n",
    "        \"frequencies\": [0.002, 0.01, 0.02],\n",
    "        \"coefficients\": [500, 100, 50],\n",
    "        \"phase\": 0,\n",
    "        \"offset\": 500,\n",
    "    },\n",
    "    \"Continuous_B\": {\n",
    "        \"features\": 10,\n",
    "        \"frequencies\": [0.001, 0.05, 0.08],\n",
    "        \"coefficients\": [80, 20, 10],\n",
    "        \"phase\": np.pi / 2,\n",
    "        \"offset\": 400,\n",
    "    },\n",
    "    \"Categorical_A\": {\n",
    "        \"features\": 1,\n",
    "        \"frequencies\": [0.1, 0.5, 0.8],\n",
    "        \"coefficients\": [.2, .1, .05],\n",
    "        \"phase\": np.pi / 2,\n",
    "        \"offset\": 10,\n",
    "    },\n",
    "        \"Categorical_B\": {\n",
    "        \"features\": 1,\n",
    "        \"frequencies\": [0.01, 0.5, 0.08],\n",
    "        \"coefficients\": [10, .1, .05],\n",
    "        \"phase\": np.pi,\n",
    "        \"offset\": 1,\n",
    "    }\n",
    "}\n",
    "\n",
    "COR_THRES = 0.02\n",
    "PAIRS_OF_INTEREST = [(1,2),(3,4)]  # ,(77,75),(99,70),(38,2),(67,62)]\n",
    "\n",
    "# Path to store output files\n",
    "outpath = Path(\"synthetic_data_II\")\n",
    "outpath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# %%\n",
    "# Add all datasets in a single matrix:\n",
    "all_feature_names = get_feature_names(SETTINGS)\n",
    "feat_means = create_mean_profiles(SETTINGS)\n",
    "\n",
    "# %%\n",
    "###### Covariance matrix definition ######\n",
    "if MODE == \"linear\":\n",
    "    covariance_matrix = make_sparse_spd_matrix(\n",
    "        dim=len(all_feature_names),\n",
    "        alpha=COV_ALPHA,\n",
    "        smallest_coef=0,\n",
    "        largest_coef=1,\n",
    "        norm_diag=True,\n",
    "        random_state=SEED_1,\n",
    "    )\n",
    "elif MODE == \"non-linear\":\n",
    "    covariance_matrix = np.identity(len(all_feature_names))\n",
    "\n",
    "ABS_MAX = np.max(abs(covariance_matrix))\n",
    "fig = plot_score_matrix(\n",
    "    covariance_matrix, all_feature_names, vmin=-ABS_MAX, vmax=ABS_MAX\n",
    ")\n",
    "fig.savefig(outpath / f\"Covariance_matrix_{PROJECT_NAME}.png\")\n",
    "\n",
    "#    dataset = np.array(\n",
    "#        [\n",
    "#            list(np.random.multivariate_normal(feat_means, covariance_matrix))\n",
    "#            for _ in range(N_SAMPLES)\n",
    "#        ]\n",
    "#    )\n",
    "\n",
    "dataset = np.random.multivariate_normal(feat_means, covariance_matrix, N_SAMPLES)\n",
    "\n",
    "\n",
    "\n",
    "# Add non-linearities\n",
    "if MODE == \"non-linear\":\n",
    "    for i, j in PAIRS_OF_INTEREST:\n",
    "        freq = np.random.choice([4, 5, 6])\n",
    "        dataset[:, i] += np.sin(freq * dataset[:, j])\n",
    "\n",
    "#scaled_dataset, _ = scale(dataset)\n",
    "# No scaling in the dataset creation! It will be handled in preprocessing.\n",
    "scaled_dataset = dataset\n",
    "\n",
    "if HIGH_CORR: # The last half of the features are combinations of the first half:\n",
    "    for i in range(scaled_dataset.shape[1]//2):\n",
    "        col_1 = np.random.choice(range(scaled_dataset.shape[1]//2))\n",
    "        col_2 = np.random.choice(range(scaled_dataset.shape[1]//2))\n",
    "        scaled_dataset[:,i+scaled_dataset.shape[1]//2] = (scaled_dataset[:,col_1]+scaled_dataset[:,col_2])/2 + np.random.normal()\n",
    "\n",
    "# Binarize the categorical dataset\n",
    "NUM_CAT = SETTINGS[\"Categorical_A\"][\"features\"] + SETTINGS[\"Categorical_B\"][\"features\"]\n",
    "columns_to_binarize = scaled_dataset[:,-NUM_CAT:]\n",
    "\n",
    "# Compute the mean of each of the categorical columns    \n",
    "means = columns_to_binarize.mean(axis=0)\n",
    "\n",
    "# Apply the binarization\n",
    "scaled_dataset[:,-NUM_CAT:] = (columns_to_binarize > means).astype(int)\n",
    "\n",
    "print(np.min(scaled_dataset),np.max(scaled_dataset))\n",
    "# Actual correlations:\n",
    "# correlations = np.empty(np.shape(covariance_matrix))\n",
    "# for ifeat in range(len(covariance_matrix)):\n",
    "#     for jfeat in range(len(covariance_matrix)):\n",
    "#         correlations[ifeat, jfeat] = pearsonr(dataset[:, ifeat], dataset[:, jfeat])[\n",
    "#             0\n",
    "#         ]\n",
    "\n",
    "correlations = np.corrcoef(scaled_dataset, rowvar=False)\n",
    "fig = plot_score_matrix(correlations, all_feature_names, vmin=-1, vmax=1, label_step=5)\n",
    "fig.savefig(outpath / f\"Correlations_{PROJECT_NAME}.png\", dpi=200)\n",
    "\n",
    "# Sort correlations by absolute value\n",
    "associations = create_ground_truth_correlations_file(correlations)\n",
    "associations.to_csv(outpath / f\"changes.{PROJECT_NAME}.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Plot feature profiles per sample\n",
    "fig = plot_feature_profiles(scaled_dataset, feat_means)\n",
    "fig.savefig(outpath / \"Multi-omic_profiles.png\")\n",
    "\n",
    "## Plot correlations\n",
    "fig = plot_feature_correlations(dataset, PAIRS_OF_INTEREST)\n",
    "fig.savefig(outpath / \"Feature_correlations.png\")\n",
    "\n",
    "fig = plot_feature_correlations(scaled_dataset, PAIRS_OF_INTEREST)\n",
    "fig.savefig(outpath / \"Feature_correlations_scaled.png\")\n",
    "\n",
    "# Write tsv files with feature values for all samples in both datasets:\n",
    "save_splitted_datasets(\n",
    "    SETTINGS, PROJECT_NAME, scaled_dataset, all_feature_names, N_SAMPLES, outpath\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running MOVE on simple synthetic data\n",
    "\n",
    "# Encode data\n",
    "#! move-dl task=encode_data data=random_continuous_paper_II \n",
    "\n",
    "# Identify assoc ks\n",
    "#! move-dl task=random_continuous_paper_II__latent data=random_continuous_paper_II \n",
    "\n",
    "# Identify assoc ks\n",
    "! move-dl task=random_continuous_paper_II__id_assoc_ks data=random_continuous_paper_II "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARC_BUCKET = \"gs://collaborations_marc/shared_2023/results_marc/results_residualized/\" #associations_final_tables/Bayes/\" #results_synthetic_small/high_N_low_corr/max_pert/\"\n",
    "\n",
    "\n",
    "! gsutil -m cp -r ./paper_figures/ {MARC_BUCKET}\n",
    "#! gsutil -m cp -r ./results_cont_paper_II/ {MARC_BUCKET}\n",
    "#! gsutil -m cp -r ./synthetic_data_II/ {MARC_BUCKET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./results_temp_id_assoc_cellType/id_assoc_200_[500]_0.0001_cellType_AMSC__id_assoc_bayes/identify_associations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib.pyplot import cm\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "DATASET = \"Continuous_B\"\n",
    "#feature_list = [\"Day\"]# + list(genes_of_interest.keys())\n",
    "feature_list = pd.read_csv(f\"./interim_data_cont_paper_II/random.random_all_sim.{DATASET}.txt\", header=None)\n",
    "feature_list = feature_list.values.flatten()\n",
    "\n",
    "figure_path = Path(\"./synthetic_data_II/figures_synthetic_small/\")\n",
    "! mkdir -p {figure_path}\n",
    "\n",
    "latent_space = np.load(\"./results_cont_paper_II/identify_associations/latent_location.npy\")\n",
    "\n",
    "def plot_3D_latent_and_displacement(\n",
    "    mu_baseline,\n",
    "    mu_perturbed,\n",
    "    feature_values,\n",
    "    feature_name,\n",
    "    show_baseline=True,\n",
    "    show_perturbed=True,\n",
    "    show_arrows=True,\n",
    "    step: int=1,\n",
    "    altitude: int=30,\n",
    "    azimuth: int=45,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the movement of the samples in the 3D latent space after perturbing one\n",
    "    input variable.\n",
    "\n",
    "    Args:\n",
    "        mu_baseline:\n",
    "            ND array with dimensions n_samples x n_latent_nodes containing\n",
    "            the latent representation of each sample\n",
    "        mu_perturbed:\n",
    "            ND array with dimensions n_samples x n_latent_nodes containing\n",
    "            the latent representation of each sample after perturbing the input\n",
    "        feature_values:\n",
    "            1D array with feature values to map to a colormap (\"bwr\"). Each sample is\n",
    "            colored according to its value for the feature of interest.\n",
    "        feature_name:\n",
    "            name of the feature mapped to a colormap\n",
    "        show_baseline:\n",
    "            plot orginal location of the samples in the latent space\n",
    "        show_perturbed:\n",
    "            plot final location (after perturbation) of the samples in latent space\n",
    "        show_arrows:\n",
    "            plot arrows from original to final location of each sample\n",
    "        angle:\n",
    "            elevation from dim1-dim2 plane for the visualization of latent space.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If latent space is not 3-dimensional (3 hidden nodes).\n",
    "    Returns:\n",
    "        Figure\n",
    "    \"\"\"\n",
    "    # construct cmap\n",
    "    #hex_colors= ['#36429bff','#78b0d3ff','#fedc8cff','#d22b27ff']\n",
    "    #my_cmap = ListedColormap(hex_colors)\n",
    "    #my_cmap = sns.color_palette(\"Dark2\", as_cmap=True)\n",
    "    my_cmap = sns.color_palette(\"RdYlBu\", as_cmap=True)\n",
    "\n",
    "    eps = 1e-16\n",
    "    if [np.shape(mu_baseline)[1], np.shape(mu_perturbed)[1]] != [3, 3]:\n",
    "        raise ValueError(\n",
    "            \" The latent space must be 3-dimensional. Redefine num_latent to 3.\"\n",
    "        )\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(7, 7))\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    ax.view_init(altitude, azimuth)\n",
    "\n",
    "    if show_baseline:\n",
    "        vmin, vmax = np.min(feature_values[::step]), np.max(feature_values[::step])\n",
    "        abs_max = np.max([abs(vmin), abs(vmax)])\n",
    "        plot = ax.scatter(\n",
    "            mu_baseline[::step, 0],\n",
    "            mu_baseline[::step, 1],\n",
    "            mu_baseline[::step, 2],\n",
    "            marker=\"o\",\n",
    "            c=feature_values[::step],\n",
    "            s=20,\n",
    "            lw=0,\n",
    "            cmap=my_cmap,\n",
    "            #vmin=0,\n",
    "            #vmax=1\n",
    "        )\n",
    "        ax.set_title(feature_name)\n",
    "        fig.colorbar(plot)\n",
    "        \n",
    "        #plt.colorbar()  # Normalize(min(feature_values[::step]),max(feature_values[::step]))), ax=ax)\n",
    "    if show_perturbed:\n",
    "        ax.scatter(\n",
    "            mu_perturbed[::step, 0],\n",
    "            mu_perturbed[::step, 1],\n",
    "            mu_perturbed[::step, 2],\n",
    "            marker=\"o\",\n",
    "            c=feature_values[::step],\n",
    "            s=10,\n",
    "            label=\"perturbed\",\n",
    "            lw=0,\n",
    "        )\n",
    "    if show_arrows:\n",
    "        u = mu_perturbed[::step, 0] - mu_baseline[::step, 0]\n",
    "        v = mu_perturbed[::step, 1] - mu_baseline[::step, 1]\n",
    "        w = mu_perturbed[::step, 2] - mu_baseline[::step, 2]\n",
    "\n",
    "        module = np.sqrt(u * u + v * v + w * w)\n",
    "\n",
    "        mask = module > eps\n",
    "\n",
    "        max_u, max_v, max_w = np.max(abs(u)), np.max(abs(v)), np.max(abs(w))\n",
    "        # Arrow colors will be weighted contributions of red -> dim1, green -> dim2, and blue-> dim3. I.e. purple arrow means movement in dims 1 and 3\n",
    "        colors = [\n",
    "            (abs(du) / max_u, abs(dv) / max_v, abs(dw) / max_w, 0.7)\n",
    "            for du, dv, dw in zip(u, v, w)\n",
    "        ]\n",
    "        ax.quiver(\n",
    "            mu_baseline[::step, 0][mask],\n",
    "            mu_baseline[::step, 1][mask],\n",
    "            mu_baseline[::step, 2][mask],\n",
    "            u[mask],\n",
    "            v[mask],\n",
    "            w[mask],\n",
    "            color=colors,\n",
    "            lw=.8,\n",
    "            )  # alpha=(1-module/np.max(module))**6, arrow_length_ratio=0)\n",
    "        # help(ax.quiver)\n",
    "    ax.set_xlabel(\"Dim 1\")\n",
    "    ax.set_ylabel(\"Dim 2\")\n",
    "    ax.set_zlabel(\"Dim 3\")\n",
    "\n",
    "    \n",
    "    # ax.set_axis_off()\n",
    "\n",
    "    return fig\n",
    "\n",
    "for i,feature in enumerate(feature_list):\n",
    "    feature_values = np.load(f\"./interim_data_cont_paper_II/random.random_all_sim.{DATASET}.npy\")\n",
    "    print(feature_values.shape)\n",
    "    \n",
    "    feature_values = feature_values[:,i]\n",
    "    latent_space_baseline = latent_space[:,:,-1]\n",
    "    latent_space_perturbed = latent_space[:,:,i]\n",
    "    \n",
    "    # # Plot latent space:\n",
    "    pic_num = 0\n",
    "    n_pictures = 100\n",
    "    \n",
    "    \n",
    "    for azimuth, altitude in zip(\n",
    "        np.linspace(0, 90, n_pictures), np.linspace(15, 45, n_pictures)\n",
    "    ):\n",
    "        \n",
    "        if pic_num == 80:\n",
    "\n",
    "            title = feature \n",
    "            fig = plot_3D_latent_and_displacement(\n",
    "                latent_space_baseline,\n",
    "                latent_space_perturbed,\n",
    "                feature_values=feature_values,\n",
    "                feature_name=f\"{title}\",\n",
    "                show_baseline=True,\n",
    "                show_perturbed=False,\n",
    "                show_arrows=False,\n",
    "                altitude=altitude,\n",
    "                azimuth=azimuth,\n",
    "            )\n",
    "            fig.savefig(\n",
    "                figure_path / f\"3D_latent_{pic_num}_perturbed_{feature}.png\", dpi=200\n",
    "            )\n",
    "            plt.close(fig)\n",
    "\n",
    "            fig = plot_3D_latent_and_displacement(\n",
    "                latent_space_baseline,\n",
    "                latent_space_perturbed,\n",
    "                feature_values=feature_values,\n",
    "                feature_name=f\"{title}\",\n",
    "                show_baseline=False,\n",
    "                show_perturbed=False,\n",
    "                show_arrows=True,\n",
    "                altitude=altitude,\n",
    "                azimuth=azimuth,\n",
    "            )\n",
    "            fig.savefig(\n",
    "                figure_path / f\"3D_latent_{pic_num}_perturbed_{feature}_arrows.png\", dpi=200\n",
    "            )\n",
    "            plt.close(fig)\n",
    "        \n",
    "        pic_num += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(1,11): # range(0,90,10):\n",
    "    image_path = figure_path / f\"3D_latent_80_perturbed_Continuous_B_{i}.png\"\n",
    "    image = mpimg.imread(image_path)\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "    image_path = figure_path / f\"3D_latent_80_perturbed_Continuous_B_{i}_arrows.png\"\n",
    "    image = mpimg.imread(image_path)\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing installation:\n",
    "\n",
    "```python\n",
    "if INSTALL:\n",
    "    # We will clone the version under development of MOVE, which can handle perturbations of continuous variables\n",
    "    #! git clone -b developer-continuous-v3 https://github.com/RasmussenLab/MOVE.git /home/jupyter/.local/bin/MOVE\n",
    "    ! git clone -b developer https://github.com/RasmussenLab/MOVE.git /home/jupyter/.local/bin/MOVE\n",
    "    %cd /home/jupyter/.local/bin/MOVE\n",
    "    # Step 3: Fetch the pull request\n",
    "    !git fetch origin pull/92/head:pr-92\n",
    "    \n",
    "    # Step 4: Checkout the branch\n",
    "    !git checkout pr-92\n",
    "    \n",
    "    sys.path.append(\"/home/jupyter/.local/bin/MOVE/src\")\n",
    "    # ! pip install -e /home/jupyter/.local/bin/MOVE/ \n",
    "    \n",
    "    # Execute notebook from starting folder\n",
    "    %cd /home/jupyter/Characterizing AMSCs using MOVE/edit\n",
    "    \n",
    "    #from terra_notebook_utils import table, gs, drs\n",
    "    # We can also enable different sections to fold\n",
    "    ! jupyter nbextension enable codefolding/main\n",
    "    ! jupyter nbextension enable collapsible_headings/main\n",
    "\n",
    "    ! pip install omegaconf upsetplot umap-learn\n",
    "    ! pip install -r /home/jupyter/.local/bin/MOVE/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for dropped columns: full nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('./data_residualized/Cells.tsv', sep='\\t')\n",
    "print(a.shape)\n",
    "\n",
    "b = np.load('./interim_data_rsd/Cells.npy')\n",
    "print(b.shape)\n",
    "\n",
    "filepath = Path('./interim_data_rsd/Cells.npy')\n",
    "data = np.load(filepath).astype(np.float32)\n",
    "data[data == 0] += 1000\n",
    "fig = plt.figure()\n",
    "plt.imshow(data, cmap='viridis')\n",
    "fig.show()\n",
    "\n",
    "mask_col = np.abs(data).sum(axis=0) != 0\n",
    "\n",
    "data[:,~mask_col] += 1000\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(data, cmap='binary')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "data = data[:, mask_col]\n",
    "print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "596px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
